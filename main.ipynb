{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "from torch.utils.data import DataLoader, ConcatDataset, random_split\n",
    "from torchvision.models import resnet50, resnet101, vgg16\n",
    "import torch.optim as optim \n",
    "from torch.optim.lr_scheduler import OneCycleLR, ReduceLROnPlateau\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os \n",
    "from skimage import io, transform \n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np \n",
    "from torchvision import transforms, utils\n",
    "from torchvision import models\n",
    "from torch.nn import functional as F\n",
    "from skimage.transform import resize\n",
    "import json\n",
    "import csv \n",
    "from matplotlib import pyplot as plt \n",
    "import random \n",
    "from math import pi \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(image):\n",
    "    plt.imshow(image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all images for data that is used in the random forest \n",
    "def image(image_path):\n",
    "\n",
    "        image = io.imread(image_path)\n",
    "        image = image.transpose((2, 0, 1)) # Go from H, W, C to C, H, W which the network expects\n",
    "\n",
    "        # image = Image.open(image_path)\n",
    "\n",
    "        # label = np.array(label, dtype=float).reshape(-1, 5)\n",
    "        \n",
    "        \n",
    "        # Convert image and label to tensors \n",
    "        image = torch.from_numpy(image).float()\n",
    "        original_w, original_h = image.shape[1], image.shape[2]\n",
    "        aspect_ratio = original_w / original_h\n",
    "        target_size = (224, 224)\n",
    "        if target_size[0] / target_size[1] > aspect_ratio:\n",
    "            # Width is larger than the desired aspect ratio\n",
    "            padded_width = target_size[0]\n",
    "            padded_height = int(padded_width / aspect_ratio)\n",
    "        else:\n",
    "            # Height is larger than the desired aspect ratio\n",
    "            padded_height = target_size[1]\n",
    "            padded_width = int(padded_height * aspect_ratio)\n",
    "\n",
    "        h_padding = (padded_width - original_w) // 2\n",
    "        v_padding = (padded_height - original_h) // 2\n",
    "        padding = (h_padding, v_padding, h_padding, v_padding)\n",
    "        padded_img_tensor = transforms.Pad(padding, padding_mode='reflect')(image)\n",
    "        image = transforms.Resize(target_size, antialias=True)(padded_img_tensor)\n",
    "\n",
    "        image = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(image)\n",
    "        image = image.unsqueeze(0)\n",
    "\n",
    "        return image\n",
    "\n",
    "\n",
    "# 1424 images train\n",
    "# 357 images validate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_file, data_path, transform=None):\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "        self.labels = pd.read_csv(csv_file) \n",
    "\n",
    "    \n",
    "    def __len__(self): \n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, index): \n",
    "        # Uncomment to get data for the randomforest \n",
    "        # location = \"./Data_tree_full.csv\"\n",
    "        # tree = pd.read_csv(location)\n",
    "        # with open(\"data_new.json\", 'r') as f: \n",
    "        #     data = json.load(f)\n",
    "        # tree[\"time\"][index]\n",
    "        # for x in tree[\"time\"]:\n",
    "        #     if x in data: \n",
    "        #         if data[x][\"picture\"] != None:\n",
    "        #             image_path = os.path.join(\"./weather_used/\", data[x][\"picture\"])\n",
    "        # image_path = os.path.join(self.data_path, self.labels.iloc[index, 5])\n",
    "        image_path = os.path.join(self.data_path, self.labels.iloc[index, 4])\n",
    "        image = io.imread(image_path)\n",
    "        # new_width = int(image.shape[1] * 0.39) # 228\n",
    "        # new_height = int(image.shape[0] * 0.39) # 184\n",
    "        new_width = 224\n",
    "        new_height = 224\n",
    "\n",
    "        # print(f\"New width: {new_width}\")\n",
    "        # print(f\"New height: {new_height}\")\n",
    "        # image = resize(image, (new_height, new_width), anti_aliasing=True)\n",
    "\n",
    "        # upsample = nn.Upsample(size=(928, 928), mode=\"bilinear\")\n",
    "\n",
    "        # Resize the image to 224 x 224 \n",
    "        image = transform.resize(image, (224, 224))\n",
    "        # image = transform.resize(image, (928, 928))\n",
    " \n",
    "\n",
    "        # image = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])(image)\n",
    "\n",
    "        image = image.transpose((2, 0, 1)) # Go from H, W, C to C, H, W which the network expects\n",
    "\n",
    "        # image = Image.open(image_path)\n",
    "\n",
    "        if self.transform: \n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # label = self.labels.iloc[index, 0:5] # With no rain label \n",
    "        label = self.labels.iloc[index, 0:4] # Removed no rain label\n",
    "        # label = np.array(label, dtype=float).reshape(-1, 5)\n",
    "        \n",
    "        \n",
    "        # Convert image and label to tensors \n",
    "        image = torch.from_numpy(image).float()\n",
    "        # image = upsample(image)\n",
    "\n",
    "\n",
    "        label = torch.tensor(label).float()\n",
    "        # image = self.random_rotation(image)\n",
    "        # image = transforms.Normalize(mean=[0.0081, 0.0132, 0.0119], std=[0.0081, 0.0132, 0.0119])(image)\n",
    "        \n",
    "        # label = F.one_hot(label, num_classes=5)\n",
    "        # print(f\"Label: {label}\")\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def random_rotation(self, image):\n",
    "        angle = random.uniform(0, 360)\n",
    "        trans = transforms.RandomRotation(degrees=angle)\n",
    "        rotated_image = trans(image)\n",
    "        \n",
    "\n",
    "\n",
    "        return rotated_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rain net maybe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model from HW3 not as good as resnet but something that we tried \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # According to the PyTorch Documentation:\n",
    "        # \"if a nn.Conv2d layer is directly followed by a nn.BatchNorm2d layer, then the bias \n",
    "        # in the convolution is not needed. The first step of BatchNorm subtracts the mean, which\n",
    "        # effectively cancels out the bias effect\"\n",
    "        # \"This applies to 1d and 3d convolutions as long as BatchNorm (or other normalization layer)\n",
    "        # normalizes on the same dimension as convolution's bias\"\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=(3, 3), padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=(3, 3), padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=(3, 3), padding=1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.LazyLinear(out_features=256)\n",
    "        self.fc2 = nn.LazyLinear(out_features=128)\n",
    "        self.fc3 = nn.LazyLinear(out_features=4)\n",
    "        self.flat = nn.Flatten()\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = self.pool(self.bn1(torch.relu(self.conv1(x)))) \n",
    "        x = self.pool(self.bn2(torch.relu(self.conv2(x)))) \n",
    "        x = self.pool(self.bn3(torch.relu(self.conv3(x)))) \n",
    "        \n",
    "        x = self.pool(self.bn4(torch.relu(self.conv4(x))))\n",
    "        x = self.flat(x)\n",
    "        x = torch.relu(self.fc1(x)) \n",
    "        x = torch.relu(self.fc2(x)) \n",
    "        x = self.fc3(x) \n",
    "        return x\n",
    "\n",
    "# model = Net()\n",
    "model = resnet50(weights=None)\n",
    "# model = resnet50()\n",
    "# model = resnet101(weights=None)\n",
    "\n",
    "# model = vgg16(weights=None)\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "# in_features = model.classifier[-1].in_features\n",
    "# model.classifier[-1] = nn.Linear(in_features, 4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the data \n",
    "# Used to test the entire model and to get the data for the random forest \n",
    "def getAllData():\n",
    "    mean_sum = 0.0\n",
    "    std_sum = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    data = MyDataset(\"./NewData.csv\", \"./weather_new/\", transform=None)\n",
    "    # Train on 80% of the dataset\n",
    "    train_size = int(0.8 * len(data)) \n",
    "\n",
    "    # Validate on the remaining 20% of the dataset\n",
    "    val_size = len(data) - train_size\n",
    "\n",
    "    data_full = DataLoader(dataset=data, batch_size=1, shuffle=False, num_workers=4)\n",
    "        \n",
    "    return data_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and validation data (used for training and testing)\n",
    "def getData():\n",
    "    mean_sum = 0.0\n",
    "    std_sum = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    # composed = transforms.Compose([\n",
    "    # ])\n",
    "    data = MyDataset(\"./NewData_4.csv\", \"./weather_new/\", transform=None)\n",
    "    # Train on 80% of the dataset\n",
    "    train_size = int(0.8 * len(data)) \n",
    "\n",
    "    # Validate on the remaining 20% of the dataset\n",
    "    val_size = len(data) - train_size\n",
    "\n",
    "    # Randomly split the dataset into training and testing data\n",
    "\n",
    "    # all_data = DataLoader(dataset=data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "\n",
    "    # for  batch, (images, labels) in enumerate(all_data):\n",
    "    #     print(f\"Image shape: \", images.shape) #[32, 3, 244, 244]\n",
    "\n",
    "    #     mean_sum += torch.mean(images, dim=(0, 2, 3)) # 0, 2, 3\n",
    "    #     std_sum += torch.mean(images, dim=(0, 2, 3))\n",
    "    #     total_samples += images.size(0)\n",
    "\n",
    "\n",
    "\n",
    "    # overall_mean = mean_sum / total_samples\n",
    "    # overall_std = std_sum / total_samples\n",
    "\n",
    "    # print(f\"Calculated Mean: \", overall_mean)\n",
    "    # print(f\"Calculated Standard Deviation: \", overall_std)\n",
    "        \n",
    "        \n",
    "    \n",
    "    train_dataset, val_dataset = random_split(data, [train_size, val_size])\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "    return train_loader, val_loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model \n",
    "def test(): \n",
    "    model.eval()\n",
    "    n_correct = 0 \n",
    "    n_incorrect = 0\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader: \n",
    "            # print(f\"Shape: {images.shape}\")\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            \n",
    "            # Forward Pass\n",
    "            output = model(images)\n",
    "            \n",
    "            test_loss += lossFunc_test(output, labels).item()\n",
    "\n",
    "\n",
    "            output.data = torch.sigmoid(output) # Convert output data to between 0 and 1\n",
    "            # output = output.to('cpu')\n",
    "            # print(f\"Predictions: {np.round(output.numpy(), decimals=3)}\")\n",
    "\n",
    "            # predictions = (output > 0.9).int()\n",
    "            predictions = (output > 0.5).int()\n",
    "            \n",
    "            # print(f\"Predictions: {predictions}\")\n",
    "            # print(f\"True labels: {labels}\")\n",
    "\n",
    "            n_correct += (predictions == labels).all(dim=1).sum()\n",
    "            n_incorrect += (predictions != labels).all(dim=1).sum()\n",
    "             \n",
    "\n",
    "\n",
    "    # output.data = torch.mul(100, output) # conver the data / confidence into %\n",
    "    test_loss /= len(val_loader.dataset)\n",
    "    print(\"\\nTest set: Avg. loss: {:.4f} Accuracy: {}/{} ({:.2f}%)\\n\".format(\n",
    "        test_loss, n_correct, len(val_loader.dataset), \n",
    "        100. * (n_correct / len(val_loader.dataset))))\n",
    "    \n",
    "    accuracy = (100. * (n_correct / len(val_loader.dataset)))\n",
    "    accuracy = accuracy.to(\"cpu\")\n",
    "    # print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model for the tree (generate the data that goes into the tree)\n",
    "def testTree():  \n",
    "    model = torch.load(\"model_FULL_28.pth\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    location = \"./Data_tree_full.csv\"\n",
    "    tree = pd.read_csv(location)\n",
    "    with open(\"data_new.json\", 'r') as f: \n",
    "        data = json.load(f)\n",
    "    \n",
    "    count = 0\n",
    "    row = []\n",
    "    # predictions = open(\"Predict\", \"w\")\n",
    "    for i, x in enumerate(tree[\"time\"]): \n",
    "        if x in data: \n",
    "\n",
    "            # print(data[x][\"picture\"])\n",
    "            if data[x][\"picture\"] != None:\n",
    "                loc = os.path.join(\"./weather_used/\", data[x][\"picture\"])\n",
    "                count += 1 \n",
    "                img = image(loc)\n",
    "                # print(img)\n",
    "                img = img.to(device)\n",
    "                output = model(img)\n",
    "\n",
    "                output.data = torch.sigmoid(output) # Convert output data to between 0 and 1\n",
    "                out = output.cpu()\n",
    "                prediction = np.round(out.detach().numpy(),decimals=3)[0]\n",
    "                # print(prediction)\n",
    "                # predictions.write(str(prediction.tolist()) + \"\\n\")\n",
    "                # predictions.write(str(prediction.tolist()))\n",
    "                # predictions.write(\"\\n\")\n",
    "                # print(f\"Predictions: {prediction}\")\n",
    "                # cpu = output.to('cpu')\n",
    "                # row.append(out)\n",
    "\n",
    "            else: \n",
    "                print(tree[\"precipitation\"][i])\n",
    "                s = pd.Series(tree[\"precipitation\"][i])\n",
    "                # predictions.write(str(s.tolist()[0]) + \"\\n\")\n",
    "                # row.append(tree[\"precipitation\"][i])\n",
    "\n",
    "    # tree.insert((len(tree.columns), \"cnn\"), row)\n",
    "    # predictions.close()\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training function \n",
    "def train(running_loss, epoch):\n",
    "    model.train()\n",
    "    for batch, (images, labels) in enumerate(train_loader): \n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad() \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if batch % log_interval == 0: \n",
    "                print('Epoch [{}/{}], Step [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch + 1, num_epochs, (batch * len(images)), len(train_loader.dataset),\n",
    "                100. * (batch / len(train_loader)), loss.item()))\n",
    "\n",
    "                # Save model and optimizer to continue training if necessary\n",
    "                torch.save(model.state_dict(), './model.pth')\n",
    "                torch.save(optimizer.state_dict(), './optimizer.pth')\n",
    "\n",
    "    # scheduler.step(running_loss)\n",
    "    return running_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Initial Test: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 20.0469 Accuracy: 30/863 (3.48%)\n",
      "\n",
      "Epoch [1/500], Step [0/3451 (0%)]\tLoss: 0.710945\n",
      "Epoch [1/500], Step [1024/3451 (30%)]\tLoss: 0.675502\n",
      "Epoch [1/500], Step [2048/3451 (59%)]\tLoss: 0.570789\n",
      "Epoch [1/500], Step [3072/3451 (89%)]\tLoss: 0.521292\n",
      "Epoch [1/500], Average Loss: 0.5796\n",
      "\n",
      "Test set: Avg. loss: 2.5169 Accuracy: 283/863 (32.79%)\n",
      "\n",
      "Epoch [2/500], Step [0/3451 (0%)]\tLoss: 0.463964\n",
      "Epoch [2/500], Step [1024/3451 (30%)]\tLoss: 0.441663\n",
      "Epoch [2/500], Step [2048/3451 (59%)]\tLoss: 0.590002\n",
      "Epoch [2/500], Step [3072/3451 (89%)]\tLoss: 0.549431\n",
      "Epoch [2/500], Average Loss: 0.5220\n",
      "Epoch [3/500], Step [0/3451 (0%)]\tLoss: 0.418037\n",
      "Epoch [3/500], Step [1024/3451 (30%)]\tLoss: 0.562911\n",
      "Epoch [3/500], Step [2048/3451 (59%)]\tLoss: 0.449747\n",
      "Epoch [3/500], Step [3072/3451 (89%)]\tLoss: 0.465348\n",
      "Epoch [3/500], Average Loss: 0.5114\n",
      "Epoch [4/500], Step [0/3451 (0%)]\tLoss: 0.528341\n",
      "Epoch [4/500], Step [1024/3451 (30%)]\tLoss: 0.426639\n",
      "Epoch [4/500], Step [2048/3451 (59%)]\tLoss: 0.505843\n",
      "Epoch [4/500], Step [3072/3451 (89%)]\tLoss: 0.466817\n",
      "Epoch [4/500], Average Loss: 0.5082\n",
      "Epoch [5/500], Step [0/3451 (0%)]\tLoss: 0.406455\n",
      "Epoch [5/500], Step [1024/3451 (30%)]\tLoss: 0.502146\n",
      "Epoch [5/500], Step [2048/3451 (59%)]\tLoss: 0.519379\n",
      "Epoch [5/500], Step [3072/3451 (89%)]\tLoss: 0.500356\n",
      "Epoch [5/500], Average Loss: 0.5094\n",
      "Epoch [6/500], Step [0/3451 (0%)]\tLoss: 0.497578\n",
      "Epoch [6/500], Step [1024/3451 (30%)]\tLoss: 0.455144\n",
      "Epoch [6/500], Step [2048/3451 (59%)]\tLoss: 0.384963\n",
      "Epoch [6/500], Step [3072/3451 (89%)]\tLoss: 0.498284\n",
      "Epoch [6/500], Average Loss: 0.4946\n",
      "\n",
      "Test set: Avg. loss: 2.0261 Accuracy: 322/863 (37.31%)\n",
      "\n",
      "Epoch [7/500], Step [0/3451 (0%)]\tLoss: 0.442273\n",
      "Epoch [7/500], Step [1024/3451 (30%)]\tLoss: 0.540330\n",
      "Epoch [7/500], Step [2048/3451 (59%)]\tLoss: 0.405060\n",
      "Epoch [7/500], Step [3072/3451 (89%)]\tLoss: 0.517919\n",
      "Epoch [7/500], Average Loss: 0.4926\n",
      "Epoch [8/500], Step [0/3451 (0%)]\tLoss: 0.463226\n",
      "Epoch [8/500], Step [1024/3451 (30%)]\tLoss: 0.501131\n",
      "Epoch [8/500], Step [2048/3451 (59%)]\tLoss: 0.523572\n",
      "Epoch [8/500], Step [3072/3451 (89%)]\tLoss: 0.477498\n",
      "Epoch [8/500], Average Loss: 0.4857\n",
      "Epoch [9/500], Step [0/3451 (0%)]\tLoss: 0.439691\n",
      "Epoch [9/500], Step [1024/3451 (30%)]\tLoss: 0.451111\n",
      "Epoch [9/500], Step [2048/3451 (59%)]\tLoss: 0.386836\n",
      "Epoch [9/500], Step [3072/3451 (89%)]\tLoss: 0.503377\n",
      "Epoch [9/500], Average Loss: 0.4767\n",
      "Epoch [10/500], Step [0/3451 (0%)]\tLoss: 0.440787\n",
      "Epoch [10/500], Step [1024/3451 (30%)]\tLoss: 0.483532\n",
      "Epoch [10/500], Step [2048/3451 (59%)]\tLoss: 0.514197\n",
      "Epoch [10/500], Step [3072/3451 (89%)]\tLoss: 0.445077\n",
      "Epoch [10/500], Average Loss: 0.4747\n",
      "Epoch [11/500], Step [0/3451 (0%)]\tLoss: 0.494878\n",
      "Epoch [11/500], Step [1024/3451 (30%)]\tLoss: 0.448107\n",
      "Epoch [11/500], Step [2048/3451 (59%)]\tLoss: 0.403851\n",
      "Epoch [11/500], Step [3072/3451 (89%)]\tLoss: 0.465506\n",
      "Epoch [11/500], Average Loss: 0.4768\n",
      "\n",
      "Test set: Avg. loss: 1.9579 Accuracy: 347/863 (40.21%)\n",
      "\n",
      "Epoch [12/500], Step [0/3451 (0%)]\tLoss: 0.422236\n",
      "Epoch [12/500], Step [1024/3451 (30%)]\tLoss: 0.474691\n",
      "Epoch [12/500], Step [2048/3451 (59%)]\tLoss: 0.436182\n",
      "Epoch [12/500], Step [3072/3451 (89%)]\tLoss: 0.514319\n",
      "Epoch [12/500], Average Loss: 0.4767\n",
      "Epoch [13/500], Step [0/3451 (0%)]\tLoss: 0.411069\n",
      "Epoch [13/500], Step [1024/3451 (30%)]\tLoss: 0.408869\n",
      "Epoch [13/500], Step [2048/3451 (59%)]\tLoss: 0.388638\n",
      "Epoch [13/500], Step [3072/3451 (89%)]\tLoss: 0.520768\n",
      "Epoch [13/500], Average Loss: 0.4697\n",
      "Epoch [14/500], Step [0/3451 (0%)]\tLoss: 0.435981\n",
      "Epoch [14/500], Step [1024/3451 (30%)]\tLoss: 0.433241\n",
      "Epoch [14/500], Step [2048/3451 (59%)]\tLoss: 0.441815\n",
      "Epoch [14/500], Step [3072/3451 (89%)]\tLoss: 0.407463\n",
      "Epoch [14/500], Average Loss: 0.4732\n",
      "Epoch [15/500], Step [0/3451 (0%)]\tLoss: 0.413477\n",
      "Epoch [15/500], Step [1024/3451 (30%)]\tLoss: 0.360491\n",
      "Epoch [15/500], Step [2048/3451 (59%)]\tLoss: 0.543677\n",
      "Epoch [15/500], Step [3072/3451 (89%)]\tLoss: 0.389259\n",
      "Epoch [15/500], Average Loss: 0.4676\n",
      "Epoch [16/500], Step [0/3451 (0%)]\tLoss: 0.540105\n",
      "Epoch [16/500], Step [1024/3451 (30%)]\tLoss: 0.396542\n",
      "Epoch [16/500], Step [2048/3451 (59%)]\tLoss: 0.491760\n",
      "Epoch [16/500], Step [3072/3451 (89%)]\tLoss: 0.434535\n",
      "Epoch [16/500], Average Loss: 0.4670\n",
      "\n",
      "Test set: Avg. loss: 1.8796 Accuracy: 365/863 (42.29%)\n",
      "\n",
      "Epoch [17/500], Step [0/3451 (0%)]\tLoss: 0.424247\n",
      "Epoch [17/500], Step [1024/3451 (30%)]\tLoss: 0.464663\n",
      "Epoch [17/500], Step [2048/3451 (59%)]\tLoss: 0.421703\n",
      "Epoch [17/500], Step [3072/3451 (89%)]\tLoss: 0.401476\n",
      "Epoch [17/500], Average Loss: 0.4687\n",
      "Epoch [18/500], Step [0/3451 (0%)]\tLoss: 0.530674\n",
      "Epoch [18/500], Step [1024/3451 (30%)]\tLoss: 0.559992\n",
      "Epoch [18/500], Step [2048/3451 (59%)]\tLoss: 0.367307\n",
      "Epoch [18/500], Step [3072/3451 (89%)]\tLoss: 0.438517\n",
      "Epoch [18/500], Average Loss: 0.4656\n",
      "Epoch [19/500], Step [0/3451 (0%)]\tLoss: 0.469263\n",
      "Epoch [19/500], Step [1024/3451 (30%)]\tLoss: 0.475783\n",
      "Epoch [19/500], Step [2048/3451 (59%)]\tLoss: 0.449594\n",
      "Epoch [19/500], Step [3072/3451 (89%)]\tLoss: 0.427811\n",
      "Epoch [19/500], Average Loss: 0.4659\n",
      "Epoch [20/500], Step [0/3451 (0%)]\tLoss: 0.488793\n",
      "Epoch [20/500], Step [1024/3451 (30%)]\tLoss: 0.367166\n",
      "Epoch [20/500], Step [2048/3451 (59%)]\tLoss: 0.478884\n",
      "Epoch [20/500], Step [3072/3451 (89%)]\tLoss: 0.408890\n",
      "Epoch [20/500], Average Loss: 0.4664\n",
      "Epoch [21/500], Step [0/3451 (0%)]\tLoss: 0.425801\n",
      "Epoch [21/500], Step [1024/3451 (30%)]\tLoss: 0.480529\n",
      "Epoch [21/500], Step [2048/3451 (59%)]\tLoss: 0.533379\n",
      "Epoch [21/500], Step [3072/3451 (89%)]\tLoss: 0.439414\n",
      "Epoch [21/500], Average Loss: 0.4598\n",
      "\n",
      "Test set: Avg. loss: 1.8832 Accuracy: 359/863 (41.60%)\n",
      "\n",
      "Epoch [22/500], Step [0/3451 (0%)]\tLoss: 0.457217\n",
      "Epoch [22/500], Step [1024/3451 (30%)]\tLoss: 0.494974\n",
      "Epoch [22/500], Step [2048/3451 (59%)]\tLoss: 0.487294\n",
      "Epoch [22/500], Step [3072/3451 (89%)]\tLoss: 0.445948\n",
      "Epoch [22/500], Average Loss: 0.4647\n",
      "Epoch [23/500], Step [0/3451 (0%)]\tLoss: 0.534666\n",
      "Epoch [23/500], Step [1024/3451 (30%)]\tLoss: 0.477134\n",
      "Epoch [23/500], Step [2048/3451 (59%)]\tLoss: 0.413078\n",
      "Epoch [23/500], Step [3072/3451 (89%)]\tLoss: 0.409623\n",
      "Epoch [23/500], Average Loss: 0.4595\n",
      "Epoch [24/500], Step [0/3451 (0%)]\tLoss: 0.443190\n",
      "Epoch [24/500], Step [1024/3451 (30%)]\tLoss: 0.563107\n",
      "Epoch [24/500], Step [2048/3451 (59%)]\tLoss: 0.489700\n",
      "Epoch [24/500], Step [3072/3451 (89%)]\tLoss: 0.357907\n",
      "Epoch [24/500], Average Loss: 0.4586\n",
      "Epoch [25/500], Step [0/3451 (0%)]\tLoss: 0.492715\n",
      "Epoch [25/500], Step [1024/3451 (30%)]\tLoss: 0.518086\n",
      "Epoch [25/500], Step [2048/3451 (59%)]\tLoss: 0.510691\n",
      "Epoch [25/500], Step [3072/3451 (89%)]\tLoss: 0.438321\n",
      "Epoch [25/500], Average Loss: 0.4595\n",
      "Epoch [26/500], Step [0/3451 (0%)]\tLoss: 0.494956\n",
      "Epoch [26/500], Step [1024/3451 (30%)]\tLoss: 0.412128\n",
      "Epoch [26/500], Step [2048/3451 (59%)]\tLoss: 0.441300\n",
      "Epoch [26/500], Step [3072/3451 (89%)]\tLoss: 0.406114\n",
      "Epoch [26/500], Average Loss: 0.4558\n",
      "\n",
      "Test set: Avg. loss: 1.8739 Accuracy: 382/863 (44.26%)\n",
      "\n",
      "Epoch [27/500], Step [0/3451 (0%)]\tLoss: 0.465199\n",
      "Epoch [27/500], Step [1024/3451 (30%)]\tLoss: 0.454494\n",
      "Epoch [27/500], Step [2048/3451 (59%)]\tLoss: 0.466760\n",
      "Epoch [27/500], Step [3072/3451 (89%)]\tLoss: 0.443542\n",
      "Epoch [27/500], Average Loss: 0.4563\n",
      "Epoch [28/500], Step [0/3451 (0%)]\tLoss: 0.462645\n",
      "Epoch [28/500], Step [1024/3451 (30%)]\tLoss: 0.417182\n",
      "Epoch [28/500], Step [2048/3451 (59%)]\tLoss: 0.342312\n",
      "Epoch [28/500], Step [3072/3451 (89%)]\tLoss: 0.461663\n",
      "Epoch [28/500], Average Loss: 0.4557\n",
      "Epoch [29/500], Step [0/3451 (0%)]\tLoss: 0.432048\n",
      "Epoch [29/500], Step [1024/3451 (30%)]\tLoss: 0.387578\n",
      "Epoch [29/500], Step [2048/3451 (59%)]\tLoss: 0.510291\n",
      "Epoch [29/500], Step [3072/3451 (89%)]\tLoss: 0.388985\n",
      "Epoch [29/500], Average Loss: 0.4573\n",
      "Epoch [30/500], Step [0/3451 (0%)]\tLoss: 0.473991\n",
      "Epoch [30/500], Step [1024/3451 (30%)]\tLoss: 0.466544\n",
      "Epoch [30/500], Step [2048/3451 (59%)]\tLoss: 0.448776\n",
      "Epoch [30/500], Step [3072/3451 (89%)]\tLoss: 0.531806\n",
      "Epoch [30/500], Average Loss: 0.4531\n",
      "Epoch [31/500], Step [0/3451 (0%)]\tLoss: 0.421197\n",
      "Epoch [31/500], Step [1024/3451 (30%)]\tLoss: 0.500414\n",
      "Epoch [31/500], Step [2048/3451 (59%)]\tLoss: 0.459971\n",
      "Epoch [31/500], Step [3072/3451 (89%)]\tLoss: 0.498549\n",
      "Epoch [31/500], Average Loss: 0.4546\n",
      "\n",
      "Test set: Avg. loss: 1.8448 Accuracy: 367/863 (42.53%)\n",
      "\n",
      "Epoch [32/500], Step [0/3451 (0%)]\tLoss: 0.376755\n",
      "Epoch [32/500], Step [1024/3451 (30%)]\tLoss: 0.439380\n",
      "Epoch [32/500], Step [2048/3451 (59%)]\tLoss: 0.399017\n",
      "Epoch [32/500], Step [3072/3451 (89%)]\tLoss: 0.571929\n",
      "Epoch [32/500], Average Loss: 0.4513\n",
      "Epoch [33/500], Step [0/3451 (0%)]\tLoss: 0.516505\n",
      "Epoch [33/500], Step [1024/3451 (30%)]\tLoss: 0.498899\n",
      "Epoch [33/500], Step [2048/3451 (59%)]\tLoss: 0.407532\n",
      "Epoch [33/500], Step [3072/3451 (89%)]\tLoss: 0.444146\n",
      "Epoch [33/500], Average Loss: 0.4495\n",
      "Epoch [34/500], Step [0/3451 (0%)]\tLoss: 0.547309\n",
      "Epoch [34/500], Step [1024/3451 (30%)]\tLoss: 0.518742\n",
      "Epoch [34/500], Step [2048/3451 (59%)]\tLoss: 0.411625\n",
      "Epoch [34/500], Step [3072/3451 (89%)]\tLoss: 0.479876\n",
      "Epoch [34/500], Average Loss: 0.4533\n",
      "Epoch [35/500], Step [0/3451 (0%)]\tLoss: 0.394183\n",
      "Epoch [35/500], Step [1024/3451 (30%)]\tLoss: 0.498789\n",
      "Epoch [35/500], Step [2048/3451 (59%)]\tLoss: 0.428921\n",
      "Epoch [35/500], Step [3072/3451 (89%)]\tLoss: 0.565570\n",
      "Epoch [35/500], Average Loss: 0.4532\n",
      "Epoch [36/500], Step [0/3451 (0%)]\tLoss: 0.408337\n",
      "Epoch [36/500], Step [1024/3451 (30%)]\tLoss: 0.396736\n",
      "Epoch [36/500], Step [2048/3451 (59%)]\tLoss: 0.461512\n",
      "Epoch [36/500], Step [3072/3451 (89%)]\tLoss: 0.539453\n",
      "Epoch [36/500], Average Loss: 0.4492\n",
      "\n",
      "Test set: Avg. loss: 1.8521 Accuracy: 393/863 (45.54%)\n",
      "\n",
      "Epoch [37/500], Step [0/3451 (0%)]\tLoss: 0.540314\n",
      "Epoch [37/500], Step [1024/3451 (30%)]\tLoss: 0.530404\n",
      "Epoch [37/500], Step [2048/3451 (59%)]\tLoss: 0.554018\n",
      "Epoch [37/500], Step [3072/3451 (89%)]\tLoss: 0.528492\n",
      "Epoch [37/500], Average Loss: 0.4509\n",
      "Epoch [38/500], Step [0/3451 (0%)]\tLoss: 0.461622\n",
      "Epoch [38/500], Step [1024/3451 (30%)]\tLoss: 0.518431\n",
      "Epoch [38/500], Step [2048/3451 (59%)]\tLoss: 0.445344\n",
      "Epoch [38/500], Step [3072/3451 (89%)]\tLoss: 0.434813\n",
      "Epoch [38/500], Average Loss: 0.4460\n",
      "Epoch [39/500], Step [0/3451 (0%)]\tLoss: 0.511607\n",
      "Epoch [39/500], Step [1024/3451 (30%)]\tLoss: 0.524252\n",
      "Epoch [39/500], Step [2048/3451 (59%)]\tLoss: 0.419896\n",
      "Epoch [39/500], Step [3072/3451 (89%)]\tLoss: 0.420881\n",
      "Epoch [39/500], Average Loss: 0.4473\n",
      "Epoch [40/500], Step [0/3451 (0%)]\tLoss: 0.408597\n",
      "Epoch [40/500], Step [1024/3451 (30%)]\tLoss: 0.424940\n",
      "Epoch [40/500], Step [2048/3451 (59%)]\tLoss: 0.446174\n",
      "Epoch [40/500], Step [3072/3451 (89%)]\tLoss: 0.400047\n",
      "Epoch [40/500], Average Loss: 0.4422\n",
      "Epoch [41/500], Step [0/3451 (0%)]\tLoss: 0.359170\n",
      "Epoch [41/500], Step [1024/3451 (30%)]\tLoss: 0.492230\n",
      "Epoch [41/500], Step [2048/3451 (59%)]\tLoss: 0.455096\n",
      "Epoch [41/500], Step [3072/3451 (89%)]\tLoss: 0.424339\n",
      "Epoch [41/500], Average Loss: 0.4408\n",
      "\n",
      "Test set: Avg. loss: 1.8718 Accuracy: 362/863 (41.95%)\n",
      "\n",
      "Epoch [42/500], Step [0/3451 (0%)]\tLoss: 0.393424\n",
      "Epoch [42/500], Step [1024/3451 (30%)]\tLoss: 0.423593\n",
      "Epoch [42/500], Step [2048/3451 (59%)]\tLoss: 0.478681\n",
      "Epoch [42/500], Step [3072/3451 (89%)]\tLoss: 0.360022\n",
      "Epoch [42/500], Average Loss: 0.4377\n",
      "Epoch [43/500], Step [0/3451 (0%)]\tLoss: 0.380962\n",
      "Epoch [43/500], Step [1024/3451 (30%)]\tLoss: 0.383763\n",
      "Epoch [43/500], Step [2048/3451 (59%)]\tLoss: 0.429595\n",
      "Epoch [43/500], Step [3072/3451 (89%)]\tLoss: 0.487008\n",
      "Epoch [43/500], Average Loss: 0.4392\n",
      "Epoch [44/500], Step [0/3451 (0%)]\tLoss: 0.412691\n",
      "Epoch [44/500], Step [1024/3451 (30%)]\tLoss: 0.446699\n",
      "Epoch [44/500], Step [2048/3451 (59%)]\tLoss: 0.424634\n",
      "Epoch [44/500], Step [3072/3451 (89%)]\tLoss: 0.583858\n",
      "Epoch [44/500], Average Loss: 0.4391\n",
      "Epoch [45/500], Step [0/3451 (0%)]\tLoss: 0.581877\n",
      "Epoch [45/500], Step [1024/3451 (30%)]\tLoss: 0.390354\n",
      "Epoch [45/500], Step [2048/3451 (59%)]\tLoss: 0.449246\n",
      "Epoch [45/500], Step [3072/3451 (89%)]\tLoss: 0.461671\n",
      "Epoch [45/500], Average Loss: 0.4355\n",
      "Epoch [46/500], Step [0/3451 (0%)]\tLoss: 0.409053\n",
      "Epoch [46/500], Step [1024/3451 (30%)]\tLoss: 0.410540\n",
      "Epoch [46/500], Step [2048/3451 (59%)]\tLoss: 0.398407\n",
      "Epoch [46/500], Step [3072/3451 (89%)]\tLoss: 0.409524\n",
      "Epoch [46/500], Average Loss: 0.4352\n",
      "\n",
      "Test set: Avg. loss: 1.8281 Accuracy: 355/863 (41.14%)\n",
      "\n",
      "Epoch [47/500], Step [0/3451 (0%)]\tLoss: 0.407641\n",
      "Epoch [47/500], Step [1024/3451 (30%)]\tLoss: 0.499379\n",
      "Epoch [47/500], Step [2048/3451 (59%)]\tLoss: 0.432117\n",
      "Epoch [47/500], Step [3072/3451 (89%)]\tLoss: 0.434472\n",
      "Epoch [47/500], Average Loss: 0.4307\n",
      "Epoch [48/500], Step [0/3451 (0%)]\tLoss: 0.354439\n",
      "Epoch [48/500], Step [1024/3451 (30%)]\tLoss: 0.558227\n",
      "Epoch [48/500], Step [2048/3451 (59%)]\tLoss: 0.494247\n",
      "Epoch [48/500], Step [3072/3451 (89%)]\tLoss: 0.420697\n",
      "Epoch [48/500], Average Loss: 0.4336\n",
      "Epoch [49/500], Step [0/3451 (0%)]\tLoss: 0.325708\n",
      "Epoch [49/500], Step [1024/3451 (30%)]\tLoss: 0.472488\n",
      "Epoch [49/500], Step [2048/3451 (59%)]\tLoss: 0.475998\n",
      "Epoch [49/500], Step [3072/3451 (89%)]\tLoss: 0.390859\n",
      "Epoch [49/500], Average Loss: 0.4335\n",
      "Epoch [50/500], Step [0/3451 (0%)]\tLoss: 0.339186\n",
      "Epoch [50/500], Step [1024/3451 (30%)]\tLoss: 0.397840\n",
      "Epoch [50/500], Step [2048/3451 (59%)]\tLoss: 0.359126\n",
      "Epoch [50/500], Step [3072/3451 (89%)]\tLoss: 0.476924\n",
      "Epoch [50/500], Average Loss: 0.4288\n",
      "Epoch [51/500], Step [0/3451 (0%)]\tLoss: 0.366559\n",
      "Epoch [51/500], Step [1024/3451 (30%)]\tLoss: 0.421392\n",
      "Epoch [51/500], Step [2048/3451 (59%)]\tLoss: 0.473110\n",
      "Epoch [51/500], Step [3072/3451 (89%)]\tLoss: 0.424928\n",
      "Epoch [51/500], Average Loss: 0.4240\n",
      "\n",
      "Test set: Avg. loss: 1.8264 Accuracy: 364/863 (42.18%)\n",
      "\n",
      "Epoch [52/500], Step [0/3451 (0%)]\tLoss: 0.377186\n",
      "Epoch [52/500], Step [1024/3451 (30%)]\tLoss: 0.508988\n",
      "Epoch [52/500], Step [2048/3451 (59%)]\tLoss: 0.404663\n",
      "Epoch [52/500], Step [3072/3451 (89%)]\tLoss: 0.406606\n",
      "Epoch [52/500], Average Loss: 0.4265\n",
      "Epoch [53/500], Step [0/3451 (0%)]\tLoss: 0.415539\n",
      "Epoch [53/500], Step [1024/3451 (30%)]\tLoss: 0.402475\n",
      "Epoch [53/500], Step [2048/3451 (59%)]\tLoss: 0.475379\n",
      "Epoch [53/500], Step [3072/3451 (89%)]\tLoss: 0.455020\n",
      "Epoch [53/500], Average Loss: 0.4237\n",
      "Epoch [54/500], Step [0/3451 (0%)]\tLoss: 0.440522\n",
      "Epoch [54/500], Step [1024/3451 (30%)]\tLoss: 0.415675\n",
      "Epoch [54/500], Step [2048/3451 (59%)]\tLoss: 0.426146\n",
      "Epoch [54/500], Step [3072/3451 (89%)]\tLoss: 0.513290\n",
      "Epoch [54/500], Average Loss: 0.4184\n",
      "Epoch [55/500], Step [0/3451 (0%)]\tLoss: 0.368509\n",
      "Epoch [55/500], Step [1024/3451 (30%)]\tLoss: 0.411762\n",
      "Epoch [55/500], Step [2048/3451 (59%)]\tLoss: 0.380054\n",
      "Epoch [55/500], Step [3072/3451 (89%)]\tLoss: 0.551466\n",
      "Epoch [55/500], Average Loss: 0.4213\n",
      "Epoch [56/500], Step [0/3451 (0%)]\tLoss: 0.360540\n",
      "Epoch [56/500], Step [1024/3451 (30%)]\tLoss: 0.406231\n",
      "Epoch [56/500], Step [2048/3451 (59%)]\tLoss: 0.422065\n",
      "Epoch [56/500], Step [3072/3451 (89%)]\tLoss: 0.479172\n",
      "Epoch [56/500], Average Loss: 0.4170\n",
      "\n",
      "Test set: Avg. loss: 1.8523 Accuracy: 367/863 (42.53%)\n",
      "\n",
      "Epoch [57/500], Step [0/3451 (0%)]\tLoss: 0.401762\n",
      "Epoch [57/500], Step [1024/3451 (30%)]\tLoss: 0.501352\n",
      "Epoch [57/500], Step [2048/3451 (59%)]\tLoss: 0.441101\n",
      "Epoch [57/500], Step [3072/3451 (89%)]\tLoss: 0.407829\n",
      "Epoch [57/500], Average Loss: 0.4181\n",
      "Epoch [58/500], Step [0/3451 (0%)]\tLoss: 0.416546\n",
      "Epoch [58/500], Step [1024/3451 (30%)]\tLoss: 0.313039\n",
      "Epoch [58/500], Step [2048/3451 (59%)]\tLoss: 0.389648\n",
      "Epoch [58/500], Step [3072/3451 (89%)]\tLoss: 0.436533\n",
      "Epoch [58/500], Average Loss: 0.4128\n",
      "Epoch [59/500], Step [0/3451 (0%)]\tLoss: 0.422620\n",
      "Epoch [59/500], Step [1024/3451 (30%)]\tLoss: 0.473173\n",
      "Epoch [59/500], Step [2048/3451 (59%)]\tLoss: 0.465410\n",
      "Epoch [59/500], Step [3072/3451 (89%)]\tLoss: 0.433388\n",
      "Epoch [59/500], Average Loss: 0.4086\n",
      "Epoch [60/500], Step [0/3451 (0%)]\tLoss: 0.352274\n",
      "Epoch [60/500], Step [1024/3451 (30%)]\tLoss: 0.407258\n",
      "Epoch [60/500], Step [2048/3451 (59%)]\tLoss: 0.411263\n",
      "Epoch [60/500], Step [3072/3451 (89%)]\tLoss: 0.375625\n",
      "Epoch [60/500], Average Loss: 0.4080\n",
      "Epoch [61/500], Step [0/3451 (0%)]\tLoss: 0.343200\n",
      "Epoch [61/500], Step [1024/3451 (30%)]\tLoss: 0.287333\n",
      "Epoch [61/500], Step [2048/3451 (59%)]\tLoss: 0.430372\n",
      "Epoch [61/500], Step [3072/3451 (89%)]\tLoss: 0.482509\n",
      "Epoch [61/500], Average Loss: 0.4052\n",
      "\n",
      "Test set: Avg. loss: 1.8631 Accuracy: 366/863 (42.41%)\n",
      "\n",
      "Epoch [62/500], Step [0/3451 (0%)]\tLoss: 0.448202\n",
      "Epoch [62/500], Step [1024/3451 (30%)]\tLoss: 0.409520\n",
      "Epoch [62/500], Step [2048/3451 (59%)]\tLoss: 0.534587\n",
      "Epoch [62/500], Step [3072/3451 (89%)]\tLoss: 0.388699\n",
      "Epoch [62/500], Average Loss: 0.4053\n",
      "Epoch [63/500], Step [0/3451 (0%)]\tLoss: 0.351445\n",
      "Epoch [63/500], Step [1024/3451 (30%)]\tLoss: 0.361126\n",
      "Epoch [63/500], Step [2048/3451 (59%)]\tLoss: 0.348199\n",
      "Epoch [63/500], Step [3072/3451 (89%)]\tLoss: 0.378988\n",
      "Epoch [63/500], Average Loss: 0.3955\n",
      "Epoch [64/500], Step [0/3451 (0%)]\tLoss: 0.421417\n",
      "Epoch [64/500], Step [1024/3451 (30%)]\tLoss: 0.427309\n",
      "Epoch [64/500], Step [2048/3451 (59%)]\tLoss: 0.498600\n",
      "Epoch [64/500], Step [3072/3451 (89%)]\tLoss: 0.457833\n",
      "Epoch [64/500], Average Loss: 0.3983\n",
      "Epoch [65/500], Step [0/3451 (0%)]\tLoss: 0.355084\n",
      "Epoch [65/500], Step [1024/3451 (30%)]\tLoss: 0.412084\n",
      "Epoch [65/500], Step [2048/3451 (59%)]\tLoss: 0.301031\n",
      "Epoch [65/500], Step [3072/3451 (89%)]\tLoss: 0.391584\n",
      "Epoch [65/500], Average Loss: 0.3893\n",
      "Epoch [66/500], Step [0/3451 (0%)]\tLoss: 0.360000\n",
      "Epoch [66/500], Step [1024/3451 (30%)]\tLoss: 0.352370\n",
      "Epoch [66/500], Step [2048/3451 (59%)]\tLoss: 0.410965\n",
      "Epoch [66/500], Step [3072/3451 (89%)]\tLoss: 0.385220\n",
      "Epoch [66/500], Average Loss: 0.3911\n",
      "\n",
      "Test set: Avg. loss: 1.9303 Accuracy: 345/863 (39.98%)\n",
      "\n",
      "Epoch [67/500], Step [0/3451 (0%)]\tLoss: 0.376743\n",
      "Epoch [67/500], Step [1024/3451 (30%)]\tLoss: 0.374043\n",
      "Epoch [67/500], Step [2048/3451 (59%)]\tLoss: 0.372744\n",
      "Epoch [67/500], Step [3072/3451 (89%)]\tLoss: 0.354322\n",
      "Epoch [67/500], Average Loss: 0.3815\n",
      "Epoch [68/500], Step [0/3451 (0%)]\tLoss: 0.404317\n",
      "Epoch [68/500], Step [1024/3451 (30%)]\tLoss: 0.267143\n",
      "Epoch [68/500], Step [2048/3451 (59%)]\tLoss: 0.440642\n",
      "Epoch [68/500], Step [3072/3451 (89%)]\tLoss: 0.374803\n",
      "Epoch [68/500], Average Loss: 0.3765\n",
      "Epoch [69/500], Step [0/3451 (0%)]\tLoss: 0.379162\n",
      "Epoch [69/500], Step [1024/3451 (30%)]\tLoss: 0.493732\n",
      "Epoch [69/500], Step [2048/3451 (59%)]\tLoss: 0.398218\n",
      "Epoch [69/500], Step [3072/3451 (89%)]\tLoss: 0.421796\n",
      "Epoch [69/500], Average Loss: 0.3840\n",
      "Epoch [70/500], Step [0/3451 (0%)]\tLoss: 0.305679\n",
      "Epoch [70/500], Step [1024/3451 (30%)]\tLoss: 0.342559\n",
      "Epoch [70/500], Step [2048/3451 (59%)]\tLoss: 0.398430\n",
      "Epoch [70/500], Step [3072/3451 (89%)]\tLoss: 0.420977\n",
      "Epoch [70/500], Average Loss: 0.3727\n",
      "Epoch [71/500], Step [0/3451 (0%)]\tLoss: 0.314044\n",
      "Epoch [71/500], Step [1024/3451 (30%)]\tLoss: 0.434940\n",
      "Epoch [71/500], Step [2048/3451 (59%)]\tLoss: 0.337727\n",
      "Epoch [71/500], Step [3072/3451 (89%)]\tLoss: 0.407582\n",
      "Epoch [71/500], Average Loss: 0.3656\n",
      "\n",
      "Test set: Avg. loss: 1.9700 Accuracy: 356/863 (41.25%)\n",
      "\n",
      "Epoch [72/500], Step [0/3451 (0%)]\tLoss: 0.241564\n",
      "Epoch [72/500], Step [1024/3451 (30%)]\tLoss: 0.304037\n",
      "Epoch [72/500], Step [2048/3451 (59%)]\tLoss: 0.485811\n",
      "Epoch [72/500], Step [3072/3451 (89%)]\tLoss: 0.416213\n",
      "Epoch [72/500], Average Loss: 0.3600\n",
      "Epoch [73/500], Step [0/3451 (0%)]\tLoss: 0.336615\n",
      "Epoch [73/500], Step [1024/3451 (30%)]\tLoss: 0.362539\n",
      "Epoch [73/500], Step [2048/3451 (59%)]\tLoss: 0.403586\n",
      "Epoch [73/500], Step [3072/3451 (89%)]\tLoss: 0.335651\n",
      "Epoch [73/500], Average Loss: 0.3575\n",
      "Epoch [74/500], Step [0/3451 (0%)]\tLoss: 0.341299\n",
      "Epoch [74/500], Step [1024/3451 (30%)]\tLoss: 0.390967\n",
      "Epoch [74/500], Step [2048/3451 (59%)]\tLoss: 0.309382\n",
      "Epoch [74/500], Step [3072/3451 (89%)]\tLoss: 0.384377\n",
      "Epoch [74/500], Average Loss: 0.3520\n",
      "Epoch [75/500], Step [0/3451 (0%)]\tLoss: 0.307247\n",
      "Epoch [75/500], Step [1024/3451 (30%)]\tLoss: 0.313506\n",
      "Epoch [75/500], Step [2048/3451 (59%)]\tLoss: 0.326667\n",
      "Epoch [75/500], Step [3072/3451 (89%)]\tLoss: 0.344307\n",
      "Epoch [75/500], Average Loss: 0.3486\n",
      "Epoch [76/500], Step [0/3451 (0%)]\tLoss: 0.390231\n",
      "Epoch [76/500], Step [1024/3451 (30%)]\tLoss: 0.257514\n",
      "Epoch [76/500], Step [2048/3451 (59%)]\tLoss: 0.431219\n",
      "Epoch [76/500], Step [3072/3451 (89%)]\tLoss: 0.450082\n",
      "Epoch [76/500], Average Loss: 0.3449\n",
      "\n",
      "Test set: Avg. loss: 2.0405 Accuracy: 356/863 (41.25%)\n",
      "\n",
      "Epoch [77/500], Step [0/3451 (0%)]\tLoss: 0.321616\n",
      "Epoch [77/500], Step [1024/3451 (30%)]\tLoss: 0.347079\n",
      "Epoch [77/500], Step [2048/3451 (59%)]\tLoss: 0.395659\n",
      "Epoch [77/500], Step [3072/3451 (89%)]\tLoss: 0.338609\n",
      "Epoch [77/500], Average Loss: 0.3391\n",
      "Epoch [78/500], Step [0/3451 (0%)]\tLoss: 0.319374\n",
      "Epoch [78/500], Step [1024/3451 (30%)]\tLoss: 0.332666\n",
      "Epoch [78/500], Step [2048/3451 (59%)]\tLoss: 0.425845\n",
      "Epoch [78/500], Step [3072/3451 (89%)]\tLoss: 0.304455\n",
      "Epoch [78/500], Average Loss: 0.3396\n",
      "Epoch [79/500], Step [0/3451 (0%)]\tLoss: 0.421403\n",
      "Epoch [79/500], Step [1024/3451 (30%)]\tLoss: 0.313950\n",
      "Epoch [79/500], Step [2048/3451 (59%)]\tLoss: 0.442845\n",
      "Epoch [79/500], Step [3072/3451 (89%)]\tLoss: 0.333141\n",
      "Epoch [79/500], Average Loss: 0.3316\n",
      "Epoch [80/500], Step [0/3451 (0%)]\tLoss: 0.291474\n",
      "Epoch [80/500], Step [1024/3451 (30%)]\tLoss: 0.285784\n",
      "Epoch [80/500], Step [2048/3451 (59%)]\tLoss: 0.252384\n",
      "Epoch [80/500], Step [3072/3451 (89%)]\tLoss: 0.409800\n",
      "Epoch [80/500], Average Loss: 0.3219\n",
      "Epoch [81/500], Step [0/3451 (0%)]\tLoss: 0.250875\n",
      "Epoch [81/500], Step [1024/3451 (30%)]\tLoss: 0.313832\n",
      "Epoch [81/500], Step [2048/3451 (59%)]\tLoss: 0.333482\n",
      "Epoch [81/500], Step [3072/3451 (89%)]\tLoss: 0.345676\n",
      "Epoch [81/500], Average Loss: 0.3227\n",
      "\n",
      "Test set: Avg. loss: 2.1783 Accuracy: 342/863 (39.63%)\n",
      "\n",
      "Epoch [82/500], Step [0/3451 (0%)]\tLoss: 0.206495\n",
      "Epoch [82/500], Step [1024/3451 (30%)]\tLoss: 0.292713\n",
      "Epoch [82/500], Step [2048/3451 (59%)]\tLoss: 0.321996\n",
      "Epoch [82/500], Step [3072/3451 (89%)]\tLoss: 0.335407\n",
      "Epoch [82/500], Average Loss: 0.3107\n",
      "Epoch [83/500], Step [0/3451 (0%)]\tLoss: 0.308194\n",
      "Epoch [83/500], Step [1024/3451 (30%)]\tLoss: 0.318964\n",
      "Epoch [83/500], Step [2048/3451 (59%)]\tLoss: 0.230511\n",
      "Epoch [83/500], Step [3072/3451 (89%)]\tLoss: 0.339922\n",
      "Epoch [83/500], Average Loss: 0.3026\n",
      "Epoch [84/500], Step [0/3451 (0%)]\tLoss: 0.304986\n",
      "Epoch [84/500], Step [1024/3451 (30%)]\tLoss: 0.259333\n",
      "Epoch [84/500], Step [2048/3451 (59%)]\tLoss: 0.330373\n",
      "Epoch [84/500], Step [3072/3451 (89%)]\tLoss: 0.361608\n",
      "Epoch [84/500], Average Loss: 0.2967\n",
      "Epoch [85/500], Step [0/3451 (0%)]\tLoss: 0.312790\n",
      "Epoch [85/500], Step [1024/3451 (30%)]\tLoss: 0.340386\n",
      "Epoch [85/500], Step [2048/3451 (59%)]\tLoss: 0.373873\n",
      "Epoch [85/500], Step [3072/3451 (89%)]\tLoss: 0.336258\n",
      "Epoch [85/500], Average Loss: 0.2918\n",
      "Epoch [86/500], Step [0/3451 (0%)]\tLoss: 0.302270\n",
      "Epoch [86/500], Step [1024/3451 (30%)]\tLoss: 0.243310\n",
      "Epoch [86/500], Step [2048/3451 (59%)]\tLoss: 0.230736\n",
      "Epoch [86/500], Step [3072/3451 (89%)]\tLoss: 0.252156\n",
      "Epoch [86/500], Average Loss: 0.2805\n",
      "\n",
      "Test set: Avg. loss: 2.3730 Accuracy: 337/863 (39.05%)\n",
      "\n",
      "Epoch [87/500], Step [0/3451 (0%)]\tLoss: 0.250478\n",
      "Epoch [87/500], Step [1024/3451 (30%)]\tLoss: 0.319482\n",
      "Epoch [87/500], Step [2048/3451 (59%)]\tLoss: 0.293971\n",
      "Epoch [87/500], Step [3072/3451 (89%)]\tLoss: 0.295170\n",
      "Epoch [87/500], Average Loss: 0.2844\n",
      "Epoch [88/500], Step [0/3451 (0%)]\tLoss: 0.193915\n",
      "Epoch [88/500], Step [1024/3451 (30%)]\tLoss: 0.233365\n",
      "Epoch [88/500], Step [2048/3451 (59%)]\tLoss: 0.263853\n",
      "Epoch [88/500], Step [3072/3451 (89%)]\tLoss: 0.269170\n",
      "Epoch [88/500], Average Loss: 0.2792\n",
      "Epoch [89/500], Step [0/3451 (0%)]\tLoss: 0.316484\n",
      "Epoch [89/500], Step [1024/3451 (30%)]\tLoss: 0.248477\n",
      "Epoch [89/500], Step [2048/3451 (59%)]\tLoss: 0.258517\n",
      "Epoch [89/500], Step [3072/3451 (89%)]\tLoss: 0.276314\n",
      "Epoch [89/500], Average Loss: 0.2687\n",
      "Epoch [90/500], Step [0/3451 (0%)]\tLoss: 0.244085\n",
      "Epoch [90/500], Step [1024/3451 (30%)]\tLoss: 0.428641\n",
      "Epoch [90/500], Step [2048/3451 (59%)]\tLoss: 0.190608\n",
      "Epoch [90/500], Step [3072/3451 (89%)]\tLoss: 0.306785\n",
      "Epoch [90/500], Average Loss: 0.2719\n",
      "Epoch [91/500], Step [0/3451 (0%)]\tLoss: 0.199554\n",
      "Epoch [91/500], Step [1024/3451 (30%)]\tLoss: 0.340298\n",
      "Epoch [91/500], Step [2048/3451 (59%)]\tLoss: 0.279829\n",
      "Epoch [91/500], Step [3072/3451 (89%)]\tLoss: 0.307518\n",
      "Epoch [91/500], Average Loss: 0.2594\n",
      "\n",
      "Test set: Avg. loss: 2.4753 Accuracy: 335/863 (38.82%)\n",
      "\n",
      "Epoch [92/500], Step [0/3451 (0%)]\tLoss: 0.337160\n",
      "Epoch [92/500], Step [1024/3451 (30%)]\tLoss: 0.293268\n",
      "Epoch [92/500], Step [2048/3451 (59%)]\tLoss: 0.220174\n",
      "Epoch [92/500], Step [3072/3451 (89%)]\tLoss: 0.250253\n",
      "Epoch [92/500], Average Loss: 0.2515\n",
      "Epoch [93/500], Step [0/3451 (0%)]\tLoss: 0.251411\n",
      "Epoch [93/500], Step [1024/3451 (30%)]\tLoss: 0.202800\n",
      "Epoch [93/500], Step [2048/3451 (59%)]\tLoss: 0.209193\n",
      "Epoch [93/500], Step [3072/3451 (89%)]\tLoss: 0.257848\n",
      "Epoch [93/500], Average Loss: 0.2386\n",
      "Epoch [94/500], Step [0/3451 (0%)]\tLoss: 0.257337\n",
      "Epoch [94/500], Step [1024/3451 (30%)]\tLoss: 0.326805\n",
      "Epoch [94/500], Step [2048/3451 (59%)]\tLoss: 0.200959\n",
      "Epoch [94/500], Step [3072/3451 (89%)]\tLoss: 0.284751\n",
      "Epoch [94/500], Average Loss: 0.2411\n",
      "Epoch [95/500], Step [0/3451 (0%)]\tLoss: 0.231796\n",
      "Epoch [95/500], Step [1024/3451 (30%)]\tLoss: 0.389757\n",
      "Epoch [95/500], Step [2048/3451 (59%)]\tLoss: 0.303956\n",
      "Epoch [95/500], Step [3072/3451 (89%)]\tLoss: 0.271110\n",
      "Epoch [95/500], Average Loss: 0.2319\n",
      "Epoch [96/500], Step [0/3451 (0%)]\tLoss: 0.181521\n",
      "Epoch [96/500], Step [1024/3451 (30%)]\tLoss: 0.247570\n",
      "Epoch [96/500], Step [2048/3451 (59%)]\tLoss: 0.200151\n",
      "Epoch [96/500], Step [3072/3451 (89%)]\tLoss: 0.191999\n",
      "Epoch [96/500], Average Loss: 0.2268\n",
      "\n",
      "Test set: Avg. loss: 2.6788 Accuracy: 324/863 (37.54%)\n",
      "\n",
      "Epoch [97/500], Step [0/3451 (0%)]\tLoss: 0.176233\n",
      "Epoch [97/500], Step [1024/3451 (30%)]\tLoss: 0.166411\n",
      "Epoch [97/500], Step [2048/3451 (59%)]\tLoss: 0.240946\n",
      "Epoch [97/500], Step [3072/3451 (89%)]\tLoss: 0.195735\n",
      "Epoch [97/500], Average Loss: 0.2164\n",
      "Epoch [98/500], Step [0/3451 (0%)]\tLoss: 0.166820\n",
      "Epoch [98/500], Step [1024/3451 (30%)]\tLoss: 0.218161\n",
      "Epoch [98/500], Step [2048/3451 (59%)]\tLoss: 0.169988\n",
      "Epoch [98/500], Step [3072/3451 (89%)]\tLoss: 0.156441\n",
      "Epoch [98/500], Average Loss: 0.2057\n",
      "Epoch [99/500], Step [0/3451 (0%)]\tLoss: 0.118762\n",
      "Epoch [99/500], Step [1024/3451 (30%)]\tLoss: 0.122108\n",
      "Epoch [99/500], Step [2048/3451 (59%)]\tLoss: 0.152645\n",
      "Epoch [99/500], Step [3072/3451 (89%)]\tLoss: 0.164938\n",
      "Epoch [99/500], Average Loss: 0.2098\n",
      "Epoch [100/500], Step [0/3451 (0%)]\tLoss: 0.234727\n",
      "Epoch [100/500], Step [1024/3451 (30%)]\tLoss: 0.251740\n",
      "Epoch [100/500], Step [2048/3451 (59%)]\tLoss: 0.240152\n",
      "Epoch [100/500], Step [3072/3451 (89%)]\tLoss: 0.338576\n",
      "Epoch [100/500], Average Loss: 0.2032\n",
      "Epoch [101/500], Step [0/3451 (0%)]\tLoss: 0.176742\n",
      "Epoch [101/500], Step [1024/3451 (30%)]\tLoss: 0.180778\n",
      "Epoch [101/500], Step [2048/3451 (59%)]\tLoss: 0.326704\n",
      "Epoch [101/500], Step [3072/3451 (89%)]\tLoss: 0.201589\n",
      "Epoch [101/500], Average Loss: 0.1958\n",
      "\n",
      "Test set: Avg. loss: 3.3076 Accuracy: 291/863 (33.72%)\n",
      "\n",
      "Epoch [102/500], Step [0/3451 (0%)]\tLoss: 0.255603\n",
      "Epoch [102/500], Step [1024/3451 (30%)]\tLoss: 0.105232\n",
      "Epoch [102/500], Step [2048/3451 (59%)]\tLoss: 0.209577\n",
      "Epoch [102/500], Step [3072/3451 (89%)]\tLoss: 0.346748\n",
      "Epoch [102/500], Average Loss: 0.1916\n",
      "Epoch [103/500], Step [0/3451 (0%)]\tLoss: 0.172650\n",
      "Epoch [103/500], Step [1024/3451 (30%)]\tLoss: 0.101199\n",
      "Epoch [103/500], Step [2048/3451 (59%)]\tLoss: 0.140546\n",
      "Epoch [103/500], Step [3072/3451 (89%)]\tLoss: 0.221482\n",
      "Epoch [103/500], Average Loss: 0.2046\n",
      "Epoch [104/500], Step [0/3451 (0%)]\tLoss: 0.173810\n",
      "Epoch [104/500], Step [1024/3451 (30%)]\tLoss: 0.131519\n",
      "Epoch [104/500], Step [2048/3451 (59%)]\tLoss: 0.134954\n",
      "Epoch [104/500], Step [3072/3451 (89%)]\tLoss: 0.135611\n",
      "Epoch [104/500], Average Loss: 0.1827\n",
      "Epoch [105/500], Step [0/3451 (0%)]\tLoss: 0.164893\n",
      "Epoch [105/500], Step [1024/3451 (30%)]\tLoss: 0.100077\n",
      "Epoch [105/500], Step [2048/3451 (59%)]\tLoss: 0.100558\n",
      "Epoch [105/500], Step [3072/3451 (89%)]\tLoss: 0.143804\n",
      "Epoch [105/500], Average Loss: 0.1686\n",
      "Epoch [106/500], Step [0/3451 (0%)]\tLoss: 0.125331\n",
      "Epoch [106/500], Step [1024/3451 (30%)]\tLoss: 0.227704\n",
      "Epoch [106/500], Step [2048/3451 (59%)]\tLoss: 0.092803\n",
      "Epoch [106/500], Step [3072/3451 (89%)]\tLoss: 0.168890\n",
      "Epoch [106/500], Average Loss: 0.1685\n",
      "\n",
      "Test set: Avg. loss: 3.3823 Accuracy: 319/863 (36.96%)\n",
      "\n",
      "Epoch [107/500], Step [0/3451 (0%)]\tLoss: 0.124554\n",
      "Epoch [107/500], Step [1024/3451 (30%)]\tLoss: 0.136670\n",
      "Epoch [107/500], Step [2048/3451 (59%)]\tLoss: 0.154564\n",
      "Epoch [107/500], Step [3072/3451 (89%)]\tLoss: 0.148335\n",
      "Epoch [107/500], Average Loss: 0.1488\n",
      "Epoch [108/500], Step [0/3451 (0%)]\tLoss: 0.096468\n",
      "Epoch [108/500], Step [1024/3451 (30%)]\tLoss: 0.149453\n",
      "Epoch [108/500], Step [2048/3451 (59%)]\tLoss: 0.172233\n",
      "Epoch [108/500], Step [3072/3451 (89%)]\tLoss: 0.188494\n",
      "Epoch [108/500], Average Loss: 0.1528\n",
      "Epoch [109/500], Step [0/3451 (0%)]\tLoss: 0.089018\n",
      "Epoch [109/500], Step [1024/3451 (30%)]\tLoss: 0.153294\n",
      "Epoch [109/500], Step [2048/3451 (59%)]\tLoss: 0.110770\n",
      "Epoch [109/500], Step [3072/3451 (89%)]\tLoss: 0.120473\n",
      "Epoch [109/500], Average Loss: 0.1498\n",
      "Epoch [110/500], Step [0/3451 (0%)]\tLoss: 0.140124\n",
      "Epoch [110/500], Step [1024/3451 (30%)]\tLoss: 0.173242\n",
      "Epoch [110/500], Step [2048/3451 (59%)]\tLoss: 0.065089\n",
      "Epoch [110/500], Step [3072/3451 (89%)]\tLoss: 0.136063\n",
      "Epoch [110/500], Average Loss: 0.1436\n",
      "Epoch [111/500], Step [0/3451 (0%)]\tLoss: 0.165801\n",
      "Epoch [111/500], Step [1024/3451 (30%)]\tLoss: 0.257070\n",
      "Epoch [111/500], Step [2048/3451 (59%)]\tLoss: 0.088240\n",
      "Epoch [111/500], Step [3072/3451 (89%)]\tLoss: 0.179733\n",
      "Epoch [111/500], Average Loss: 0.1455\n",
      "\n",
      "Test set: Avg. loss: 3.8900 Accuracy: 326/863 (37.78%)\n",
      "\n",
      "Epoch [112/500], Step [0/3451 (0%)]\tLoss: 0.137789\n",
      "Epoch [112/500], Step [1024/3451 (30%)]\tLoss: 0.096530\n",
      "Epoch [112/500], Step [2048/3451 (59%)]\tLoss: 0.128467\n",
      "Epoch [112/500], Step [3072/3451 (89%)]\tLoss: 0.156099\n",
      "Epoch [112/500], Average Loss: 0.1440\n",
      "Epoch [113/500], Step [0/3451 (0%)]\tLoss: 0.101485\n",
      "Epoch [113/500], Step [1024/3451 (30%)]\tLoss: 0.193826\n",
      "Epoch [113/500], Step [2048/3451 (59%)]\tLoss: 0.247378\n",
      "Epoch [113/500], Step [3072/3451 (89%)]\tLoss: 0.096964\n",
      "Epoch [113/500], Average Loss: 0.1531\n",
      "Epoch [114/500], Step [0/3451 (0%)]\tLoss: 0.222118\n",
      "Epoch [114/500], Step [1024/3451 (30%)]\tLoss: 0.174900\n",
      "Epoch [114/500], Step [2048/3451 (59%)]\tLoss: 0.147115\n",
      "Epoch [114/500], Step [3072/3451 (89%)]\tLoss: 0.124175\n",
      "Epoch [114/500], Average Loss: 0.1417\n",
      "Epoch [115/500], Step [0/3451 (0%)]\tLoss: 0.158106\n",
      "Epoch [115/500], Step [1024/3451 (30%)]\tLoss: 0.149951\n",
      "Epoch [115/500], Step [2048/3451 (59%)]\tLoss: 0.143153\n",
      "Epoch [115/500], Step [3072/3451 (89%)]\tLoss: 0.086375\n",
      "Epoch [115/500], Average Loss: 0.1324\n",
      "Epoch [116/500], Step [0/3451 (0%)]\tLoss: 0.119063\n",
      "Epoch [116/500], Step [1024/3451 (30%)]\tLoss: 0.089860\n",
      "Epoch [116/500], Step [2048/3451 (59%)]\tLoss: 0.109932\n",
      "Epoch [116/500], Step [3072/3451 (89%)]\tLoss: 0.077042\n",
      "Epoch [116/500], Average Loss: 0.1379\n",
      "\n",
      "Test set: Avg. loss: 3.8222 Accuracy: 320/863 (37.08%)\n",
      "\n",
      "Epoch [117/500], Step [0/3451 (0%)]\tLoss: 0.116192\n",
      "Epoch [117/500], Step [1024/3451 (30%)]\tLoss: 0.150806\n",
      "Epoch [117/500], Step [2048/3451 (59%)]\tLoss: 0.136962\n",
      "Epoch [117/500], Step [3072/3451 (89%)]\tLoss: 0.175418\n",
      "Epoch [117/500], Average Loss: 0.1355\n",
      "Epoch [118/500], Step [0/3451 (0%)]\tLoss: 0.083578\n",
      "Epoch [118/500], Step [1024/3451 (30%)]\tLoss: 0.150743\n",
      "Epoch [118/500], Step [2048/3451 (59%)]\tLoss: 0.136932\n",
      "Epoch [118/500], Step [3072/3451 (89%)]\tLoss: 0.226886\n",
      "Epoch [118/500], Average Loss: 0.1294\n",
      "Epoch [119/500], Step [0/3451 (0%)]\tLoss: 0.100331\n",
      "Epoch [119/500], Step [1024/3451 (30%)]\tLoss: 0.090011\n",
      "Epoch [119/500], Step [2048/3451 (59%)]\tLoss: 0.081267\n",
      "Epoch [119/500], Step [3072/3451 (89%)]\tLoss: 0.059349\n",
      "Epoch [119/500], Average Loss: 0.1282\n",
      "Epoch [120/500], Step [0/3451 (0%)]\tLoss: 0.077247\n",
      "Epoch [120/500], Step [1024/3451 (30%)]\tLoss: 0.127982\n",
      "Epoch [120/500], Step [2048/3451 (59%)]\tLoss: 0.076652\n",
      "Epoch [120/500], Step [3072/3451 (89%)]\tLoss: 0.057622\n",
      "Epoch [120/500], Average Loss: 0.1213\n",
      "Epoch [121/500], Step [0/3451 (0%)]\tLoss: 0.064680\n",
      "Epoch [121/500], Step [1024/3451 (30%)]\tLoss: 0.102902\n",
      "Epoch [121/500], Step [2048/3451 (59%)]\tLoss: 0.073556\n",
      "Epoch [121/500], Step [3072/3451 (89%)]\tLoss: 0.194061\n",
      "Epoch [121/500], Average Loss: 0.1160\n",
      "\n",
      "Test set: Avg. loss: 3.6625 Accuracy: 335/863 (38.82%)\n",
      "\n",
      "Epoch [122/500], Step [0/3451 (0%)]\tLoss: 0.062533\n",
      "Epoch [122/500], Step [1024/3451 (30%)]\tLoss: 0.146105\n",
      "Epoch [122/500], Step [2048/3451 (59%)]\tLoss: 0.093441\n",
      "Epoch [122/500], Step [3072/3451 (89%)]\tLoss: 0.075262\n",
      "Epoch [122/500], Average Loss: 0.1162\n",
      "Epoch [123/500], Step [0/3451 (0%)]\tLoss: 0.072721\n",
      "Epoch [123/500], Step [1024/3451 (30%)]\tLoss: 0.090622\n",
      "Epoch [123/500], Step [2048/3451 (59%)]\tLoss: 0.141912\n",
      "Epoch [123/500], Step [3072/3451 (89%)]\tLoss: 0.092876\n",
      "Epoch [123/500], Average Loss: 0.1165\n",
      "Epoch [124/500], Step [0/3451 (0%)]\tLoss: 0.071637\n",
      "Epoch [124/500], Step [1024/3451 (30%)]\tLoss: 0.053996\n",
      "Epoch [124/500], Step [2048/3451 (59%)]\tLoss: 0.100755\n",
      "Epoch [124/500], Step [3072/3451 (89%)]\tLoss: 0.105302\n",
      "Epoch [124/500], Average Loss: 0.1154\n",
      "Epoch [125/500], Step [0/3451 (0%)]\tLoss: 0.131328\n",
      "Epoch [125/500], Step [1024/3451 (30%)]\tLoss: 0.095084\n",
      "Epoch [125/500], Step [2048/3451 (59%)]\tLoss: 0.038264\n",
      "Epoch [125/500], Step [3072/3451 (89%)]\tLoss: 0.071452\n",
      "Epoch [125/500], Average Loss: 0.1155\n",
      "Epoch [126/500], Step [0/3451 (0%)]\tLoss: 0.100780\n",
      "Epoch [126/500], Step [1024/3451 (30%)]\tLoss: 0.075335\n",
      "Epoch [126/500], Step [2048/3451 (59%)]\tLoss: 0.257781\n",
      "Epoch [126/500], Step [3072/3451 (89%)]\tLoss: 0.047007\n",
      "Epoch [126/500], Average Loss: 0.1112\n",
      "\n",
      "Test set: Avg. loss: 4.2138 Accuracy: 314/863 (36.38%)\n",
      "\n",
      "Epoch [127/500], Step [0/3451 (0%)]\tLoss: 0.078724\n",
      "Epoch [127/500], Step [1024/3451 (30%)]\tLoss: 0.075081\n",
      "Epoch [127/500], Step [2048/3451 (59%)]\tLoss: 0.143379\n",
      "Epoch [127/500], Step [3072/3451 (89%)]\tLoss: 0.213335\n",
      "Epoch [127/500], Average Loss: 0.1090\n",
      "Epoch [128/500], Step [0/3451 (0%)]\tLoss: 0.091120\n",
      "Epoch [128/500], Step [1024/3451 (30%)]\tLoss: 0.072176\n",
      "Epoch [128/500], Step [2048/3451 (59%)]\tLoss: 0.174488\n",
      "Epoch [128/500], Step [3072/3451 (89%)]\tLoss: 0.109869\n",
      "Epoch [128/500], Average Loss: 0.1136\n",
      "Epoch [129/500], Step [0/3451 (0%)]\tLoss: 0.135074\n",
      "Epoch [129/500], Step [1024/3451 (30%)]\tLoss: 0.130155\n",
      "Epoch [129/500], Step [2048/3451 (59%)]\tLoss: 0.082802\n",
      "Epoch [129/500], Step [3072/3451 (89%)]\tLoss: 0.096463\n",
      "Epoch [129/500], Average Loss: 0.1114\n",
      "Epoch [130/500], Step [0/3451 (0%)]\tLoss: 0.066958\n",
      "Epoch [130/500], Step [1024/3451 (30%)]\tLoss: 0.173518\n",
      "Epoch [130/500], Step [2048/3451 (59%)]\tLoss: 0.099357\n",
      "Epoch [130/500], Step [3072/3451 (89%)]\tLoss: 0.117754\n",
      "Epoch [130/500], Average Loss: 0.1050\n",
      "Epoch [131/500], Step [0/3451 (0%)]\tLoss: 0.116413\n",
      "Epoch [131/500], Step [1024/3451 (30%)]\tLoss: 0.075983\n",
      "Epoch [131/500], Step [2048/3451 (59%)]\tLoss: 0.155744\n",
      "Epoch [131/500], Step [3072/3451 (89%)]\tLoss: 0.136762\n",
      "Epoch [131/500], Average Loss: 0.1008\n",
      "\n",
      "Test set: Avg. loss: 4.1245 Accuracy: 332/863 (38.47%)\n",
      "\n",
      "Epoch [132/500], Step [0/3451 (0%)]\tLoss: 0.113630\n",
      "Epoch [132/500], Step [1024/3451 (30%)]\tLoss: 0.099548\n",
      "Epoch [132/500], Step [2048/3451 (59%)]\tLoss: 0.104325\n",
      "Epoch [132/500], Step [3072/3451 (89%)]\tLoss: 0.118587\n",
      "Epoch [132/500], Average Loss: 0.1026\n",
      "Epoch [133/500], Step [0/3451 (0%)]\tLoss: 0.077479\n",
      "Epoch [133/500], Step [1024/3451 (30%)]\tLoss: 0.078031\n",
      "Epoch [133/500], Step [2048/3451 (59%)]\tLoss: 0.153331\n",
      "Epoch [133/500], Step [3072/3451 (89%)]\tLoss: 0.130313\n",
      "Epoch [133/500], Average Loss: 0.0996\n",
      "Epoch [134/500], Step [0/3451 (0%)]\tLoss: 0.146748\n",
      "Epoch [134/500], Step [1024/3451 (30%)]\tLoss: 0.193424\n",
      "Epoch [134/500], Step [2048/3451 (59%)]\tLoss: 0.120371\n",
      "Epoch [134/500], Step [3072/3451 (89%)]\tLoss: 0.062463\n",
      "Epoch [134/500], Average Loss: 0.1100\n",
      "Epoch [135/500], Step [0/3451 (0%)]\tLoss: 0.069008\n",
      "Epoch [135/500], Step [1024/3451 (30%)]\tLoss: 0.104437\n",
      "Epoch [135/500], Step [2048/3451 (59%)]\tLoss: 0.086897\n",
      "Epoch [135/500], Step [3072/3451 (89%)]\tLoss: 0.124636\n",
      "Epoch [135/500], Average Loss: 0.1119\n",
      "Epoch [136/500], Step [0/3451 (0%)]\tLoss: 0.101115\n",
      "Epoch [136/500], Step [1024/3451 (30%)]\tLoss: 0.089014\n",
      "Epoch [136/500], Step [2048/3451 (59%)]\tLoss: 0.068496\n",
      "Epoch [136/500], Step [3072/3451 (89%)]\tLoss: 0.048003\n",
      "Epoch [136/500], Average Loss: 0.1044\n",
      "\n",
      "Test set: Avg. loss: 4.4309 Accuracy: 312/863 (36.15%)\n",
      "\n",
      "Epoch [137/500], Step [0/3451 (0%)]\tLoss: 0.057404\n",
      "Epoch [137/500], Step [1024/3451 (30%)]\tLoss: 0.061436\n",
      "Epoch [137/500], Step [2048/3451 (59%)]\tLoss: 0.103016\n",
      "Epoch [137/500], Step [3072/3451 (89%)]\tLoss: 0.134052\n",
      "Epoch [137/500], Average Loss: 0.1081\n",
      "Epoch [138/500], Step [0/3451 (0%)]\tLoss: 0.105722\n",
      "Epoch [138/500], Step [1024/3451 (30%)]\tLoss: 0.111996\n",
      "Epoch [138/500], Step [2048/3451 (59%)]\tLoss: 0.114081\n",
      "Epoch [138/500], Step [3072/3451 (89%)]\tLoss: 0.139422\n",
      "Epoch [138/500], Average Loss: 0.1092\n",
      "Epoch [139/500], Step [0/3451 (0%)]\tLoss: 0.090443\n",
      "Epoch [139/500], Step [1024/3451 (30%)]\tLoss: 0.132335\n",
      "Epoch [139/500], Step [2048/3451 (59%)]\tLoss: 0.068988\n",
      "Epoch [139/500], Step [3072/3451 (89%)]\tLoss: 0.130960\n",
      "Epoch [139/500], Average Loss: 0.1044\n",
      "Epoch [140/500], Step [0/3451 (0%)]\tLoss: 0.079237\n",
      "Epoch [140/500], Step [1024/3451 (30%)]\tLoss: 0.156044\n",
      "Epoch [140/500], Step [2048/3451 (59%)]\tLoss: 0.094288\n",
      "Epoch [140/500], Step [3072/3451 (89%)]\tLoss: 0.074680\n",
      "Epoch [140/500], Average Loss: 0.1116\n",
      "Epoch [141/500], Step [0/3451 (0%)]\tLoss: 0.063010\n",
      "Epoch [141/500], Step [1024/3451 (30%)]\tLoss: 0.135697\n",
      "Epoch [141/500], Step [2048/3451 (59%)]\tLoss: 0.114997\n",
      "Epoch [141/500], Step [3072/3451 (89%)]\tLoss: 0.074149\n",
      "Epoch [141/500], Average Loss: 0.0972\n",
      "\n",
      "Test set: Avg. loss: 5.0373 Accuracy: 307/863 (35.57%)\n",
      "\n",
      "Epoch [142/500], Step [0/3451 (0%)]\tLoss: 0.080734\n",
      "Epoch [142/500], Step [1024/3451 (30%)]\tLoss: 0.053916\n",
      "Epoch [142/500], Step [2048/3451 (59%)]\tLoss: 0.135064\n",
      "Epoch [142/500], Step [3072/3451 (89%)]\tLoss: 0.046076\n",
      "Epoch [142/500], Average Loss: 0.0947\n",
      "Epoch [143/500], Step [0/3451 (0%)]\tLoss: 0.113944\n",
      "Epoch [143/500], Step [1024/3451 (30%)]\tLoss: 0.084987\n",
      "Epoch [143/500], Step [2048/3451 (59%)]\tLoss: 0.089106\n",
      "Epoch [143/500], Step [3072/3451 (89%)]\tLoss: 0.097607\n",
      "Epoch [143/500], Average Loss: 0.1054\n",
      "Epoch [144/500], Step [0/3451 (0%)]\tLoss: 0.067218\n",
      "Epoch [144/500], Step [1024/3451 (30%)]\tLoss: 0.185080\n",
      "Epoch [144/500], Step [2048/3451 (59%)]\tLoss: 0.082805\n",
      "Epoch [144/500], Step [3072/3451 (89%)]\tLoss: 0.147137\n",
      "Epoch [144/500], Average Loss: 0.1067\n",
      "Epoch [145/500], Step [0/3451 (0%)]\tLoss: 0.076455\n",
      "Epoch [145/500], Step [1024/3451 (30%)]\tLoss: 0.036845\n",
      "Epoch [145/500], Step [2048/3451 (59%)]\tLoss: 0.132724\n",
      "Epoch [145/500], Step [3072/3451 (89%)]\tLoss: 0.032952\n",
      "Epoch [145/500], Average Loss: 0.1084\n",
      "Epoch [146/500], Step [0/3451 (0%)]\tLoss: 0.036238\n",
      "Epoch [146/500], Step [1024/3451 (30%)]\tLoss: 0.152994\n",
      "Epoch [146/500], Step [2048/3451 (59%)]\tLoss: 0.083501\n",
      "Epoch [146/500], Step [3072/3451 (89%)]\tLoss: 0.088325\n",
      "Epoch [146/500], Average Loss: 0.0987\n",
      "\n",
      "Test set: Avg. loss: 4.3200 Accuracy: 316/863 (36.62%)\n",
      "\n",
      "Epoch [147/500], Step [0/3451 (0%)]\tLoss: 0.114565\n",
      "Epoch [147/500], Step [1024/3451 (30%)]\tLoss: 0.021536\n",
      "Epoch [147/500], Step [2048/3451 (59%)]\tLoss: 0.046049\n",
      "Epoch [147/500], Step [3072/3451 (89%)]\tLoss: 0.068551\n",
      "Epoch [147/500], Average Loss: 0.0960\n",
      "Epoch [148/500], Step [0/3451 (0%)]\tLoss: 0.101939\n",
      "Epoch [148/500], Step [1024/3451 (30%)]\tLoss: 0.160350\n",
      "Epoch [148/500], Step [2048/3451 (59%)]\tLoss: 0.172730\n",
      "Epoch [148/500], Step [3072/3451 (89%)]\tLoss: 0.225007\n",
      "Epoch [148/500], Average Loss: 0.0975\n",
      "Epoch [149/500], Step [0/3451 (0%)]\tLoss: 0.100696\n",
      "Epoch [149/500], Step [1024/3451 (30%)]\tLoss: 0.073943\n",
      "Epoch [149/500], Step [2048/3451 (59%)]\tLoss: 0.120069\n",
      "Epoch [149/500], Step [3072/3451 (89%)]\tLoss: 0.104940\n",
      "Epoch [149/500], Average Loss: 0.0932\n",
      "Epoch [150/500], Step [0/3451 (0%)]\tLoss: 0.109688\n",
      "Epoch [150/500], Step [1024/3451 (30%)]\tLoss: 0.046678\n",
      "Epoch [150/500], Step [2048/3451 (59%)]\tLoss: 0.051909\n",
      "Epoch [150/500], Step [3072/3451 (89%)]\tLoss: 0.111583\n",
      "Epoch [150/500], Average Loss: 0.0906\n",
      "Epoch [151/500], Step [0/3451 (0%)]\tLoss: 0.079444\n",
      "Epoch [151/500], Step [1024/3451 (30%)]\tLoss: 0.131544\n",
      "Epoch [151/500], Step [2048/3451 (59%)]\tLoss: 0.041602\n",
      "Epoch [151/500], Step [3072/3451 (89%)]\tLoss: 0.055383\n",
      "Epoch [151/500], Average Loss: 0.0883\n",
      "\n",
      "Test set: Avg. loss: 4.5151 Accuracy: 326/863 (37.78%)\n",
      "\n",
      "Epoch [152/500], Step [0/3451 (0%)]\tLoss: 0.075220\n",
      "Epoch [152/500], Step [1024/3451 (30%)]\tLoss: 0.081502\n",
      "Epoch [152/500], Step [2048/3451 (59%)]\tLoss: 0.055606\n",
      "Epoch [152/500], Step [3072/3451 (89%)]\tLoss: 0.169078\n",
      "Epoch [152/500], Average Loss: 0.0918\n",
      "Epoch [153/500], Step [0/3451 (0%)]\tLoss: 0.085465\n",
      "Epoch [153/500], Step [1024/3451 (30%)]\tLoss: 0.143144\n",
      "Epoch [153/500], Step [2048/3451 (59%)]\tLoss: 0.196648\n",
      "Epoch [153/500], Step [3072/3451 (89%)]\tLoss: 0.092119\n",
      "Epoch [153/500], Average Loss: 0.0920\n",
      "Epoch [154/500], Step [0/3451 (0%)]\tLoss: 0.068225\n",
      "Epoch [154/500], Step [1024/3451 (30%)]\tLoss: 0.093654\n",
      "Epoch [154/500], Step [2048/3451 (59%)]\tLoss: 0.283403\n",
      "Epoch [154/500], Step [3072/3451 (89%)]\tLoss: 0.054070\n",
      "Epoch [154/500], Average Loss: 0.0891\n",
      "Epoch [155/500], Step [0/3451 (0%)]\tLoss: 0.111876\n",
      "Epoch [155/500], Step [1024/3451 (30%)]\tLoss: 0.162119\n",
      "Epoch [155/500], Step [2048/3451 (59%)]\tLoss: 0.165856\n",
      "Epoch [155/500], Step [3072/3451 (89%)]\tLoss: 0.051791\n",
      "Epoch [155/500], Average Loss: 0.0878\n",
      "Epoch [156/500], Step [0/3451 (0%)]\tLoss: 0.185264\n",
      "Epoch [156/500], Step [1024/3451 (30%)]\tLoss: 0.047216\n",
      "Epoch [156/500], Step [2048/3451 (59%)]\tLoss: 0.080745\n",
      "Epoch [156/500], Step [3072/3451 (89%)]\tLoss: 0.081611\n",
      "Epoch [156/500], Average Loss: 0.0924\n",
      "\n",
      "Test set: Avg. loss: 4.7361 Accuracy: 315/863 (36.50%)\n",
      "\n",
      "Epoch [157/500], Step [0/3451 (0%)]\tLoss: 0.017990\n",
      "Epoch [157/500], Step [1024/3451 (30%)]\tLoss: 0.048454\n",
      "Epoch [157/500], Step [2048/3451 (59%)]\tLoss: 0.082576\n",
      "Epoch [157/500], Step [3072/3451 (89%)]\tLoss: 0.069993\n",
      "Epoch [157/500], Average Loss: 0.0881\n",
      "Epoch [158/500], Step [0/3451 (0%)]\tLoss: 0.058258\n",
      "Epoch [158/500], Step [1024/3451 (30%)]\tLoss: 0.134036\n",
      "Epoch [158/500], Step [2048/3451 (59%)]\tLoss: 0.114764\n",
      "Epoch [158/500], Step [3072/3451 (89%)]\tLoss: 0.048517\n",
      "Epoch [158/500], Average Loss: 0.0863\n",
      "Epoch [159/500], Step [0/3451 (0%)]\tLoss: 0.055889\n",
      "Epoch [159/500], Step [1024/3451 (30%)]\tLoss: 0.074740\n",
      "Epoch [159/500], Step [2048/3451 (59%)]\tLoss: 0.067561\n",
      "Epoch [159/500], Step [3072/3451 (89%)]\tLoss: 0.117543\n",
      "Epoch [159/500], Average Loss: 0.0899\n",
      "Epoch [160/500], Step [0/3451 (0%)]\tLoss: 0.098639\n",
      "Epoch [160/500], Step [1024/3451 (30%)]\tLoss: 0.048392\n",
      "Epoch [160/500], Step [2048/3451 (59%)]\tLoss: 0.139004\n",
      "Epoch [160/500], Step [3072/3451 (89%)]\tLoss: 0.068192\n",
      "Epoch [160/500], Average Loss: 0.0876\n",
      "Epoch [161/500], Step [0/3451 (0%)]\tLoss: 0.065855\n",
      "Epoch [161/500], Step [1024/3451 (30%)]\tLoss: 0.095159\n",
      "Epoch [161/500], Step [2048/3451 (59%)]\tLoss: 0.055803\n",
      "Epoch [161/500], Step [3072/3451 (89%)]\tLoss: 0.078005\n",
      "Epoch [161/500], Average Loss: 0.0842\n",
      "\n",
      "Test set: Avg. loss: 4.5757 Accuracy: 306/863 (35.46%)\n",
      "\n",
      "Epoch [162/500], Step [0/3451 (0%)]\tLoss: 0.141877\n",
      "Epoch [162/500], Step [1024/3451 (30%)]\tLoss: 0.054553\n",
      "Epoch [162/500], Step [2048/3451 (59%)]\tLoss: 0.031084\n",
      "Epoch [162/500], Step [3072/3451 (89%)]\tLoss: 0.106928\n",
      "Epoch [162/500], Average Loss: 0.0842\n",
      "Epoch [163/500], Step [0/3451 (0%)]\tLoss: 0.031077\n",
      "Epoch [163/500], Step [1024/3451 (30%)]\tLoss: 0.091705\n",
      "Epoch [163/500], Step [2048/3451 (59%)]\tLoss: 0.050509\n",
      "Epoch [163/500], Step [3072/3451 (89%)]\tLoss: 0.103557\n",
      "Epoch [163/500], Average Loss: 0.0839\n",
      "Epoch [164/500], Step [0/3451 (0%)]\tLoss: 0.087253\n",
      "Epoch [164/500], Step [1024/3451 (30%)]\tLoss: 0.068652\n",
      "Epoch [164/500], Step [2048/3451 (59%)]\tLoss: 0.023646\n",
      "Epoch [164/500], Step [3072/3451 (89%)]\tLoss: 0.058763\n",
      "Epoch [164/500], Average Loss: 0.0854\n",
      "Epoch [165/500], Step [0/3451 (0%)]\tLoss: 0.057585\n",
      "Epoch [165/500], Step [1024/3451 (30%)]\tLoss: 0.127078\n",
      "Epoch [165/500], Step [2048/3451 (59%)]\tLoss: 0.095322\n",
      "Epoch [165/500], Step [3072/3451 (89%)]\tLoss: 0.073494\n",
      "Epoch [165/500], Average Loss: 0.0834\n",
      "Epoch [166/500], Step [0/3451 (0%)]\tLoss: 0.101546\n",
      "Epoch [166/500], Step [1024/3451 (30%)]\tLoss: 0.095073\n",
      "Epoch [166/500], Step [2048/3451 (59%)]\tLoss: 0.136030\n",
      "Epoch [166/500], Step [3072/3451 (89%)]\tLoss: 0.113242\n",
      "Epoch [166/500], Average Loss: 0.0873\n",
      "\n",
      "Test set: Avg. loss: 4.9261 Accuracy: 316/863 (36.62%)\n",
      "\n",
      "Epoch [167/500], Step [0/3451 (0%)]\tLoss: 0.070358\n",
      "Epoch [167/500], Step [1024/3451 (30%)]\tLoss: 0.071202\n",
      "Epoch [167/500], Step [2048/3451 (59%)]\tLoss: 0.043346\n",
      "Epoch [167/500], Step [3072/3451 (89%)]\tLoss: 0.093610\n",
      "Epoch [167/500], Average Loss: 0.0856\n",
      "Epoch [168/500], Step [0/3451 (0%)]\tLoss: 0.057824\n",
      "Epoch [168/500], Step [1024/3451 (30%)]\tLoss: 0.085678\n",
      "Epoch [168/500], Step [2048/3451 (59%)]\tLoss: 0.049096\n",
      "Epoch [168/500], Step [3072/3451 (89%)]\tLoss: 0.069384\n",
      "Epoch [168/500], Average Loss: 0.0839\n",
      "Epoch [169/500], Step [0/3451 (0%)]\tLoss: 0.038708\n",
      "Epoch [169/500], Step [1024/3451 (30%)]\tLoss: 0.015794\n",
      "Epoch [169/500], Step [2048/3451 (59%)]\tLoss: 0.055156\n",
      "Epoch [169/500], Step [3072/3451 (89%)]\tLoss: 0.176982\n",
      "Epoch [169/500], Average Loss: 0.0804\n",
      "Epoch [170/500], Step [0/3451 (0%)]\tLoss: 0.056242\n",
      "Epoch [170/500], Step [1024/3451 (30%)]\tLoss: 0.062474\n",
      "Epoch [170/500], Step [2048/3451 (59%)]\tLoss: 0.153659\n",
      "Epoch [170/500], Step [3072/3451 (89%)]\tLoss: 0.048790\n",
      "Epoch [170/500], Average Loss: 0.0842\n",
      "Epoch [171/500], Step [0/3451 (0%)]\tLoss: 0.125421\n",
      "Epoch [171/500], Step [1024/3451 (30%)]\tLoss: 0.162927\n",
      "Epoch [171/500], Step [2048/3451 (59%)]\tLoss: 0.126397\n",
      "Epoch [171/500], Step [3072/3451 (89%)]\tLoss: 0.175998\n",
      "Epoch [171/500], Average Loss: 0.0793\n",
      "\n",
      "Test set: Avg. loss: 5.0805 Accuracy: 326/863 (37.78%)\n",
      "\n",
      "Epoch [172/500], Step [0/3451 (0%)]\tLoss: 0.067457\n",
      "Epoch [172/500], Step [1024/3451 (30%)]\tLoss: 0.048914\n",
      "Epoch [172/500], Step [2048/3451 (59%)]\tLoss: 0.100148\n",
      "Epoch [172/500], Step [3072/3451 (89%)]\tLoss: 0.058127\n",
      "Epoch [172/500], Average Loss: 0.0808\n",
      "Epoch [173/500], Step [0/3451 (0%)]\tLoss: 0.111465\n",
      "Epoch [173/500], Step [1024/3451 (30%)]\tLoss: 0.108873\n",
      "Epoch [173/500], Step [2048/3451 (59%)]\tLoss: 0.073576\n",
      "Epoch [173/500], Step [3072/3451 (89%)]\tLoss: 0.095484\n",
      "Epoch [173/500], Average Loss: 0.0805\n",
      "Epoch [174/500], Step [0/3451 (0%)]\tLoss: 0.045772\n",
      "Epoch [174/500], Step [1024/3451 (30%)]\tLoss: 0.103459\n",
      "Epoch [174/500], Step [2048/3451 (59%)]\tLoss: 0.034935\n",
      "Epoch [174/500], Step [3072/3451 (89%)]\tLoss: 0.074489\n",
      "Epoch [174/500], Average Loss: 0.0824\n",
      "Epoch [175/500], Step [0/3451 (0%)]\tLoss: 0.041178\n",
      "Epoch [175/500], Step [1024/3451 (30%)]\tLoss: 0.073279\n",
      "Epoch [175/500], Step [2048/3451 (59%)]\tLoss: 0.040546\n",
      "Epoch [175/500], Step [3072/3451 (89%)]\tLoss: 0.058432\n",
      "Epoch [175/500], Average Loss: 0.0807\n",
      "Epoch [176/500], Step [0/3451 (0%)]\tLoss: 0.040560\n",
      "Epoch [176/500], Step [1024/3451 (30%)]\tLoss: 0.057345\n",
      "Epoch [176/500], Step [2048/3451 (59%)]\tLoss: 0.107042\n",
      "Epoch [176/500], Step [3072/3451 (89%)]\tLoss: 0.059947\n",
      "Epoch [176/500], Average Loss: 0.0795\n",
      "\n",
      "Test set: Avg. loss: 4.7778 Accuracy: 340/863 (39.40%)\n",
      "\n",
      "Epoch [177/500], Step [0/3451 (0%)]\tLoss: 0.040642\n",
      "Epoch [177/500], Step [1024/3451 (30%)]\tLoss: 0.082027\n",
      "Epoch [177/500], Step [2048/3451 (59%)]\tLoss: 0.129452\n",
      "Epoch [177/500], Step [3072/3451 (89%)]\tLoss: 0.051761\n",
      "Epoch [177/500], Average Loss: 0.0788\n",
      "Epoch [178/500], Step [0/3451 (0%)]\tLoss: 0.057672\n",
      "Epoch [178/500], Step [1024/3451 (30%)]\tLoss: 0.044037\n",
      "Epoch [178/500], Step [2048/3451 (59%)]\tLoss: 0.087383\n",
      "Epoch [178/500], Step [3072/3451 (89%)]\tLoss: 0.068021\n",
      "Epoch [178/500], Average Loss: 0.0809\n",
      "Epoch [179/500], Step [0/3451 (0%)]\tLoss: 0.063785\n",
      "Epoch [179/500], Step [1024/3451 (30%)]\tLoss: 0.038634\n",
      "Epoch [179/500], Step [2048/3451 (59%)]\tLoss: 0.099352\n",
      "Epoch [179/500], Step [3072/3451 (89%)]\tLoss: 0.066289\n",
      "Epoch [179/500], Average Loss: 0.0771\n",
      "Epoch [180/500], Step [0/3451 (0%)]\tLoss: 0.235688\n",
      "Epoch [180/500], Step [1024/3451 (30%)]\tLoss: 0.278170\n",
      "Epoch [180/500], Step [2048/3451 (59%)]\tLoss: 0.143748\n",
      "Epoch [180/500], Step [3072/3451 (89%)]\tLoss: 0.113590\n",
      "Epoch [180/500], Average Loss: 0.0816\n",
      "Epoch [181/500], Step [0/3451 (0%)]\tLoss: 0.030813\n",
      "Epoch [181/500], Step [1024/3451 (30%)]\tLoss: 0.123986\n",
      "Epoch [181/500], Step [2048/3451 (59%)]\tLoss: 0.042741\n",
      "Epoch [181/500], Step [3072/3451 (89%)]\tLoss: 0.060815\n",
      "Epoch [181/500], Average Loss: 0.0835\n",
      "\n",
      "Test set: Avg. loss: 4.8854 Accuracy: 338/863 (39.17%)\n",
      "\n",
      "Epoch [182/500], Step [0/3451 (0%)]\tLoss: 0.108149\n",
      "Epoch [182/500], Step [1024/3451 (30%)]\tLoss: 0.093534\n",
      "Epoch [182/500], Step [2048/3451 (59%)]\tLoss: 0.137619\n",
      "Epoch [182/500], Step [3072/3451 (89%)]\tLoss: 0.060512\n",
      "Epoch [182/500], Average Loss: 0.0805\n",
      "Epoch [183/500], Step [0/3451 (0%)]\tLoss: 0.047073\n",
      "Epoch [183/500], Step [1024/3451 (30%)]\tLoss: 0.015877\n",
      "Epoch [183/500], Step [2048/3451 (59%)]\tLoss: 0.065082\n",
      "Epoch [183/500], Step [3072/3451 (89%)]\tLoss: 0.048881\n",
      "Epoch [183/500], Average Loss: 0.0807\n",
      "Epoch [184/500], Step [0/3451 (0%)]\tLoss: 0.072253\n",
      "Epoch [184/500], Step [1024/3451 (30%)]\tLoss: 0.058057\n",
      "Epoch [184/500], Step [2048/3451 (59%)]\tLoss: 0.057742\n",
      "Epoch [184/500], Step [3072/3451 (89%)]\tLoss: 0.079752\n",
      "Epoch [184/500], Average Loss: 0.0841\n",
      "Epoch [185/500], Step [0/3451 (0%)]\tLoss: 0.098153\n",
      "Epoch [185/500], Step [1024/3451 (30%)]\tLoss: 0.157069\n",
      "Epoch [185/500], Step [2048/3451 (59%)]\tLoss: 0.099462\n",
      "Epoch [185/500], Step [3072/3451 (89%)]\tLoss: 0.071191\n",
      "Epoch [185/500], Average Loss: 0.0815\n",
      "Epoch [186/500], Step [0/3451 (0%)]\tLoss: 0.052608\n",
      "Epoch [186/500], Step [1024/3451 (30%)]\tLoss: 0.045352\n",
      "Epoch [186/500], Step [2048/3451 (59%)]\tLoss: 0.063884\n",
      "Epoch [186/500], Step [3072/3451 (89%)]\tLoss: 0.105566\n",
      "Epoch [186/500], Average Loss: 0.0802\n",
      "\n",
      "Test set: Avg. loss: 4.9372 Accuracy: 337/863 (39.05%)\n",
      "\n",
      "Epoch [187/500], Step [0/3451 (0%)]\tLoss: 0.076060\n",
      "Epoch [187/500], Step [1024/3451 (30%)]\tLoss: 0.024351\n",
      "Epoch [187/500], Step [2048/3451 (59%)]\tLoss: 0.109058\n",
      "Epoch [187/500], Step [3072/3451 (89%)]\tLoss: 0.115380\n",
      "Epoch [187/500], Average Loss: 0.0838\n",
      "Epoch [188/500], Step [0/3451 (0%)]\tLoss: 0.102724\n",
      "Epoch [188/500], Step [1024/3451 (30%)]\tLoss: 0.054637\n",
      "Epoch [188/500], Step [2048/3451 (59%)]\tLoss: 0.043618\n",
      "Epoch [188/500], Step [3072/3451 (89%)]\tLoss: 0.097796\n",
      "Epoch [188/500], Average Loss: 0.0800\n",
      "Epoch [189/500], Step [0/3451 (0%)]\tLoss: 0.065237\n",
      "Epoch [189/500], Step [1024/3451 (30%)]\tLoss: 0.071425\n",
      "Epoch [189/500], Step [2048/3451 (59%)]\tLoss: 0.091615\n",
      "Epoch [189/500], Step [3072/3451 (89%)]\tLoss: 0.097541\n",
      "Epoch [189/500], Average Loss: 0.0801\n",
      "Epoch [190/500], Step [0/3451 (0%)]\tLoss: 0.160329\n",
      "Epoch [190/500], Step [1024/3451 (30%)]\tLoss: 0.053742\n",
      "Epoch [190/500], Step [2048/3451 (59%)]\tLoss: 0.120384\n",
      "Epoch [190/500], Step [3072/3451 (89%)]\tLoss: 0.068807\n",
      "Epoch 00190: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch [190/500], Average Loss: 0.0812\n",
      "Epoch [191/500], Step [0/3451 (0%)]\tLoss: 0.089263\n",
      "Epoch [191/500], Step [1024/3451 (30%)]\tLoss: 0.015027\n",
      "Epoch [191/500], Step [2048/3451 (59%)]\tLoss: 0.110088\n",
      "Epoch [191/500], Step [3072/3451 (89%)]\tLoss: 0.072447\n",
      "Epoch [191/500], Average Loss: 0.0772\n",
      "\n",
      "Test set: Avg. loss: 4.9479 Accuracy: 340/863 (39.40%)\n",
      "\n",
      "Epoch [192/500], Step [0/3451 (0%)]\tLoss: 0.055351\n",
      "Epoch [192/500], Step [1024/3451 (30%)]\tLoss: 0.119176\n",
      "Epoch [192/500], Step [2048/3451 (59%)]\tLoss: 0.066314\n",
      "Epoch [192/500], Step [3072/3451 (89%)]\tLoss: 0.066673\n",
      "Epoch [192/500], Average Loss: 0.0755\n",
      "Epoch [193/500], Step [0/3451 (0%)]\tLoss: 0.030326\n",
      "Epoch [193/500], Step [1024/3451 (30%)]\tLoss: 0.111404\n",
      "Epoch [193/500], Step [2048/3451 (59%)]\tLoss: 0.100634\n",
      "Epoch [193/500], Step [3072/3451 (89%)]\tLoss: 0.049320\n",
      "Epoch [193/500], Average Loss: 0.0743\n",
      "Epoch [194/500], Step [0/3451 (0%)]\tLoss: 0.094711\n",
      "Epoch [194/500], Step [1024/3451 (30%)]\tLoss: 0.020789\n",
      "Epoch [194/500], Step [2048/3451 (59%)]\tLoss: 0.055415\n",
      "Epoch [194/500], Step [3072/3451 (89%)]\tLoss: 0.081432\n",
      "Epoch [194/500], Average Loss: 0.0745\n",
      "Epoch [195/500], Step [0/3451 (0%)]\tLoss: 0.032051\n",
      "Epoch [195/500], Step [1024/3451 (30%)]\tLoss: 0.108965\n",
      "Epoch [195/500], Step [2048/3451 (59%)]\tLoss: 0.059678\n",
      "Epoch [195/500], Step [3072/3451 (89%)]\tLoss: 0.083042\n",
      "Epoch [195/500], Average Loss: 0.0730\n",
      "Epoch [196/500], Step [0/3451 (0%)]\tLoss: 0.086366\n",
      "Epoch [196/500], Step [1024/3451 (30%)]\tLoss: 0.092755\n",
      "Epoch [196/500], Step [2048/3451 (59%)]\tLoss: 0.039924\n",
      "Epoch [196/500], Step [3072/3451 (89%)]\tLoss: 0.036767\n",
      "Epoch [196/500], Average Loss: 0.0741\n",
      "\n",
      "Test set: Avg. loss: 4.9081 Accuracy: 335/863 (38.82%)\n",
      "\n",
      "Epoch [197/500], Step [0/3451 (0%)]\tLoss: 0.074656\n",
      "Epoch [197/500], Step [1024/3451 (30%)]\tLoss: 0.067079\n",
      "Epoch [197/500], Step [2048/3451 (59%)]\tLoss: 0.063432\n",
      "Epoch [197/500], Step [3072/3451 (89%)]\tLoss: 0.027447\n",
      "Epoch [197/500], Average Loss: 0.0735\n",
      "Epoch [198/500], Step [0/3451 (0%)]\tLoss: 0.065827\n",
      "Epoch [198/500], Step [1024/3451 (30%)]\tLoss: 0.111842\n",
      "Epoch [198/500], Step [2048/3451 (59%)]\tLoss: 0.068947\n",
      "Epoch [198/500], Step [3072/3451 (89%)]\tLoss: 0.094043\n",
      "Epoch [198/500], Average Loss: 0.0740\n",
      "Epoch [199/500], Step [0/3451 (0%)]\tLoss: 0.093867\n",
      "Epoch [199/500], Step [1024/3451 (30%)]\tLoss: 0.050727\n",
      "Epoch [199/500], Step [2048/3451 (59%)]\tLoss: 0.026223\n",
      "Epoch [199/500], Step [3072/3451 (89%)]\tLoss: 0.043402\n",
      "Epoch [199/500], Average Loss: 0.0736\n",
      "Epoch [200/500], Step [0/3451 (0%)]\tLoss: 0.046728\n",
      "Epoch [200/500], Step [1024/3451 (30%)]\tLoss: 0.070359\n",
      "Epoch [200/500], Step [2048/3451 (59%)]\tLoss: 0.032634\n",
      "Epoch [200/500], Step [3072/3451 (89%)]\tLoss: 0.060961\n",
      "Epoch [200/500], Average Loss: 0.0722\n",
      "Epoch [201/500], Step [0/3451 (0%)]\tLoss: 0.033814\n",
      "Epoch [201/500], Step [1024/3451 (30%)]\tLoss: 0.099933\n",
      "Epoch [201/500], Step [2048/3451 (59%)]\tLoss: 0.020660\n",
      "Epoch [201/500], Step [3072/3451 (89%)]\tLoss: 0.098485\n",
      "Epoch [201/500], Average Loss: 0.0719\n",
      "\n",
      "Test set: Avg. loss: 4.8423 Accuracy: 340/863 (39.40%)\n",
      "\n",
      "Epoch [202/500], Step [0/3451 (0%)]\tLoss: 0.105174\n",
      "Epoch [202/500], Step [1024/3451 (30%)]\tLoss: 0.138567\n",
      "Epoch [202/500], Step [2048/3451 (59%)]\tLoss: 0.221264\n",
      "Epoch [202/500], Step [3072/3451 (89%)]\tLoss: 0.040002\n",
      "Epoch [202/500], Average Loss: 0.0719\n",
      "Epoch [203/500], Step [0/3451 (0%)]\tLoss: 0.069409\n",
      "Epoch [203/500], Step [1024/3451 (30%)]\tLoss: 0.074686\n",
      "Epoch [203/500], Step [2048/3451 (59%)]\tLoss: 0.156180\n",
      "Epoch [203/500], Step [3072/3451 (89%)]\tLoss: 0.120792\n",
      "Epoch [203/500], Average Loss: 0.0725\n",
      "Epoch [204/500], Step [0/3451 (0%)]\tLoss: 0.096293\n",
      "Epoch [204/500], Step [1024/3451 (30%)]\tLoss: 0.109785\n",
      "Epoch [204/500], Step [2048/3451 (59%)]\tLoss: 0.018352\n",
      "Epoch [204/500], Step [3072/3451 (89%)]\tLoss: 0.106142\n",
      "Epoch [204/500], Average Loss: 0.0742\n",
      "Epoch [205/500], Step [0/3451 (0%)]\tLoss: 0.105713\n",
      "Epoch [205/500], Step [1024/3451 (30%)]\tLoss: 0.082071\n",
      "Epoch [205/500], Step [2048/3451 (59%)]\tLoss: 0.086226\n",
      "Epoch [205/500], Step [3072/3451 (89%)]\tLoss: 0.076676\n",
      "Epoch [205/500], Average Loss: 0.0720\n",
      "Epoch [206/500], Step [0/3451 (0%)]\tLoss: 0.021545\n",
      "Epoch [206/500], Step [1024/3451 (30%)]\tLoss: 0.102836\n",
      "Epoch [206/500], Step [2048/3451 (59%)]\tLoss: 0.048092\n",
      "Epoch [206/500], Step [3072/3451 (89%)]\tLoss: 0.030622\n",
      "Epoch [206/500], Average Loss: 0.0729\n",
      "\n",
      "Test set: Avg. loss: 4.8386 Accuracy: 338/863 (39.17%)\n",
      "\n",
      "Epoch [207/500], Step [0/3451 (0%)]\tLoss: 0.107649\n",
      "Epoch [207/500], Step [1024/3451 (30%)]\tLoss: 0.063852\n",
      "Epoch [207/500], Step [2048/3451 (59%)]\tLoss: 0.054658\n",
      "Epoch [207/500], Step [3072/3451 (89%)]\tLoss: 0.026529\n",
      "Epoch [207/500], Average Loss: 0.0723\n",
      "Epoch [208/500], Step [0/3451 (0%)]\tLoss: 0.061657\n",
      "Epoch [208/500], Step [1024/3451 (30%)]\tLoss: 0.043413\n",
      "Epoch [208/500], Step [2048/3451 (59%)]\tLoss: 0.077762\n",
      "Epoch [208/500], Step [3072/3451 (89%)]\tLoss: 0.147843\n",
      "Epoch [208/500], Average Loss: 0.0723\n",
      "Epoch [209/500], Step [0/3451 (0%)]\tLoss: 0.059154\n",
      "Epoch [209/500], Step [1024/3451 (30%)]\tLoss: 0.094223\n",
      "Epoch [209/500], Step [2048/3451 (59%)]\tLoss: 0.078211\n",
      "Epoch [209/500], Step [3072/3451 (89%)]\tLoss: 0.026863\n",
      "Epoch [209/500], Average Loss: 0.0718\n",
      "Epoch [210/500], Step [0/3451 (0%)]\tLoss: 0.027876\n",
      "Epoch [210/500], Step [1024/3451 (30%)]\tLoss: 0.085517\n",
      "Epoch [210/500], Step [2048/3451 (59%)]\tLoss: 0.009331\n",
      "Epoch [210/500], Step [3072/3451 (89%)]\tLoss: 0.085849\n",
      "Epoch [210/500], Average Loss: 0.0728\n",
      "Epoch [211/500], Step [0/3451 (0%)]\tLoss: 0.071748\n",
      "Epoch [211/500], Step [1024/3451 (30%)]\tLoss: 0.124239\n",
      "Epoch [211/500], Step [2048/3451 (59%)]\tLoss: 0.082000\n",
      "Epoch [211/500], Step [3072/3451 (89%)]\tLoss: 0.044896\n",
      "Epoch [211/500], Average Loss: 0.0712\n",
      "\n",
      "Test set: Avg. loss: 4.8434 Accuracy: 336/863 (38.93%)\n",
      "\n",
      "Epoch [212/500], Step [0/3451 (0%)]\tLoss: 0.043876\n",
      "Epoch [212/500], Step [1024/3451 (30%)]\tLoss: 0.052470\n",
      "Epoch [212/500], Step [2048/3451 (59%)]\tLoss: 0.075358\n",
      "Epoch [212/500], Step [3072/3451 (89%)]\tLoss: 0.107340\n",
      "Epoch [212/500], Average Loss: 0.0713\n",
      "Epoch [213/500], Step [0/3451 (0%)]\tLoss: 0.071190\n",
      "Epoch [213/500], Step [1024/3451 (30%)]\tLoss: 0.025450\n",
      "Epoch [213/500], Step [2048/3451 (59%)]\tLoss: 0.043070\n",
      "Epoch [213/500], Step [3072/3451 (89%)]\tLoss: 0.051758\n",
      "Epoch [213/500], Average Loss: 0.0720\n",
      "Epoch [214/500], Step [0/3451 (0%)]\tLoss: 0.026769\n",
      "Epoch [214/500], Step [1024/3451 (30%)]\tLoss: 0.039662\n",
      "Epoch [214/500], Step [2048/3451 (59%)]\tLoss: 0.097817\n",
      "Epoch [214/500], Step [3072/3451 (89%)]\tLoss: 0.078325\n",
      "Epoch [214/500], Average Loss: 0.0711\n",
      "Epoch [215/500], Step [0/3451 (0%)]\tLoss: 0.110170\n",
      "Epoch [215/500], Step [1024/3451 (30%)]\tLoss: 0.078249\n",
      "Epoch [215/500], Step [2048/3451 (59%)]\tLoss: 0.070516\n",
      "Epoch [215/500], Step [3072/3451 (89%)]\tLoss: 0.093972\n",
      "Epoch [215/500], Average Loss: 0.0717\n",
      "Epoch [216/500], Step [0/3451 (0%)]\tLoss: 0.018960\n",
      "Epoch [216/500], Step [1024/3451 (30%)]\tLoss: 0.072367\n",
      "Epoch [216/500], Step [2048/3451 (59%)]\tLoss: 0.058263\n",
      "Epoch [216/500], Step [3072/3451 (89%)]\tLoss: 0.116549\n",
      "Epoch [216/500], Average Loss: 0.0713\n",
      "\n",
      "Test set: Avg. loss: 4.8048 Accuracy: 333/863 (38.59%)\n",
      "\n",
      "Epoch [217/500], Step [0/3451 (0%)]\tLoss: 0.102373\n",
      "Epoch [217/500], Step [1024/3451 (30%)]\tLoss: 0.053118\n",
      "Epoch [217/500], Step [2048/3451 (59%)]\tLoss: 0.021249\n",
      "Epoch [217/500], Step [3072/3451 (89%)]\tLoss: 0.064032\n",
      "Epoch [217/500], Average Loss: 0.0754\n",
      "Epoch [218/500], Step [0/3451 (0%)]\tLoss: 0.023302\n",
      "Epoch [218/500], Step [1024/3451 (30%)]\tLoss: 0.024498\n",
      "Epoch [218/500], Step [2048/3451 (59%)]\tLoss: 0.037925\n",
      "Epoch [218/500], Step [3072/3451 (89%)]\tLoss: 0.106436\n",
      "Epoch [218/500], Average Loss: 0.0708\n",
      "Epoch [219/500], Step [0/3451 (0%)]\tLoss: 0.070022\n",
      "Epoch [219/500], Step [1024/3451 (30%)]\tLoss: 0.036403\n",
      "Epoch [219/500], Step [2048/3451 (59%)]\tLoss: 0.079497\n",
      "Epoch [219/500], Step [3072/3451 (89%)]\tLoss: 0.027486\n",
      "Epoch [219/500], Average Loss: 0.0715\n",
      "Epoch [220/500], Step [0/3451 (0%)]\tLoss: 0.069141\n",
      "Epoch [220/500], Step [1024/3451 (30%)]\tLoss: 0.136074\n",
      "Epoch [220/500], Step [2048/3451 (59%)]\tLoss: 0.090285\n",
      "Epoch [220/500], Step [3072/3451 (89%)]\tLoss: 0.088646\n",
      "Epoch [220/500], Average Loss: 0.0713\n",
      "Epoch [221/500], Step [0/3451 (0%)]\tLoss: 0.061990\n",
      "Epoch [221/500], Step [1024/3451 (30%)]\tLoss: 0.088088\n",
      "Epoch [221/500], Step [2048/3451 (59%)]\tLoss: 0.061404\n",
      "Epoch [221/500], Step [3072/3451 (89%)]\tLoss: 0.077549\n",
      "Epoch [221/500], Average Loss: 0.0720\n",
      "\n",
      "Test set: Avg. loss: 4.8772 Accuracy: 338/863 (39.17%)\n",
      "\n",
      "Epoch [222/500], Step [0/3451 (0%)]\tLoss: 0.086345\n",
      "Epoch [222/500], Step [1024/3451 (30%)]\tLoss: 0.043992\n",
      "Epoch [222/500], Step [2048/3451 (59%)]\tLoss: 0.054694\n",
      "Epoch [222/500], Step [3072/3451 (89%)]\tLoss: 0.058010\n",
      "Epoch [222/500], Average Loss: 0.0717\n",
      "Epoch [223/500], Step [0/3451 (0%)]\tLoss: 0.012137\n",
      "Epoch [223/500], Step [1024/3451 (30%)]\tLoss: 0.061420\n",
      "Epoch [223/500], Step [2048/3451 (59%)]\tLoss: 0.097936\n",
      "Epoch [223/500], Step [3072/3451 (89%)]\tLoss: 0.157240\n",
      "Epoch [223/500], Average Loss: 0.0715\n",
      "Epoch [224/500], Step [0/3451 (0%)]\tLoss: 0.096756\n",
      "Epoch [224/500], Step [1024/3451 (30%)]\tLoss: 0.087988\n",
      "Epoch [224/500], Step [2048/3451 (59%)]\tLoss: 0.121241\n",
      "Epoch [224/500], Step [3072/3451 (89%)]\tLoss: 0.075646\n",
      "Epoch [224/500], Average Loss: 0.0710\n",
      "Epoch [225/500], Step [0/3451 (0%)]\tLoss: 0.029379\n",
      "Epoch [225/500], Step [1024/3451 (30%)]\tLoss: 0.029425\n",
      "Epoch [225/500], Step [2048/3451 (59%)]\tLoss: 0.100711\n",
      "Epoch [225/500], Step [3072/3451 (89%)]\tLoss: 0.057779\n",
      "Epoch [225/500], Average Loss: 0.0714\n",
      "Epoch [226/500], Step [0/3451 (0%)]\tLoss: 0.058769\n",
      "Epoch [226/500], Step [1024/3451 (30%)]\tLoss: 0.153297\n",
      "Epoch [226/500], Step [2048/3451 (59%)]\tLoss: 0.063073\n",
      "Epoch [226/500], Step [3072/3451 (89%)]\tLoss: 0.074843\n",
      "Epoch [226/500], Average Loss: 0.0710\n",
      "\n",
      "Test set: Avg. loss: 4.9996 Accuracy: 339/863 (39.28%)\n",
      "\n",
      "Epoch [227/500], Step [0/3451 (0%)]\tLoss: 0.051975\n",
      "Epoch [227/500], Step [1024/3451 (30%)]\tLoss: 0.037230\n",
      "Epoch [227/500], Step [2048/3451 (59%)]\tLoss: 0.049460\n",
      "Epoch [227/500], Step [3072/3451 (89%)]\tLoss: 0.156643\n",
      "Epoch [227/500], Average Loss: 0.0713\n",
      "Epoch [228/500], Step [0/3451 (0%)]\tLoss: 0.061808\n",
      "Epoch [228/500], Step [1024/3451 (30%)]\tLoss: 0.037042\n",
      "Epoch [228/500], Step [2048/3451 (59%)]\tLoss: 0.048978\n",
      "Epoch [228/500], Step [3072/3451 (89%)]\tLoss: 0.022214\n",
      "Epoch [228/500], Average Loss: 0.0706\n",
      "Epoch [229/500], Step [0/3451 (0%)]\tLoss: 0.100213\n",
      "Epoch [229/500], Step [1024/3451 (30%)]\tLoss: 0.051391\n",
      "Epoch [229/500], Step [2048/3451 (59%)]\tLoss: 0.057987\n",
      "Epoch [229/500], Step [3072/3451 (89%)]\tLoss: 0.096296\n",
      "Epoch [229/500], Average Loss: 0.0709\n",
      "Epoch [230/500], Step [0/3451 (0%)]\tLoss: 0.042820\n",
      "Epoch [230/500], Step [1024/3451 (30%)]\tLoss: 0.125523\n",
      "Epoch [230/500], Step [2048/3451 (59%)]\tLoss: 0.051204\n",
      "Epoch [230/500], Step [3072/3451 (89%)]\tLoss: 0.093195\n",
      "Epoch [230/500], Average Loss: 0.0713\n",
      "Epoch [231/500], Step [0/3451 (0%)]\tLoss: 0.122024\n",
      "Epoch [231/500], Step [1024/3451 (30%)]\tLoss: 0.057826\n",
      "Epoch [231/500], Step [2048/3451 (59%)]\tLoss: 0.130404\n",
      "Epoch [231/500], Step [3072/3451 (89%)]\tLoss: 0.046229\n",
      "Epoch [231/500], Average Loss: 0.0710\n",
      "\n",
      "Test set: Avg. loss: 4.8577 Accuracy: 335/863 (38.82%)\n",
      "\n",
      "Epoch [232/500], Step [0/3451 (0%)]\tLoss: 0.059301\n",
      "Epoch [232/500], Step [1024/3451 (30%)]\tLoss: 0.077923\n",
      "Epoch [232/500], Step [2048/3451 (59%)]\tLoss: 0.088210\n",
      "Epoch [232/500], Step [3072/3451 (89%)]\tLoss: 0.123677\n",
      "Epoch [232/500], Average Loss: 0.0711\n",
      "Epoch [233/500], Step [0/3451 (0%)]\tLoss: 0.086076\n",
      "Epoch [233/500], Step [1024/3451 (30%)]\tLoss: 0.064184\n",
      "Epoch [233/500], Step [2048/3451 (59%)]\tLoss: 0.065819\n",
      "Epoch [233/500], Step [3072/3451 (89%)]\tLoss: 0.054996\n",
      "Epoch [233/500], Average Loss: 0.0706\n",
      "Epoch [234/500], Step [0/3451 (0%)]\tLoss: 0.042913\n",
      "Epoch [234/500], Step [1024/3451 (30%)]\tLoss: 0.011151\n",
      "Epoch [234/500], Step [2048/3451 (59%)]\tLoss: 0.032019\n",
      "Epoch [234/500], Step [3072/3451 (89%)]\tLoss: 0.072164\n",
      "Epoch [234/500], Average Loss: 0.0697\n",
      "Epoch [235/500], Step [0/3451 (0%)]\tLoss: 0.067912\n",
      "Epoch [235/500], Step [1024/3451 (30%)]\tLoss: 0.043299\n",
      "Epoch [235/500], Step [2048/3451 (59%)]\tLoss: 0.074571\n",
      "Epoch [235/500], Step [3072/3451 (89%)]\tLoss: 0.054868\n",
      "Epoch [235/500], Average Loss: 0.0707\n",
      "Epoch [236/500], Step [0/3451 (0%)]\tLoss: 0.063187\n",
      "Epoch [236/500], Step [1024/3451 (30%)]\tLoss: 0.173315\n",
      "Epoch [236/500], Step [2048/3451 (59%)]\tLoss: 0.045815\n",
      "Epoch [236/500], Step [3072/3451 (89%)]\tLoss: 0.111753\n",
      "Epoch [236/500], Average Loss: 0.0715\n",
      "\n",
      "Test set: Avg. loss: 4.8542 Accuracy: 332/863 (38.47%)\n",
      "\n",
      "Epoch [237/500], Step [0/3451 (0%)]\tLoss: 0.006960\n",
      "Epoch [237/500], Step [1024/3451 (30%)]\tLoss: 0.106535\n",
      "Epoch [237/500], Step [2048/3451 (59%)]\tLoss: 0.048382\n",
      "Epoch [237/500], Step [3072/3451 (89%)]\tLoss: 0.060221\n",
      "Epoch [237/500], Average Loss: 0.0701\n",
      "Epoch [238/500], Step [0/3451 (0%)]\tLoss: 0.039439\n",
      "Epoch [238/500], Step [1024/3451 (30%)]\tLoss: 0.061797\n",
      "Epoch [238/500], Step [2048/3451 (59%)]\tLoss: 0.048650\n",
      "Epoch [238/500], Step [3072/3451 (89%)]\tLoss: 0.112755\n",
      "Epoch [238/500], Average Loss: 0.0712\n",
      "Epoch [239/500], Step [0/3451 (0%)]\tLoss: 0.051689\n",
      "Epoch [239/500], Step [1024/3451 (30%)]\tLoss: 0.086509\n",
      "Epoch [239/500], Step [2048/3451 (59%)]\tLoss: 0.057182\n",
      "Epoch [239/500], Step [3072/3451 (89%)]\tLoss: 0.084538\n",
      "Epoch [239/500], Average Loss: 0.0705\n",
      "Epoch [240/500], Step [0/3451 (0%)]\tLoss: 0.082405\n",
      "Epoch [240/500], Step [1024/3451 (30%)]\tLoss: 0.110470\n",
      "Epoch [240/500], Step [2048/3451 (59%)]\tLoss: 0.064945\n",
      "Epoch [240/500], Step [3072/3451 (89%)]\tLoss: 0.032206\n",
      "Epoch [240/500], Average Loss: 0.0706\n",
      "Epoch [241/500], Step [0/3451 (0%)]\tLoss: 0.016159\n",
      "Epoch [241/500], Step [1024/3451 (30%)]\tLoss: 0.055817\n",
      "Epoch [241/500], Step [2048/3451 (59%)]\tLoss: 0.058362\n",
      "Epoch [241/500], Step [3072/3451 (89%)]\tLoss: 0.121547\n",
      "Epoch [241/500], Average Loss: 0.0707\n",
      "\n",
      "Test set: Avg. loss: 4.8992 Accuracy: 339/863 (39.28%)\n",
      "\n",
      "Epoch [242/500], Step [0/3451 (0%)]\tLoss: 0.092046\n",
      "Epoch [242/500], Step [1024/3451 (30%)]\tLoss: 0.046264\n",
      "Epoch [242/500], Step [2048/3451 (59%)]\tLoss: 0.105800\n",
      "Epoch [242/500], Step [3072/3451 (89%)]\tLoss: 0.085138\n",
      "Epoch [242/500], Average Loss: 0.0701\n",
      "Epoch [243/500], Step [0/3451 (0%)]\tLoss: 0.064385\n",
      "Epoch [243/500], Step [1024/3451 (30%)]\tLoss: 0.076105\n",
      "Epoch [243/500], Step [2048/3451 (59%)]\tLoss: 0.079142\n",
      "Epoch [243/500], Step [3072/3451 (89%)]\tLoss: 0.111514\n",
      "Epoch [243/500], Average Loss: 0.0705\n",
      "Epoch [244/500], Step [0/3451 (0%)]\tLoss: 0.104888\n",
      "Epoch [244/500], Step [1024/3451 (30%)]\tLoss: 0.111403\n",
      "Epoch [244/500], Step [2048/3451 (59%)]\tLoss: 0.078101\n",
      "Epoch [244/500], Step [3072/3451 (89%)]\tLoss: 0.088907\n",
      "Epoch [244/500], Average Loss: 0.0705\n",
      "Epoch [245/500], Step [0/3451 (0%)]\tLoss: 0.088330\n",
      "Epoch [245/500], Step [1024/3451 (30%)]\tLoss: 0.025588\n",
      "Epoch [245/500], Step [2048/3451 (59%)]\tLoss: 0.141754\n",
      "Epoch [245/500], Step [3072/3451 (89%)]\tLoss: 0.124949\n",
      "Epoch [245/500], Average Loss: 0.0697\n",
      "Epoch [246/500], Step [0/3451 (0%)]\tLoss: 0.047191\n",
      "Epoch [246/500], Step [1024/3451 (30%)]\tLoss: 0.035650\n",
      "Epoch [246/500], Step [2048/3451 (59%)]\tLoss: 0.092734\n",
      "Epoch [246/500], Step [3072/3451 (89%)]\tLoss: 0.070637\n",
      "Epoch [246/500], Average Loss: 0.0712\n",
      "\n",
      "Test set: Avg. loss: 4.7886 Accuracy: 333/863 (38.59%)\n",
      "\n",
      "Epoch [247/500], Step [0/3451 (0%)]\tLoss: 0.086020\n",
      "Epoch [247/500], Step [1024/3451 (30%)]\tLoss: 0.062037\n",
      "Epoch [247/500], Step [2048/3451 (59%)]\tLoss: 0.099265\n",
      "Epoch [247/500], Step [3072/3451 (89%)]\tLoss: 0.041313\n",
      "Epoch [247/500], Average Loss: 0.0694\n",
      "Epoch [248/500], Step [0/3451 (0%)]\tLoss: 0.134961\n",
      "Epoch [248/500], Step [1024/3451 (30%)]\tLoss: 0.056610\n",
      "Epoch [248/500], Step [2048/3451 (59%)]\tLoss: 0.086689\n",
      "Epoch [248/500], Step [3072/3451 (89%)]\tLoss: 0.052606\n",
      "Epoch [248/500], Average Loss: 0.0706\n",
      "Epoch [249/500], Step [0/3451 (0%)]\tLoss: 0.072353\n",
      "Epoch [249/500], Step [1024/3451 (30%)]\tLoss: 0.032486\n",
      "Epoch [249/500], Step [2048/3451 (59%)]\tLoss: 0.063413\n",
      "Epoch [249/500], Step [3072/3451 (89%)]\tLoss: 0.060804\n",
      "Epoch [249/500], Average Loss: 0.0699\n",
      "Epoch [250/500], Step [0/3451 (0%)]\tLoss: 0.043973\n",
      "Epoch [250/500], Step [1024/3451 (30%)]\tLoss: 0.241905\n",
      "Epoch [250/500], Step [2048/3451 (59%)]\tLoss: 0.028187\n",
      "Epoch [250/500], Step [3072/3451 (89%)]\tLoss: 0.147837\n",
      "Epoch [250/500], Average Loss: 0.0707\n",
      "Epoch [251/500], Step [0/3451 (0%)]\tLoss: 0.083086\n",
      "Epoch [251/500], Step [1024/3451 (30%)]\tLoss: 0.034756\n",
      "Epoch [251/500], Step [2048/3451 (59%)]\tLoss: 0.125554\n",
      "Epoch [251/500], Step [3072/3451 (89%)]\tLoss: 0.084694\n",
      "Epoch [251/500], Average Loss: 0.0709\n",
      "\n",
      "Test set: Avg. loss: 4.8594 Accuracy: 338/863 (39.17%)\n",
      "\n",
      "Epoch [252/500], Step [0/3451 (0%)]\tLoss: 0.013880\n",
      "Epoch [252/500], Step [1024/3451 (30%)]\tLoss: 0.029426\n",
      "Epoch [252/500], Step [2048/3451 (59%)]\tLoss: 0.135258\n",
      "Epoch [252/500], Step [3072/3451 (89%)]\tLoss: 0.030312\n",
      "Epoch [252/500], Average Loss: 0.0708\n",
      "Epoch [253/500], Step [0/3451 (0%)]\tLoss: 0.124511\n",
      "Epoch [253/500], Step [1024/3451 (30%)]\tLoss: 0.091486\n",
      "Epoch [253/500], Step [2048/3451 (59%)]\tLoss: 0.053644\n",
      "Epoch [253/500], Step [3072/3451 (89%)]\tLoss: 0.052376\n",
      "Epoch [253/500], Average Loss: 0.0697\n",
      "Epoch [254/500], Step [0/3451 (0%)]\tLoss: 0.094615\n",
      "Epoch [254/500], Step [1024/3451 (30%)]\tLoss: 0.042195\n",
      "Epoch [254/500], Step [2048/3451 (59%)]\tLoss: 0.099664\n",
      "Epoch [254/500], Step [3072/3451 (89%)]\tLoss: 0.065056\n",
      "Epoch [254/500], Average Loss: 0.0698\n",
      "Epoch [255/500], Step [0/3451 (0%)]\tLoss: 0.157078\n",
      "Epoch [255/500], Step [1024/3451 (30%)]\tLoss: 0.077467\n",
      "Epoch [255/500], Step [2048/3451 (59%)]\tLoss: 0.045375\n",
      "Epoch [255/500], Step [3072/3451 (89%)]\tLoss: 0.026863\n",
      "Epoch [255/500], Average Loss: 0.0712\n",
      "Epoch [256/500], Step [0/3451 (0%)]\tLoss: 0.074433\n",
      "Epoch [256/500], Step [1024/3451 (30%)]\tLoss: 0.039765\n",
      "Epoch [256/500], Step [2048/3451 (59%)]\tLoss: 0.037942\n",
      "Epoch [256/500], Step [3072/3451 (89%)]\tLoss: 0.110110\n",
      "Epoch [256/500], Average Loss: 0.0703\n",
      "\n",
      "Test set: Avg. loss: 4.8170 Accuracy: 334/863 (38.70%)\n",
      "\n",
      "Epoch [257/500], Step [0/3451 (0%)]\tLoss: 0.036876\n",
      "Epoch [257/500], Step [1024/3451 (30%)]\tLoss: 0.043913\n",
      "Epoch [257/500], Step [2048/3451 (59%)]\tLoss: 0.092768\n",
      "Epoch [257/500], Step [3072/3451 (89%)]\tLoss: 0.039069\n",
      "Epoch [257/500], Average Loss: 0.0708\n",
      "Epoch [258/500], Step [0/3451 (0%)]\tLoss: 0.058953\n",
      "Epoch [258/500], Step [1024/3451 (30%)]\tLoss: 0.055632\n",
      "Epoch [258/500], Step [2048/3451 (59%)]\tLoss: 0.049906\n",
      "Epoch [258/500], Step [3072/3451 (89%)]\tLoss: 0.043937\n",
      "Epoch 00258: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch [258/500], Average Loss: 0.0698\n",
      "Epoch [259/500], Step [0/3451 (0%)]\tLoss: 0.048940\n",
      "Epoch [259/500], Step [1024/3451 (30%)]\tLoss: 0.109073\n",
      "Epoch [259/500], Step [2048/3451 (59%)]\tLoss: 0.022892\n",
      "Epoch [259/500], Step [3072/3451 (89%)]\tLoss: 0.043927\n",
      "Epoch [259/500], Average Loss: 0.0695\n",
      "Epoch [260/500], Step [0/3451 (0%)]\tLoss: 0.047916\n",
      "Epoch [260/500], Step [1024/3451 (30%)]\tLoss: 0.081487\n",
      "Epoch [260/500], Step [2048/3451 (59%)]\tLoss: 0.048257\n",
      "Epoch [260/500], Step [3072/3451 (89%)]\tLoss: 0.049696\n",
      "Epoch [260/500], Average Loss: 0.0696\n",
      "Epoch [261/500], Step [0/3451 (0%)]\tLoss: 0.070037\n",
      "Epoch [261/500], Step [1024/3451 (30%)]\tLoss: 0.011554\n",
      "Epoch [261/500], Step [2048/3451 (59%)]\tLoss: 0.116797\n",
      "Epoch [261/500], Step [3072/3451 (89%)]\tLoss: 0.153094\n",
      "Epoch [261/500], Average Loss: 0.0698\n",
      "\n",
      "Test set: Avg. loss: 4.8173 Accuracy: 343/863 (39.75%)\n",
      "\n",
      "Epoch [262/500], Step [0/3451 (0%)]\tLoss: 0.092851\n",
      "Epoch [262/500], Step [1024/3451 (30%)]\tLoss: 0.047613\n",
      "Epoch [262/500], Step [2048/3451 (59%)]\tLoss: 0.079020\n",
      "Epoch [262/500], Step [3072/3451 (89%)]\tLoss: 0.058739\n",
      "Epoch [262/500], Average Loss: 0.0692\n",
      "Epoch [263/500], Step [0/3451 (0%)]\tLoss: 0.036096\n",
      "Epoch [263/500], Step [1024/3451 (30%)]\tLoss: 0.101570\n",
      "Epoch [263/500], Step [2048/3451 (59%)]\tLoss: 0.063369\n",
      "Epoch [263/500], Step [3072/3451 (89%)]\tLoss: 0.077848\n",
      "Epoch [263/500], Average Loss: 0.0698\n",
      "Epoch [264/500], Step [0/3451 (0%)]\tLoss: 0.042682\n",
      "Epoch [264/500], Step [1024/3451 (30%)]\tLoss: 0.052894\n",
      "Epoch [264/500], Step [2048/3451 (59%)]\tLoss: 0.056052\n",
      "Epoch [264/500], Step [3072/3451 (89%)]\tLoss: 0.122456\n",
      "Epoch [264/500], Average Loss: 0.0711\n",
      "Epoch [265/500], Step [0/3451 (0%)]\tLoss: 0.041893\n",
      "Epoch [265/500], Step [1024/3451 (30%)]\tLoss: 0.048387\n",
      "Epoch [265/500], Step [2048/3451 (59%)]\tLoss: 0.066956\n",
      "Epoch [265/500], Step [3072/3451 (89%)]\tLoss: 0.090042\n",
      "Epoch [265/500], Average Loss: 0.0684\n",
      "Epoch [266/500], Step [0/3451 (0%)]\tLoss: 0.113428\n",
      "Epoch [266/500], Step [1024/3451 (30%)]\tLoss: 0.040716\n",
      "Epoch [266/500], Step [2048/3451 (59%)]\tLoss: 0.083665\n",
      "Epoch [266/500], Step [3072/3451 (89%)]\tLoss: 0.071537\n",
      "Epoch [266/500], Average Loss: 0.0697\n",
      "\n",
      "Test set: Avg. loss: 4.8616 Accuracy: 331/863 (38.35%)\n",
      "\n",
      "Epoch [267/500], Step [0/3451 (0%)]\tLoss: 0.110245\n",
      "Epoch [267/500], Step [1024/3451 (30%)]\tLoss: 0.077076\n",
      "Epoch [267/500], Step [2048/3451 (59%)]\tLoss: 0.012419\n",
      "Epoch [267/500], Step [3072/3451 (89%)]\tLoss: 0.043151\n",
      "Epoch [267/500], Average Loss: 0.0696\n",
      "Epoch [268/500], Step [0/3451 (0%)]\tLoss: 0.062811\n",
      "Epoch [268/500], Step [1024/3451 (30%)]\tLoss: 0.076038\n",
      "Epoch [268/500], Step [2048/3451 (59%)]\tLoss: 0.056201\n",
      "Epoch [268/500], Step [3072/3451 (89%)]\tLoss: 0.052152\n",
      "Epoch [268/500], Average Loss: 0.0701\n",
      "Epoch [269/500], Step [0/3451 (0%)]\tLoss: 0.106620\n",
      "Epoch [269/500], Step [1024/3451 (30%)]\tLoss: 0.033586\n",
      "Epoch [269/500], Step [2048/3451 (59%)]\tLoss: 0.132949\n",
      "Epoch [269/500], Step [3072/3451 (89%)]\tLoss: 0.036566\n",
      "Epoch [269/500], Average Loss: 0.0694\n",
      "Epoch [270/500], Step [0/3451 (0%)]\tLoss: 0.075267\n",
      "Epoch [270/500], Step [1024/3451 (30%)]\tLoss: 0.020422\n",
      "Epoch [270/500], Step [2048/3451 (59%)]\tLoss: 0.069198\n",
      "Epoch [270/500], Step [3072/3451 (89%)]\tLoss: 0.094261\n",
      "Epoch [270/500], Average Loss: 0.0707\n",
      "Epoch [271/500], Step [0/3451 (0%)]\tLoss: 0.107021\n",
      "Epoch [271/500], Step [1024/3451 (30%)]\tLoss: 0.032514\n",
      "Epoch [271/500], Step [2048/3451 (59%)]\tLoss: 0.047897\n",
      "Epoch [271/500], Step [3072/3451 (89%)]\tLoss: 0.112615\n",
      "Epoch [271/500], Average Loss: 0.0695\n",
      "\n",
      "Test set: Avg. loss: 4.9893 Accuracy: 342/863 (39.63%)\n",
      "\n",
      "Epoch [272/500], Step [0/3451 (0%)]\tLoss: 0.074286\n",
      "Epoch [272/500], Step [1024/3451 (30%)]\tLoss: 0.044270\n",
      "Epoch [272/500], Step [2048/3451 (59%)]\tLoss: 0.066965\n",
      "Epoch [272/500], Step [3072/3451 (89%)]\tLoss: 0.063141\n",
      "Epoch [272/500], Average Loss: 0.0706\n",
      "Epoch [273/500], Step [0/3451 (0%)]\tLoss: 0.046122\n",
      "Epoch [273/500], Step [1024/3451 (30%)]\tLoss: 0.060526\n",
      "Epoch [273/500], Step [2048/3451 (59%)]\tLoss: 0.119567\n",
      "Epoch [273/500], Step [3072/3451 (89%)]\tLoss: 0.066978\n",
      "Epoch [273/500], Average Loss: 0.0688\n",
      "Epoch [274/500], Step [0/3451 (0%)]\tLoss: 0.073359\n",
      "Epoch [274/500], Step [1024/3451 (30%)]\tLoss: 0.046084\n",
      "Epoch [274/500], Step [2048/3451 (59%)]\tLoss: 0.053654\n",
      "Epoch [274/500], Step [3072/3451 (89%)]\tLoss: 0.054639\n",
      "Epoch [274/500], Average Loss: 0.0703\n",
      "Epoch [275/500], Step [0/3451 (0%)]\tLoss: 0.060712\n",
      "Epoch [275/500], Step [1024/3451 (30%)]\tLoss: 0.067563\n",
      "Epoch [275/500], Step [2048/3451 (59%)]\tLoss: 0.050872\n",
      "Epoch [275/500], Step [3072/3451 (89%)]\tLoss: 0.080292\n",
      "Epoch [275/500], Average Loss: 0.0696\n",
      "Epoch [276/500], Step [0/3451 (0%)]\tLoss: 0.057382\n",
      "Epoch [276/500], Step [1024/3451 (30%)]\tLoss: 0.021976\n",
      "Epoch [276/500], Step [2048/3451 (59%)]\tLoss: 0.114594\n",
      "Epoch [276/500], Step [3072/3451 (89%)]\tLoss: 0.077377\n",
      "Epoch 00276: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch [276/500], Average Loss: 0.0707\n",
      "\n",
      "Test set: Avg. loss: 4.9112 Accuracy: 336/863 (38.93%)\n",
      "\n",
      "Epoch [277/500], Step [0/3451 (0%)]\tLoss: 0.032373\n",
      "Epoch [277/500], Step [1024/3451 (30%)]\tLoss: 0.062249\n",
      "Epoch [277/500], Step [2048/3451 (59%)]\tLoss: 0.067854\n",
      "Epoch [277/500], Step [3072/3451 (89%)]\tLoss: 0.089322\n",
      "Epoch [277/500], Average Loss: 0.0709\n",
      "Epoch [278/500], Step [0/3451 (0%)]\tLoss: 0.036111\n",
      "Epoch [278/500], Step [1024/3451 (30%)]\tLoss: 0.054016\n",
      "Epoch [278/500], Step [2048/3451 (59%)]\tLoss: 0.036543\n",
      "Epoch [278/500], Step [3072/3451 (89%)]\tLoss: 0.101558\n",
      "Epoch [278/500], Average Loss: 0.0688\n",
      "Epoch [279/500], Step [0/3451 (0%)]\tLoss: 0.049345\n",
      "Epoch [279/500], Step [1024/3451 (30%)]\tLoss: 0.071807\n",
      "Epoch [279/500], Step [2048/3451 (59%)]\tLoss: 0.127987\n",
      "Epoch [279/500], Step [3072/3451 (89%)]\tLoss: 0.101022\n",
      "Epoch [279/500], Average Loss: 0.0688\n",
      "Epoch [280/500], Step [0/3451 (0%)]\tLoss: 0.195307\n",
      "Epoch [280/500], Step [1024/3451 (30%)]\tLoss: 0.075301\n",
      "Epoch [280/500], Step [2048/3451 (59%)]\tLoss: 0.046037\n",
      "Epoch [280/500], Step [3072/3451 (89%)]\tLoss: 0.069543\n",
      "Epoch [280/500], Average Loss: 0.0691\n",
      "Epoch [281/500], Step [0/3451 (0%)]\tLoss: 0.107304\n",
      "Epoch [281/500], Step [1024/3451 (30%)]\tLoss: 0.072831\n",
      "Epoch [281/500], Step [2048/3451 (59%)]\tLoss: 0.076028\n",
      "Epoch [281/500], Step [3072/3451 (89%)]\tLoss: 0.045570\n",
      "Epoch [281/500], Average Loss: 0.0686\n",
      "\n",
      "Test set: Avg. loss: 4.8565 Accuracy: 339/863 (39.28%)\n",
      "\n",
      "Epoch [282/500], Step [0/3451 (0%)]\tLoss: 0.110444\n",
      "Epoch [282/500], Step [1024/3451 (30%)]\tLoss: 0.075571\n",
      "Epoch [282/500], Step [2048/3451 (59%)]\tLoss: 0.032990\n",
      "Epoch [282/500], Step [3072/3451 (89%)]\tLoss: 0.068199\n",
      "Epoch [282/500], Average Loss: 0.0695\n",
      "Epoch [283/500], Step [0/3451 (0%)]\tLoss: 0.089061\n",
      "Epoch [283/500], Step [1024/3451 (30%)]\tLoss: 0.115756\n",
      "Epoch [283/500], Step [2048/3451 (59%)]\tLoss: 0.020544\n",
      "Epoch [283/500], Step [3072/3451 (89%)]\tLoss: 0.053778\n",
      "Epoch [283/500], Average Loss: 0.0695\n",
      "Epoch [284/500], Step [0/3451 (0%)]\tLoss: 0.047056\n",
      "Epoch [284/500], Step [1024/3451 (30%)]\tLoss: 0.103126\n",
      "Epoch [284/500], Step [2048/3451 (59%)]\tLoss: 0.092879\n",
      "Epoch [284/500], Step [3072/3451 (89%)]\tLoss: 0.152950\n",
      "Epoch [284/500], Average Loss: 0.0691\n",
      "Epoch [285/500], Step [0/3451 (0%)]\tLoss: 0.048867\n",
      "Epoch [285/500], Step [1024/3451 (30%)]\tLoss: 0.021481\n",
      "Epoch [285/500], Step [2048/3451 (59%)]\tLoss: 0.057250\n",
      "Epoch [285/500], Step [3072/3451 (89%)]\tLoss: 0.115469\n",
      "Epoch [285/500], Average Loss: 0.0690\n",
      "Epoch [286/500], Step [0/3451 (0%)]\tLoss: 0.040593\n",
      "Epoch [286/500], Step [1024/3451 (30%)]\tLoss: 0.054401\n",
      "Epoch [286/500], Step [2048/3451 (59%)]\tLoss: 0.125831\n",
      "Epoch [286/500], Step [3072/3451 (89%)]\tLoss: 0.102840\n",
      "Epoch [286/500], Average Loss: 0.0693\n",
      "\n",
      "Test set: Avg. loss: 4.8486 Accuracy: 330/863 (38.24%)\n",
      "\n",
      "Epoch [287/500], Step [0/3451 (0%)]\tLoss: 0.092933\n",
      "Epoch [287/500], Step [1024/3451 (30%)]\tLoss: 0.138425\n",
      "Epoch [287/500], Step [2048/3451 (59%)]\tLoss: 0.030586\n",
      "Epoch [287/500], Step [3072/3451 (89%)]\tLoss: 0.092515\n",
      "Epoch [287/500], Average Loss: 0.0697\n",
      "Epoch [288/500], Step [0/3451 (0%)]\tLoss: 0.096234\n",
      "Epoch [288/500], Step [1024/3451 (30%)]\tLoss: 0.124677\n",
      "Epoch [288/500], Step [2048/3451 (59%)]\tLoss: 0.043446\n",
      "Epoch [288/500], Step [3072/3451 (89%)]\tLoss: 0.047729\n",
      "Epoch [288/500], Average Loss: 0.0691\n",
      "Epoch [289/500], Step [0/3451 (0%)]\tLoss: 0.068259\n",
      "Epoch [289/500], Step [1024/3451 (30%)]\tLoss: 0.098456\n",
      "Epoch [289/500], Step [2048/3451 (59%)]\tLoss: 0.075997\n",
      "Epoch [289/500], Step [3072/3451 (89%)]\tLoss: 0.073115\n",
      "Epoch [289/500], Average Loss: 0.0706\n",
      "Epoch [290/500], Step [0/3451 (0%)]\tLoss: 0.049979\n",
      "Epoch [290/500], Step [1024/3451 (30%)]\tLoss: 0.081094\n",
      "Epoch [290/500], Step [2048/3451 (59%)]\tLoss: 0.082851\n",
      "Epoch [290/500], Step [3072/3451 (89%)]\tLoss: 0.052886\n",
      "Epoch [290/500], Average Loss: 0.0703\n",
      "Epoch [291/500], Step [0/3451 (0%)]\tLoss: 0.052384\n",
      "Epoch [291/500], Step [1024/3451 (30%)]\tLoss: 0.152420\n",
      "Epoch [291/500], Step [2048/3451 (59%)]\tLoss: 0.066163\n",
      "Epoch [291/500], Step [3072/3451 (89%)]\tLoss: 0.083570\n",
      "Epoch [291/500], Average Loss: 0.0710\n",
      "\n",
      "Test set: Avg. loss: 5.0066 Accuracy: 335/863 (38.82%)\n",
      "\n",
      "Epoch [292/500], Step [0/3451 (0%)]\tLoss: 0.055543\n",
      "Epoch [292/500], Step [1024/3451 (30%)]\tLoss: 0.067647\n",
      "Epoch [292/500], Step [2048/3451 (59%)]\tLoss: 0.045292\n",
      "Epoch [292/500], Step [3072/3451 (89%)]\tLoss: 0.040804\n",
      "Epoch [292/500], Average Loss: 0.0690\n",
      "Epoch [293/500], Step [0/3451 (0%)]\tLoss: 0.148434\n",
      "Epoch [293/500], Step [1024/3451 (30%)]\tLoss: 0.098569\n",
      "Epoch [293/500], Step [2048/3451 (59%)]\tLoss: 0.073445\n",
      "Epoch [293/500], Step [3072/3451 (89%)]\tLoss: 0.092639\n",
      "Epoch [293/500], Average Loss: 0.0700\n",
      "Epoch [294/500], Step [0/3451 (0%)]\tLoss: 0.032246\n",
      "Epoch [294/500], Step [1024/3451 (30%)]\tLoss: 0.038682\n",
      "Epoch [294/500], Step [2048/3451 (59%)]\tLoss: 0.120298\n",
      "Epoch [294/500], Step [3072/3451 (89%)]\tLoss: 0.027759\n",
      "Epoch [294/500], Average Loss: 0.0692\n",
      "Epoch [295/500], Step [0/3451 (0%)]\tLoss: 0.028242\n",
      "Epoch [295/500], Step [1024/3451 (30%)]\tLoss: 0.062953\n",
      "Epoch [295/500], Step [2048/3451 (59%)]\tLoss: 0.051599\n",
      "Epoch [295/500], Step [3072/3451 (89%)]\tLoss: 0.088264\n",
      "Epoch [295/500], Average Loss: 0.0695\n",
      "Epoch [296/500], Step [0/3451 (0%)]\tLoss: 0.131851\n",
      "Epoch [296/500], Step [1024/3451 (30%)]\tLoss: 0.058490\n",
      "Epoch [296/500], Step [2048/3451 (59%)]\tLoss: 0.058839\n",
      "Epoch [296/500], Step [3072/3451 (89%)]\tLoss: 0.089200\n",
      "Epoch [296/500], Average Loss: 0.0687\n",
      "\n",
      "Test set: Avg. loss: 4.8565 Accuracy: 331/863 (38.35%)\n",
      "\n",
      "Epoch [297/500], Step [0/3451 (0%)]\tLoss: 0.082158\n",
      "Epoch [297/500], Step [1024/3451 (30%)]\tLoss: 0.104772\n",
      "Epoch [297/500], Step [2048/3451 (59%)]\tLoss: 0.063504\n",
      "Epoch [297/500], Step [3072/3451 (89%)]\tLoss: 0.058117\n",
      "Epoch [297/500], Average Loss: 0.0709\n",
      "Epoch [298/500], Step [0/3451 (0%)]\tLoss: 0.063961\n",
      "Epoch [298/500], Step [1024/3451 (30%)]\tLoss: 0.090828\n",
      "Epoch [298/500], Step [2048/3451 (59%)]\tLoss: 0.085781\n",
      "Epoch [298/500], Step [3072/3451 (89%)]\tLoss: 0.026943\n",
      "Epoch [298/500], Average Loss: 0.0704\n",
      "Epoch [299/500], Step [0/3451 (0%)]\tLoss: 0.033461\n",
      "Epoch [299/500], Step [1024/3451 (30%)]\tLoss: 0.064083\n",
      "Epoch [299/500], Step [2048/3451 (59%)]\tLoss: 0.134489\n",
      "Epoch [299/500], Step [3072/3451 (89%)]\tLoss: 0.026693\n",
      "Epoch [299/500], Average Loss: 0.0688\n",
      "Epoch [300/500], Step [0/3451 (0%)]\tLoss: 0.080867\n",
      "Epoch [300/500], Step [1024/3451 (30%)]\tLoss: 0.017790\n",
      "Epoch [300/500], Step [2048/3451 (59%)]\tLoss: 0.049071\n",
      "Epoch [300/500], Step [3072/3451 (89%)]\tLoss: 0.106581\n",
      "Epoch [300/500], Average Loss: 0.0693\n",
      "Epoch [301/500], Step [0/3451 (0%)]\tLoss: 0.120664\n",
      "Epoch [301/500], Step [1024/3451 (30%)]\tLoss: 0.078404\n",
      "Epoch [301/500], Step [2048/3451 (59%)]\tLoss: 0.050932\n",
      "Epoch [301/500], Step [3072/3451 (89%)]\tLoss: 0.035656\n",
      "Epoch [301/500], Average Loss: 0.0688\n",
      "\n",
      "Test set: Avg. loss: 4.8895 Accuracy: 332/863 (38.47%)\n",
      "\n",
      "Epoch [302/500], Step [0/3451 (0%)]\tLoss: 0.008186\n",
      "Epoch [302/500], Step [1024/3451 (30%)]\tLoss: 0.024339\n",
      "Epoch [302/500], Step [2048/3451 (59%)]\tLoss: 0.041651\n",
      "Epoch [302/500], Step [3072/3451 (89%)]\tLoss: 0.045104\n",
      "Epoch [302/500], Average Loss: 0.0695\n",
      "Epoch [303/500], Step [0/3451 (0%)]\tLoss: 0.100006\n",
      "Epoch [303/500], Step [1024/3451 (30%)]\tLoss: 0.052186\n",
      "Epoch [303/500], Step [2048/3451 (59%)]\tLoss: 0.074552\n",
      "Epoch [303/500], Step [3072/3451 (89%)]\tLoss: 0.100140\n",
      "Epoch [303/500], Average Loss: 0.0700\n",
      "Epoch [304/500], Step [0/3451 (0%)]\tLoss: 0.082771\n",
      "Epoch [304/500], Step [1024/3451 (30%)]\tLoss: 0.038946\n",
      "Epoch [304/500], Step [2048/3451 (59%)]\tLoss: 0.046327\n",
      "Epoch [304/500], Step [3072/3451 (89%)]\tLoss: 0.075872\n",
      "Epoch [304/500], Average Loss: 0.0693\n",
      "Epoch [305/500], Step [0/3451 (0%)]\tLoss: 0.098231\n",
      "Epoch [305/500], Step [1024/3451 (30%)]\tLoss: 0.120186\n",
      "Epoch [305/500], Step [2048/3451 (59%)]\tLoss: 0.079229\n",
      "Epoch [305/500], Step [3072/3451 (89%)]\tLoss: 0.115897\n",
      "Epoch [305/500], Average Loss: 0.0707\n",
      "Epoch [306/500], Step [0/3451 (0%)]\tLoss: 0.091272\n",
      "Epoch [306/500], Step [1024/3451 (30%)]\tLoss: 0.077264\n",
      "Epoch [306/500], Step [2048/3451 (59%)]\tLoss: 0.111992\n",
      "Epoch [306/500], Step [3072/3451 (89%)]\tLoss: 0.025672\n",
      "Epoch [306/500], Average Loss: 0.0694\n",
      "\n",
      "Test set: Avg. loss: 4.9096 Accuracy: 333/863 (38.59%)\n",
      "\n",
      "Epoch [307/500], Step [0/3451 (0%)]\tLoss: 0.132292\n",
      "Epoch [307/500], Step [1024/3451 (30%)]\tLoss: 0.093029\n",
      "Epoch [307/500], Step [2048/3451 (59%)]\tLoss: 0.092843\n",
      "Epoch [307/500], Step [3072/3451 (89%)]\tLoss: 0.169958\n",
      "Epoch [307/500], Average Loss: 0.0694\n",
      "Epoch [308/500], Step [0/3451 (0%)]\tLoss: 0.067572\n",
      "Epoch [308/500], Step [1024/3451 (30%)]\tLoss: 0.025090\n",
      "Epoch [308/500], Step [2048/3451 (59%)]\tLoss: 0.024368\n",
      "Epoch [308/500], Step [3072/3451 (89%)]\tLoss: 0.180433\n",
      "Epoch [308/500], Average Loss: 0.0707\n",
      "Epoch [309/500], Step [0/3451 (0%)]\tLoss: 0.071378\n",
      "Epoch [309/500], Step [1024/3451 (30%)]\tLoss: 0.065542\n",
      "Epoch [309/500], Step [2048/3451 (59%)]\tLoss: 0.063709\n",
      "Epoch [309/500], Step [3072/3451 (89%)]\tLoss: 0.090928\n",
      "Epoch [309/500], Average Loss: 0.0692\n",
      "Epoch [310/500], Step [0/3451 (0%)]\tLoss: 0.067892\n",
      "Epoch [310/500], Step [1024/3451 (30%)]\tLoss: 0.124910\n",
      "Epoch [310/500], Step [2048/3451 (59%)]\tLoss: 0.034266\n",
      "Epoch [310/500], Step [3072/3451 (89%)]\tLoss: 0.073603\n",
      "Epoch [310/500], Average Loss: 0.0703\n",
      "Epoch [311/500], Step [0/3451 (0%)]\tLoss: 0.029147\n",
      "Epoch [311/500], Step [1024/3451 (30%)]\tLoss: 0.104460\n",
      "Epoch [311/500], Step [2048/3451 (59%)]\tLoss: 0.112795\n",
      "Epoch [311/500], Step [3072/3451 (89%)]\tLoss: 0.041145\n",
      "Epoch [311/500], Average Loss: 0.0694\n",
      "\n",
      "Test set: Avg. loss: 4.7575 Accuracy: 335/863 (38.82%)\n",
      "\n",
      "Epoch [312/500], Step [0/3451 (0%)]\tLoss: 0.107473\n",
      "Epoch [312/500], Step [1024/3451 (30%)]\tLoss: 0.052704\n",
      "Epoch [312/500], Step [2048/3451 (59%)]\tLoss: 0.167077\n",
      "Epoch [312/500], Step [3072/3451 (89%)]\tLoss: 0.125432\n",
      "Epoch [312/500], Average Loss: 0.0715\n",
      "Epoch [313/500], Step [0/3451 (0%)]\tLoss: 0.137534\n",
      "Epoch [313/500], Step [1024/3451 (30%)]\tLoss: 0.067525\n",
      "Epoch [313/500], Step [2048/3451 (59%)]\tLoss: 0.110696\n",
      "Epoch [313/500], Step [3072/3451 (89%)]\tLoss: 0.062829\n",
      "Epoch [313/500], Average Loss: 0.0694\n",
      "Epoch [314/500], Step [0/3451 (0%)]\tLoss: 0.039019\n",
      "Epoch [314/500], Step [1024/3451 (30%)]\tLoss: 0.058350\n",
      "Epoch [314/500], Step [2048/3451 (59%)]\tLoss: 0.104772\n",
      "Epoch [314/500], Step [3072/3451 (89%)]\tLoss: 0.083910\n",
      "Epoch [314/500], Average Loss: 0.0707\n",
      "Epoch [315/500], Step [0/3451 (0%)]\tLoss: 0.075902\n",
      "Epoch [315/500], Step [1024/3451 (30%)]\tLoss: 0.081109\n",
      "Epoch [315/500], Step [2048/3451 (59%)]\tLoss: 0.066019\n",
      "Epoch [315/500], Step [3072/3451 (89%)]\tLoss: 0.067763\n",
      "Epoch [315/500], Average Loss: 0.0695\n",
      "Epoch [316/500], Step [0/3451 (0%)]\tLoss: 0.057977\n",
      "Epoch [316/500], Step [1024/3451 (30%)]\tLoss: 0.040011\n",
      "Epoch [316/500], Step [2048/3451 (59%)]\tLoss: 0.023462\n",
      "Epoch [316/500], Step [3072/3451 (89%)]\tLoss: 0.022182\n",
      "Epoch [316/500], Average Loss: 0.0689\n",
      "\n",
      "Test set: Avg. loss: 5.0351 Accuracy: 338/863 (39.17%)\n",
      "\n",
      "Epoch [317/500], Step [0/3451 (0%)]\tLoss: 0.043272\n",
      "Epoch [317/500], Step [1024/3451 (30%)]\tLoss: 0.159657\n",
      "Epoch [317/500], Step [2048/3451 (59%)]\tLoss: 0.035467\n",
      "Epoch [317/500], Step [3072/3451 (89%)]\tLoss: 0.087410\n",
      "Epoch [317/500], Average Loss: 0.0696\n",
      "Epoch [318/500], Step [0/3451 (0%)]\tLoss: 0.035969\n",
      "Epoch [318/500], Step [1024/3451 (30%)]\tLoss: 0.036212\n",
      "Epoch [318/500], Step [2048/3451 (59%)]\tLoss: 0.022111\n",
      "Epoch [318/500], Step [3072/3451 (89%)]\tLoss: 0.103254\n",
      "Epoch [318/500], Average Loss: 0.0701\n",
      "Epoch [319/500], Step [0/3451 (0%)]\tLoss: 0.074183\n",
      "Epoch [319/500], Step [1024/3451 (30%)]\tLoss: 0.039086\n",
      "Epoch [319/500], Step [2048/3451 (59%)]\tLoss: 0.056533\n",
      "Epoch [319/500], Step [3072/3451 (89%)]\tLoss: 0.095904\n",
      "Epoch [319/500], Average Loss: 0.0693\n",
      "Epoch [320/500], Step [0/3451 (0%)]\tLoss: 0.041812\n",
      "Epoch [320/500], Step [1024/3451 (30%)]\tLoss: 0.160429\n",
      "Epoch [320/500], Step [2048/3451 (59%)]\tLoss: 0.048596\n",
      "Epoch [320/500], Step [3072/3451 (89%)]\tLoss: 0.103604\n",
      "Epoch [320/500], Average Loss: 0.0705\n",
      "Epoch [321/500], Step [0/3451 (0%)]\tLoss: 0.010147\n",
      "Epoch [321/500], Step [1024/3451 (30%)]\tLoss: 0.070573\n",
      "Epoch [321/500], Step [2048/3451 (59%)]\tLoss: 0.035338\n",
      "Epoch [321/500], Step [3072/3451 (89%)]\tLoss: 0.030756\n",
      "Epoch [321/500], Average Loss: 0.0696\n",
      "\n",
      "Test set: Avg. loss: 4.8782 Accuracy: 333/863 (38.59%)\n",
      "\n",
      "Epoch [322/500], Step [0/3451 (0%)]\tLoss: 0.073565\n",
      "Epoch [322/500], Step [1024/3451 (30%)]\tLoss: 0.095719\n",
      "Epoch [322/500], Step [2048/3451 (59%)]\tLoss: 0.031948\n",
      "Epoch [322/500], Step [3072/3451 (89%)]\tLoss: 0.046769\n",
      "Epoch [322/500], Average Loss: 0.0691\n",
      "Epoch [323/500], Step [0/3451 (0%)]\tLoss: 0.100324\n",
      "Epoch [323/500], Step [1024/3451 (30%)]\tLoss: 0.087742\n",
      "Epoch [323/500], Step [2048/3451 (59%)]\tLoss: 0.078628\n",
      "Epoch [323/500], Step [3072/3451 (89%)]\tLoss: 0.063652\n",
      "Epoch [323/500], Average Loss: 0.0693\n",
      "Epoch [324/500], Step [0/3451 (0%)]\tLoss: 0.080948\n",
      "Epoch [324/500], Step [1024/3451 (30%)]\tLoss: 0.069913\n",
      "Epoch [324/500], Step [2048/3451 (59%)]\tLoss: 0.108339\n",
      "Epoch [324/500], Step [3072/3451 (89%)]\tLoss: 0.060207\n",
      "Epoch [324/500], Average Loss: 0.0694\n",
      "Epoch [325/500], Step [0/3451 (0%)]\tLoss: 0.049010\n",
      "Epoch [325/500], Step [1024/3451 (30%)]\tLoss: 0.086729\n",
      "Epoch [325/500], Step [2048/3451 (59%)]\tLoss: 0.113606\n",
      "Epoch [325/500], Step [3072/3451 (89%)]\tLoss: 0.105388\n",
      "Epoch [325/500], Average Loss: 0.0702\n",
      "Epoch [326/500], Step [0/3451 (0%)]\tLoss: 0.061634\n",
      "Epoch [326/500], Step [1024/3451 (30%)]\tLoss: 0.076003\n",
      "Epoch [326/500], Step [2048/3451 (59%)]\tLoss: 0.082169\n",
      "Epoch [326/500], Step [3072/3451 (89%)]\tLoss: 0.025275\n",
      "Epoch [326/500], Average Loss: 0.0694\n",
      "\n",
      "Test set: Avg. loss: 4.9421 Accuracy: 340/863 (39.40%)\n",
      "\n",
      "Epoch [327/500], Step [0/3451 (0%)]\tLoss: 0.064733\n",
      "Epoch [327/500], Step [1024/3451 (30%)]\tLoss: 0.083606\n",
      "Epoch [327/500], Step [2048/3451 (59%)]\tLoss: 0.088717\n",
      "Epoch [327/500], Step [3072/3451 (89%)]\tLoss: 0.039164\n",
      "Epoch [327/500], Average Loss: 0.0697\n",
      "Epoch [328/500], Step [0/3451 (0%)]\tLoss: 0.148340\n",
      "Epoch [328/500], Step [1024/3451 (30%)]\tLoss: 0.190667\n",
      "Epoch [328/500], Step [2048/3451 (59%)]\tLoss: 0.091319\n",
      "Epoch [328/500], Step [3072/3451 (89%)]\tLoss: 0.059118\n",
      "Epoch [328/500], Average Loss: 0.0706\n",
      "Epoch [329/500], Step [0/3451 (0%)]\tLoss: 0.067994\n",
      "Epoch [329/500], Step [1024/3451 (30%)]\tLoss: 0.069476\n",
      "Epoch [329/500], Step [2048/3451 (59%)]\tLoss: 0.068668\n",
      "Epoch [329/500], Step [3072/3451 (89%)]\tLoss: 0.078928\n",
      "Epoch [329/500], Average Loss: 0.0701\n",
      "Epoch [330/500], Step [0/3451 (0%)]\tLoss: 0.085144\n",
      "Epoch [330/500], Step [1024/3451 (30%)]\tLoss: 0.012873\n",
      "Epoch [330/500], Step [2048/3451 (59%)]\tLoss: 0.152003\n",
      "Epoch [330/500], Step [3072/3451 (89%)]\tLoss: 0.131045\n",
      "Epoch [330/500], Average Loss: 0.0695\n",
      "Epoch [331/500], Step [0/3451 (0%)]\tLoss: 0.062861\n",
      "Epoch [331/500], Step [1024/3451 (30%)]\tLoss: 0.094183\n",
      "Epoch [331/500], Step [2048/3451 (59%)]\tLoss: 0.112001\n",
      "Epoch [331/500], Step [3072/3451 (89%)]\tLoss: 0.035095\n",
      "Epoch [331/500], Average Loss: 0.0699\n",
      "\n",
      "Test set: Avg. loss: 4.9104 Accuracy: 339/863 (39.28%)\n",
      "\n",
      "Epoch [332/500], Step [0/3451 (0%)]\tLoss: 0.054493\n",
      "Epoch [332/500], Step [1024/3451 (30%)]\tLoss: 0.065550\n",
      "Epoch [332/500], Step [2048/3451 (59%)]\tLoss: 0.027621\n",
      "Epoch [332/500], Step [3072/3451 (89%)]\tLoss: 0.050041\n",
      "Epoch [332/500], Average Loss: 0.0694\n",
      "Epoch [333/500], Step [0/3451 (0%)]\tLoss: 0.173024\n",
      "Epoch [333/500], Step [1024/3451 (30%)]\tLoss: 0.096824\n",
      "Epoch [333/500], Step [2048/3451 (59%)]\tLoss: 0.039505\n",
      "Epoch [333/500], Step [3072/3451 (89%)]\tLoss: 0.156580\n",
      "Epoch [333/500], Average Loss: 0.0710\n",
      "Epoch [334/500], Step [0/3451 (0%)]\tLoss: 0.112554\n",
      "Epoch [334/500], Step [1024/3451 (30%)]\tLoss: 0.062723\n",
      "Epoch [334/500], Step [2048/3451 (59%)]\tLoss: 0.031770\n",
      "Epoch [334/500], Step [3072/3451 (89%)]\tLoss: 0.017773\n",
      "Epoch [334/500], Average Loss: 0.0690\n",
      "Epoch [335/500], Step [0/3451 (0%)]\tLoss: 0.112713\n",
      "Epoch [335/500], Step [1024/3451 (30%)]\tLoss: 0.055020\n",
      "Epoch [335/500], Step [2048/3451 (59%)]\tLoss: 0.035701\n",
      "Epoch [335/500], Step [3072/3451 (89%)]\tLoss: 0.065234\n",
      "Epoch [335/500], Average Loss: 0.0691\n",
      "Epoch [336/500], Step [0/3451 (0%)]\tLoss: 0.075145\n",
      "Epoch [336/500], Step [1024/3451 (30%)]\tLoss: 0.087370\n",
      "Epoch [336/500], Step [2048/3451 (59%)]\tLoss: 0.030049\n",
      "Epoch [336/500], Step [3072/3451 (89%)]\tLoss: 0.124845\n",
      "Epoch [336/500], Average Loss: 0.0685\n",
      "\n",
      "Test set: Avg. loss: 4.8779 Accuracy: 331/863 (38.35%)\n",
      "\n",
      "Epoch [337/500], Step [0/3451 (0%)]\tLoss: 0.031990\n",
      "Epoch [337/500], Step [1024/3451 (30%)]\tLoss: 0.099673\n",
      "Epoch [337/500], Step [2048/3451 (59%)]\tLoss: 0.068616\n",
      "Epoch [337/500], Step [3072/3451 (89%)]\tLoss: 0.092453\n",
      "Epoch [337/500], Average Loss: 0.0706\n",
      "Epoch [338/500], Step [0/3451 (0%)]\tLoss: 0.053096\n",
      "Epoch [338/500], Step [1024/3451 (30%)]\tLoss: 0.041626\n",
      "Epoch [338/500], Step [2048/3451 (59%)]\tLoss: 0.076071\n",
      "Epoch [338/500], Step [3072/3451 (89%)]\tLoss: 0.080314\n",
      "Epoch [338/500], Average Loss: 0.0696\n",
      "Epoch [339/500], Step [0/3451 (0%)]\tLoss: 0.058000\n",
      "Epoch [339/500], Step [1024/3451 (30%)]\tLoss: 0.085508\n",
      "Epoch [339/500], Step [2048/3451 (59%)]\tLoss: 0.107219\n",
      "Epoch [339/500], Step [3072/3451 (89%)]\tLoss: 0.080727\n",
      "Epoch [339/500], Average Loss: 0.0694\n",
      "Epoch [340/500], Step [0/3451 (0%)]\tLoss: 0.057341\n",
      "Epoch [340/500], Step [1024/3451 (30%)]\tLoss: 0.050898\n",
      "Epoch [340/500], Step [2048/3451 (59%)]\tLoss: 0.050622\n",
      "Epoch [340/500], Step [3072/3451 (89%)]\tLoss: 0.061510\n",
      "Epoch [340/500], Average Loss: 0.0695\n",
      "Epoch [341/500], Step [0/3451 (0%)]\tLoss: 0.094806\n",
      "Epoch [341/500], Step [1024/3451 (30%)]\tLoss: 0.070884\n",
      "Epoch [341/500], Step [2048/3451 (59%)]\tLoss: 0.041528\n",
      "Epoch [341/500], Step [3072/3451 (89%)]\tLoss: 0.114884\n",
      "Epoch [341/500], Average Loss: 0.0688\n",
      "\n",
      "Test set: Avg. loss: 4.8585 Accuracy: 330/863 (38.24%)\n",
      "\n",
      "Epoch [342/500], Step [0/3451 (0%)]\tLoss: 0.049188\n",
      "Epoch [342/500], Step [1024/3451 (30%)]\tLoss: 0.072237\n",
      "Epoch [342/500], Step [2048/3451 (59%)]\tLoss: 0.091165\n",
      "Epoch [342/500], Step [3072/3451 (89%)]\tLoss: 0.047372\n",
      "Epoch [342/500], Average Loss: 0.0694\n",
      "Epoch [343/500], Step [0/3451 (0%)]\tLoss: 0.033952\n",
      "Epoch [343/500], Step [1024/3451 (30%)]\tLoss: 0.080505\n",
      "Epoch [343/500], Step [2048/3451 (59%)]\tLoss: 0.102480\n",
      "Epoch [343/500], Step [3072/3451 (89%)]\tLoss: 0.203913\n",
      "Epoch [343/500], Average Loss: 0.0696\n",
      "Epoch [344/500], Step [0/3451 (0%)]\tLoss: 0.080426\n",
      "Epoch [344/500], Step [1024/3451 (30%)]\tLoss: 0.096272\n",
      "Epoch [344/500], Step [2048/3451 (59%)]\tLoss: 0.062559\n",
      "Epoch [344/500], Step [3072/3451 (89%)]\tLoss: 0.055961\n",
      "Epoch [344/500], Average Loss: 0.0696\n",
      "Epoch [345/500], Step [0/3451 (0%)]\tLoss: 0.037874\n",
      "Epoch [345/500], Step [1024/3451 (30%)]\tLoss: 0.055760\n",
      "Epoch [345/500], Step [2048/3451 (59%)]\tLoss: 0.071165\n",
      "Epoch [345/500], Step [3072/3451 (89%)]\tLoss: 0.053238\n",
      "Epoch [345/500], Average Loss: 0.0692\n",
      "Epoch [346/500], Step [0/3451 (0%)]\tLoss: 0.037506\n",
      "Epoch [346/500], Step [1024/3451 (30%)]\tLoss: 0.100638\n",
      "Epoch [346/500], Step [2048/3451 (59%)]\tLoss: 0.098559\n",
      "Epoch [346/500], Step [3072/3451 (89%)]\tLoss: 0.106460\n",
      "Epoch [346/500], Average Loss: 0.0696\n",
      "\n",
      "Test set: Avg. loss: 4.9967 Accuracy: 338/863 (39.17%)\n",
      "\n",
      "Epoch [347/500], Step [0/3451 (0%)]\tLoss: 0.137548\n",
      "Epoch [347/500], Step [1024/3451 (30%)]\tLoss: 0.080423\n",
      "Epoch [347/500], Step [2048/3451 (59%)]\tLoss: 0.128271\n",
      "Epoch [347/500], Step [3072/3451 (89%)]\tLoss: 0.076239\n",
      "Epoch [347/500], Average Loss: 0.0707\n",
      "Epoch [348/500], Step [0/3451 (0%)]\tLoss: 0.042833\n",
      "Epoch [348/500], Step [1024/3451 (30%)]\tLoss: 0.008842\n",
      "Epoch [348/500], Step [2048/3451 (59%)]\tLoss: 0.053506\n",
      "Epoch [348/500], Step [3072/3451 (89%)]\tLoss: 0.078698\n",
      "Epoch [348/500], Average Loss: 0.0696\n",
      "Epoch [349/500], Step [0/3451 (0%)]\tLoss: 0.066536\n",
      "Epoch [349/500], Step [1024/3451 (30%)]\tLoss: 0.052973\n",
      "Epoch [349/500], Step [2048/3451 (59%)]\tLoss: 0.046529\n",
      "Epoch [349/500], Step [3072/3451 (89%)]\tLoss: 0.093497\n",
      "Epoch [349/500], Average Loss: 0.0699\n",
      "Epoch [350/500], Step [0/3451 (0%)]\tLoss: 0.055354\n",
      "Epoch [350/500], Step [1024/3451 (30%)]\tLoss: 0.079007\n",
      "Epoch [350/500], Step [2048/3451 (59%)]\tLoss: 0.088063\n",
      "Epoch [350/500], Step [3072/3451 (89%)]\tLoss: 0.112459\n",
      "Epoch [350/500], Average Loss: 0.0700\n",
      "Epoch [351/500], Step [0/3451 (0%)]\tLoss: 0.009492\n",
      "Epoch [351/500], Step [1024/3451 (30%)]\tLoss: 0.041495\n",
      "Epoch [351/500], Step [2048/3451 (59%)]\tLoss: 0.029256\n",
      "Epoch [351/500], Step [3072/3451 (89%)]\tLoss: 0.160813\n",
      "Epoch [351/500], Average Loss: 0.0699\n",
      "\n",
      "Test set: Avg. loss: 4.9063 Accuracy: 331/863 (38.35%)\n",
      "\n",
      "Epoch [352/500], Step [0/3451 (0%)]\tLoss: 0.088950\n",
      "Epoch [352/500], Step [1024/3451 (30%)]\tLoss: 0.077116\n",
      "Epoch [352/500], Step [2048/3451 (59%)]\tLoss: 0.033346\n",
      "Epoch [352/500], Step [3072/3451 (89%)]\tLoss: 0.050793\n",
      "Epoch [352/500], Average Loss: 0.0695\n",
      "Epoch [353/500], Step [0/3451 (0%)]\tLoss: 0.073112\n",
      "Epoch [353/500], Step [1024/3451 (30%)]\tLoss: 0.041876\n",
      "Epoch [353/500], Step [2048/3451 (59%)]\tLoss: 0.024704\n",
      "Epoch [353/500], Step [3072/3451 (89%)]\tLoss: 0.136537\n",
      "Epoch [353/500], Average Loss: 0.0696\n",
      "Epoch [354/500], Step [0/3451 (0%)]\tLoss: 0.037168\n",
      "Epoch [354/500], Step [1024/3451 (30%)]\tLoss: 0.124874\n",
      "Epoch [354/500], Step [2048/3451 (59%)]\tLoss: 0.044717\n",
      "Epoch [354/500], Step [3072/3451 (89%)]\tLoss: 0.097495\n",
      "Epoch [354/500], Average Loss: 0.0688\n",
      "Epoch [355/500], Step [0/3451 (0%)]\tLoss: 0.140272\n",
      "Epoch [355/500], Step [1024/3451 (30%)]\tLoss: 0.040813\n",
      "Epoch [355/500], Step [2048/3451 (59%)]\tLoss: 0.119112\n",
      "Epoch [355/500], Step [3072/3451 (89%)]\tLoss: 0.077480\n",
      "Epoch [355/500], Average Loss: 0.0700\n",
      "Epoch [356/500], Step [0/3451 (0%)]\tLoss: 0.091752\n",
      "Epoch [356/500], Step [1024/3451 (30%)]\tLoss: 0.098990\n",
      "Epoch [356/500], Step [2048/3451 (59%)]\tLoss: 0.073505\n",
      "Epoch [356/500], Step [3072/3451 (89%)]\tLoss: 0.078819\n",
      "Epoch [356/500], Average Loss: 0.0696\n",
      "\n",
      "Test set: Avg. loss: 4.8265 Accuracy: 336/863 (38.93%)\n",
      "\n",
      "Epoch [357/500], Step [0/3451 (0%)]\tLoss: 0.089844\n",
      "Epoch [357/500], Step [1024/3451 (30%)]\tLoss: 0.054700\n",
      "Epoch [357/500], Step [2048/3451 (59%)]\tLoss: 0.089217\n",
      "Epoch [357/500], Step [3072/3451 (89%)]\tLoss: 0.077235\n",
      "Epoch [357/500], Average Loss: 0.0694\n",
      "Epoch [358/500], Step [0/3451 (0%)]\tLoss: 0.065434\n",
      "Epoch [358/500], Step [1024/3451 (30%)]\tLoss: 0.081799\n",
      "Epoch [358/500], Step [2048/3451 (59%)]\tLoss: 0.025759\n",
      "Epoch [358/500], Step [3072/3451 (89%)]\tLoss: 0.083948\n",
      "Epoch [358/500], Average Loss: 0.0692\n",
      "Epoch [359/500], Step [0/3451 (0%)]\tLoss: 0.087956\n",
      "Epoch [359/500], Step [1024/3451 (30%)]\tLoss: 0.070340\n",
      "Epoch [359/500], Step [2048/3451 (59%)]\tLoss: 0.021824\n",
      "Epoch [359/500], Step [3072/3451 (89%)]\tLoss: 0.049256\n",
      "Epoch [359/500], Average Loss: 0.0704\n",
      "Epoch [360/500], Step [0/3451 (0%)]\tLoss: 0.066881\n",
      "Epoch [360/500], Step [1024/3451 (30%)]\tLoss: 0.119205\n",
      "Epoch [360/500], Step [2048/3451 (59%)]\tLoss: 0.125253\n",
      "Epoch [360/500], Step [3072/3451 (89%)]\tLoss: 0.078376\n",
      "Epoch [360/500], Average Loss: 0.0694\n",
      "Epoch [361/500], Step [0/3451 (0%)]\tLoss: 0.045167\n",
      "Epoch [361/500], Step [1024/3451 (30%)]\tLoss: 0.061742\n",
      "Epoch [361/500], Step [2048/3451 (59%)]\tLoss: 0.032983\n",
      "Epoch [361/500], Step [3072/3451 (89%)]\tLoss: 0.336281\n",
      "Epoch [361/500], Average Loss: 0.0719\n",
      "\n",
      "Test set: Avg. loss: 4.8725 Accuracy: 338/863 (39.17%)\n",
      "\n",
      "Epoch [362/500], Step [0/3451 (0%)]\tLoss: 0.147252\n",
      "Epoch [362/500], Step [1024/3451 (30%)]\tLoss: 0.051522\n",
      "Epoch [362/500], Step [2048/3451 (59%)]\tLoss: 0.012834\n",
      "Epoch [362/500], Step [3072/3451 (89%)]\tLoss: 0.083427\n",
      "Epoch [362/500], Average Loss: 0.0702\n",
      "Epoch [363/500], Step [0/3451 (0%)]\tLoss: 0.068202\n",
      "Epoch [363/500], Step [1024/3451 (30%)]\tLoss: 0.023809\n",
      "Epoch [363/500], Step [2048/3451 (59%)]\tLoss: 0.073715\n",
      "Epoch [363/500], Step [3072/3451 (89%)]\tLoss: 0.078957\n",
      "Epoch [363/500], Average Loss: 0.0692\n",
      "Epoch [364/500], Step [0/3451 (0%)]\tLoss: 0.039173\n",
      "Epoch [364/500], Step [1024/3451 (30%)]\tLoss: 0.073852\n",
      "Epoch [364/500], Step [2048/3451 (59%)]\tLoss: 0.039101\n",
      "Epoch [364/500], Step [3072/3451 (89%)]\tLoss: 0.092160\n",
      "Epoch [364/500], Average Loss: 0.0694\n",
      "Epoch [365/500], Step [0/3451 (0%)]\tLoss: 0.073526\n",
      "Epoch [365/500], Step [1024/3451 (30%)]\tLoss: 0.049067\n",
      "Epoch [365/500], Step [2048/3451 (59%)]\tLoss: 0.021489\n",
      "Epoch [365/500], Step [3072/3451 (89%)]\tLoss: 0.046680\n",
      "Epoch [365/500], Average Loss: 0.0697\n",
      "Epoch [366/500], Step [0/3451 (0%)]\tLoss: 0.060058\n",
      "Epoch [366/500], Step [1024/3451 (30%)]\tLoss: 0.078890\n",
      "Epoch [366/500], Step [2048/3451 (59%)]\tLoss: 0.088460\n",
      "Epoch [366/500], Step [3072/3451 (89%)]\tLoss: 0.036392\n",
      "Epoch [366/500], Average Loss: 0.0698\n",
      "\n",
      "Test set: Avg. loss: 4.8928 Accuracy: 328/863 (38.01%)\n",
      "\n",
      "Epoch [367/500], Step [0/3451 (0%)]\tLoss: 0.064217\n",
      "Epoch [367/500], Step [1024/3451 (30%)]\tLoss: 0.053951\n",
      "Epoch [367/500], Step [2048/3451 (59%)]\tLoss: 0.097227\n",
      "Epoch [367/500], Step [3072/3451 (89%)]\tLoss: 0.109870\n",
      "Epoch [367/500], Average Loss: 0.0688\n",
      "Epoch [368/500], Step [0/3451 (0%)]\tLoss: 0.076188\n",
      "Epoch [368/500], Step [1024/3451 (30%)]\tLoss: 0.078630\n",
      "Epoch [368/500], Step [2048/3451 (59%)]\tLoss: 0.057825\n",
      "Epoch [368/500], Step [3072/3451 (89%)]\tLoss: 0.031378\n",
      "Epoch [368/500], Average Loss: 0.0690\n",
      "Epoch [369/500], Step [0/3451 (0%)]\tLoss: 0.083892\n",
      "Epoch [369/500], Step [1024/3451 (30%)]\tLoss: 0.054679\n",
      "Epoch [369/500], Step [2048/3451 (59%)]\tLoss: 0.109558\n",
      "Epoch [369/500], Step [3072/3451 (89%)]\tLoss: 0.036657\n",
      "Epoch [369/500], Average Loss: 0.0690\n",
      "Epoch [370/500], Step [0/3451 (0%)]\tLoss: 0.055564\n",
      "Epoch [370/500], Step [1024/3451 (30%)]\tLoss: 0.062256\n",
      "Epoch [370/500], Step [2048/3451 (59%)]\tLoss: 0.034640\n",
      "Epoch [370/500], Step [3072/3451 (89%)]\tLoss: 0.080155\n",
      "Epoch [370/500], Average Loss: 0.0693\n",
      "Epoch [371/500], Step [0/3451 (0%)]\tLoss: 0.052848\n",
      "Epoch [371/500], Step [1024/3451 (30%)]\tLoss: 0.088161\n",
      "Epoch [371/500], Step [2048/3451 (59%)]\tLoss: 0.025878\n",
      "Epoch [371/500], Step [3072/3451 (89%)]\tLoss: 0.107578\n",
      "Epoch [371/500], Average Loss: 0.0700\n",
      "\n",
      "Test set: Avg. loss: 4.8671 Accuracy: 333/863 (38.59%)\n",
      "\n",
      "Epoch [372/500], Step [0/3451 (0%)]\tLoss: 0.029796\n",
      "Epoch [372/500], Step [1024/3451 (30%)]\tLoss: 0.027125\n",
      "Epoch [372/500], Step [2048/3451 (59%)]\tLoss: 0.049029\n",
      "Epoch [372/500], Step [3072/3451 (89%)]\tLoss: 0.036716\n",
      "Epoch [372/500], Average Loss: 0.0706\n",
      "Epoch [373/500], Step [0/3451 (0%)]\tLoss: 0.070921\n",
      "Epoch [373/500], Step [1024/3451 (30%)]\tLoss: 0.086321\n",
      "Epoch [373/500], Step [2048/3451 (59%)]\tLoss: 0.044355\n",
      "Epoch [373/500], Step [3072/3451 (89%)]\tLoss: 0.095249\n",
      "Epoch [373/500], Average Loss: 0.0697\n",
      "Epoch [374/500], Step [0/3451 (0%)]\tLoss: 0.123650\n",
      "Epoch [374/500], Step [1024/3451 (30%)]\tLoss: 0.019876\n",
      "Epoch [374/500], Step [2048/3451 (59%)]\tLoss: 0.041919\n",
      "Epoch [374/500], Step [3072/3451 (89%)]\tLoss: 0.058732\n",
      "Epoch [374/500], Average Loss: 0.0703\n",
      "Epoch [375/500], Step [0/3451 (0%)]\tLoss: 0.124207\n",
      "Epoch [375/500], Step [1024/3451 (30%)]\tLoss: 0.060025\n",
      "Epoch [375/500], Step [2048/3451 (59%)]\tLoss: 0.059964\n",
      "Epoch [375/500], Step [3072/3451 (89%)]\tLoss: 0.142682\n",
      "Epoch [375/500], Average Loss: 0.0698\n",
      "Epoch [376/500], Step [0/3451 (0%)]\tLoss: 0.041938\n",
      "Epoch [376/500], Step [1024/3451 (30%)]\tLoss: 0.031077\n",
      "Epoch [376/500], Step [2048/3451 (59%)]\tLoss: 0.122163\n",
      "Epoch [376/500], Step [3072/3451 (89%)]\tLoss: 0.037140\n",
      "Epoch [376/500], Average Loss: 0.0694\n",
      "\n",
      "Test set: Avg. loss: 4.9436 Accuracy: 334/863 (38.70%)\n",
      "\n",
      "Epoch [377/500], Step [0/3451 (0%)]\tLoss: 0.020604\n",
      "Epoch [377/500], Step [1024/3451 (30%)]\tLoss: 0.053410\n",
      "Epoch [377/500], Step [2048/3451 (59%)]\tLoss: 0.056046\n",
      "Epoch [377/500], Step [3072/3451 (89%)]\tLoss: 0.101620\n",
      "Epoch [377/500], Average Loss: 0.0697\n",
      "Epoch [378/500], Step [0/3451 (0%)]\tLoss: 0.100933\n",
      "Epoch [378/500], Step [1024/3451 (30%)]\tLoss: 0.083505\n",
      "Epoch [378/500], Step [2048/3451 (59%)]\tLoss: 0.053913\n",
      "Epoch [378/500], Step [3072/3451 (89%)]\tLoss: 0.074240\n",
      "Epoch [378/500], Average Loss: 0.0688\n",
      "Epoch [379/500], Step [0/3451 (0%)]\tLoss: 0.072280\n",
      "Epoch [379/500], Step [1024/3451 (30%)]\tLoss: 0.068421\n",
      "Epoch [379/500], Step [2048/3451 (59%)]\tLoss: 0.035013\n",
      "Epoch [379/500], Step [3072/3451 (89%)]\tLoss: 0.106582\n",
      "Epoch [379/500], Average Loss: 0.0691\n",
      "Epoch [380/500], Step [0/3451 (0%)]\tLoss: 0.017174\n",
      "Epoch [380/500], Step [1024/3451 (30%)]\tLoss: 0.106144\n",
      "Epoch [380/500], Step [2048/3451 (59%)]\tLoss: 0.102498\n",
      "Epoch [380/500], Step [3072/3451 (89%)]\tLoss: 0.094910\n",
      "Epoch [380/500], Average Loss: 0.0700\n",
      "Epoch [381/500], Step [0/3451 (0%)]\tLoss: 0.115541\n",
      "Epoch [381/500], Step [1024/3451 (30%)]\tLoss: 0.084512\n",
      "Epoch [381/500], Step [2048/3451 (59%)]\tLoss: 0.074022\n",
      "Epoch [381/500], Step [3072/3451 (89%)]\tLoss: 0.057622\n",
      "Epoch [381/500], Average Loss: 0.0703\n",
      "\n",
      "Test set: Avg. loss: 4.8672 Accuracy: 335/863 (38.82%)\n",
      "\n",
      "Epoch [382/500], Step [0/3451 (0%)]\tLoss: 0.055295\n",
      "Epoch [382/500], Step [1024/3451 (30%)]\tLoss: 0.069034\n",
      "Epoch [382/500], Step [2048/3451 (59%)]\tLoss: 0.040803\n",
      "Epoch [382/500], Step [3072/3451 (89%)]\tLoss: 0.073787\n",
      "Epoch [382/500], Average Loss: 0.0701\n",
      "Epoch [383/500], Step [0/3451 (0%)]\tLoss: 0.048460\n",
      "Epoch [383/500], Step [1024/3451 (30%)]\tLoss: 0.137338\n",
      "Epoch [383/500], Step [2048/3451 (59%)]\tLoss: 0.078265\n",
      "Epoch [383/500], Step [3072/3451 (89%)]\tLoss: 0.037498\n",
      "Epoch [383/500], Average Loss: 0.0692\n",
      "Epoch [384/500], Step [0/3451 (0%)]\tLoss: 0.049331\n",
      "Epoch [384/500], Step [1024/3451 (30%)]\tLoss: 0.081996\n",
      "Epoch [384/500], Step [2048/3451 (59%)]\tLoss: 0.041154\n",
      "Epoch [384/500], Step [3072/3451 (89%)]\tLoss: 0.067932\n",
      "Epoch [384/500], Average Loss: 0.0708\n",
      "Epoch [385/500], Step [0/3451 (0%)]\tLoss: 0.066530\n",
      "Epoch [385/500], Step [1024/3451 (30%)]\tLoss: 0.075470\n",
      "Epoch [385/500], Step [2048/3451 (59%)]\tLoss: 0.100080\n",
      "Epoch [385/500], Step [3072/3451 (89%)]\tLoss: 0.029064\n",
      "Epoch [385/500], Average Loss: 0.0725\n",
      "Epoch [386/500], Step [0/3451 (0%)]\tLoss: 0.023697\n",
      "Epoch [386/500], Step [1024/3451 (30%)]\tLoss: 0.059247\n",
      "Epoch [386/500], Step [2048/3451 (59%)]\tLoss: 0.039372\n",
      "Epoch [386/500], Step [3072/3451 (89%)]\tLoss: 0.016683\n",
      "Epoch [386/500], Average Loss: 0.0689\n",
      "\n",
      "Test set: Avg. loss: 4.9336 Accuracy: 338/863 (39.17%)\n",
      "\n",
      "Epoch [387/500], Step [0/3451 (0%)]\tLoss: 0.113315\n",
      "Epoch [387/500], Step [1024/3451 (30%)]\tLoss: 0.096706\n",
      "Epoch [387/500], Step [2048/3451 (59%)]\tLoss: 0.032174\n",
      "Epoch [387/500], Step [3072/3451 (89%)]\tLoss: 0.057689\n",
      "Epoch [387/500], Average Loss: 0.0691\n",
      "Epoch [388/500], Step [0/3451 (0%)]\tLoss: 0.063763\n",
      "Epoch [388/500], Step [1024/3451 (30%)]\tLoss: 0.122469\n",
      "Epoch [388/500], Step [2048/3451 (59%)]\tLoss: 0.050655\n",
      "Epoch [388/500], Step [3072/3451 (89%)]\tLoss: 0.083504\n",
      "Epoch [388/500], Average Loss: 0.0709\n",
      "Epoch [389/500], Step [0/3451 (0%)]\tLoss: 0.140348\n",
      "Epoch [389/500], Step [1024/3451 (30%)]\tLoss: 0.089625\n",
      "Epoch [389/500], Step [2048/3451 (59%)]\tLoss: 0.082086\n",
      "Epoch [389/500], Step [3072/3451 (89%)]\tLoss: 0.026107\n",
      "Epoch [389/500], Average Loss: 0.0693\n",
      "Epoch [390/500], Step [0/3451 (0%)]\tLoss: 0.068381\n",
      "Epoch [390/500], Step [1024/3451 (30%)]\tLoss: 0.083134\n",
      "Epoch [390/500], Step [2048/3451 (59%)]\tLoss: 0.038579\n",
      "Epoch [390/500], Step [3072/3451 (89%)]\tLoss: 0.090788\n",
      "Epoch [390/500], Average Loss: 0.0691\n",
      "Epoch [391/500], Step [0/3451 (0%)]\tLoss: 0.060184\n",
      "Epoch [391/500], Step [1024/3451 (30%)]\tLoss: 0.099337\n",
      "Epoch [391/500], Step [2048/3451 (59%)]\tLoss: 0.093030\n",
      "Epoch [391/500], Step [3072/3451 (89%)]\tLoss: 0.049282\n",
      "Epoch [391/500], Average Loss: 0.0701\n",
      "\n",
      "Test set: Avg. loss: 4.8678 Accuracy: 340/863 (39.40%)\n",
      "\n",
      "Epoch [392/500], Step [0/3451 (0%)]\tLoss: 0.027088\n",
      "Epoch [392/500], Step [1024/3451 (30%)]\tLoss: 0.128994\n",
      "Epoch [392/500], Step [2048/3451 (59%)]\tLoss: 0.076508\n",
      "Epoch [392/500], Step [3072/3451 (89%)]\tLoss: 0.053666\n",
      "Epoch [392/500], Average Loss: 0.0698\n",
      "Epoch [393/500], Step [0/3451 (0%)]\tLoss: 0.014422\n",
      "Epoch [393/500], Step [1024/3451 (30%)]\tLoss: 0.049176\n",
      "Epoch [393/500], Step [2048/3451 (59%)]\tLoss: 0.037926\n",
      "Epoch [393/500], Step [3072/3451 (89%)]\tLoss: 0.012030\n",
      "Epoch [393/500], Average Loss: 0.0699\n",
      "Epoch [394/500], Step [0/3451 (0%)]\tLoss: 0.029818\n",
      "Epoch [394/500], Step [1024/3451 (30%)]\tLoss: 0.058932\n",
      "Epoch [394/500], Step [2048/3451 (59%)]\tLoss: 0.043908\n",
      "Epoch [394/500], Step [3072/3451 (89%)]\tLoss: 0.048652\n",
      "Epoch [394/500], Average Loss: 0.0709\n",
      "Epoch [395/500], Step [0/3451 (0%)]\tLoss: 0.073558\n",
      "Epoch [395/500], Step [1024/3451 (30%)]\tLoss: 0.007929\n",
      "Epoch [395/500], Step [2048/3451 (59%)]\tLoss: 0.102881\n",
      "Epoch [395/500], Step [3072/3451 (89%)]\tLoss: 0.159292\n",
      "Epoch [395/500], Average Loss: 0.0713\n",
      "Epoch [396/500], Step [0/3451 (0%)]\tLoss: 0.132753\n",
      "Epoch [396/500], Step [1024/3451 (30%)]\tLoss: 0.069661\n",
      "Epoch [396/500], Step [2048/3451 (59%)]\tLoss: 0.024988\n",
      "Epoch [396/500], Step [3072/3451 (89%)]\tLoss: 0.059482\n",
      "Epoch [396/500], Average Loss: 0.0697\n",
      "\n",
      "Test set: Avg. loss: 4.8950 Accuracy: 340/863 (39.40%)\n",
      "\n",
      "Epoch [397/500], Step [0/3451 (0%)]\tLoss: 0.032479\n",
      "Epoch [397/500], Step [1024/3451 (30%)]\tLoss: 0.079844\n",
      "Epoch [397/500], Step [2048/3451 (59%)]\tLoss: 0.064762\n",
      "Epoch [397/500], Step [3072/3451 (89%)]\tLoss: 0.088858\n",
      "Epoch [397/500], Average Loss: 0.0695\n",
      "Epoch [398/500], Step [0/3451 (0%)]\tLoss: 0.031652\n",
      "Epoch [398/500], Step [1024/3451 (30%)]\tLoss: 0.081234\n",
      "Epoch [398/500], Step [2048/3451 (59%)]\tLoss: 0.065426\n",
      "Epoch [398/500], Step [3072/3451 (89%)]\tLoss: 0.045292\n",
      "Epoch [398/500], Average Loss: 0.0689\n",
      "Epoch [399/500], Step [0/3451 (0%)]\tLoss: 0.033145\n",
      "Epoch [399/500], Step [1024/3451 (30%)]\tLoss: 0.042764\n",
      "Epoch [399/500], Step [2048/3451 (59%)]\tLoss: 0.123899\n",
      "Epoch [399/500], Step [3072/3451 (89%)]\tLoss: 0.071354\n",
      "Epoch [399/500], Average Loss: 0.0701\n",
      "Epoch [400/500], Step [0/3451 (0%)]\tLoss: 0.045936\n",
      "Epoch [400/500], Step [1024/3451 (30%)]\tLoss: 0.085562\n",
      "Epoch [400/500], Step [2048/3451 (59%)]\tLoss: 0.035892\n",
      "Epoch [400/500], Step [3072/3451 (89%)]\tLoss: 0.088933\n",
      "Epoch [400/500], Average Loss: 0.0691\n",
      "Epoch [401/500], Step [0/3451 (0%)]\tLoss: 0.111158\n",
      "Epoch [401/500], Step [1024/3451 (30%)]\tLoss: 0.026276\n",
      "Epoch [401/500], Step [2048/3451 (59%)]\tLoss: 0.080071\n",
      "Epoch [401/500], Step [3072/3451 (89%)]\tLoss: 0.124610\n",
      "Epoch [401/500], Average Loss: 0.0696\n",
      "\n",
      "Test set: Avg. loss: 4.8542 Accuracy: 338/863 (39.17%)\n",
      "\n",
      "Epoch [402/500], Step [0/3451 (0%)]\tLoss: 0.060909\n",
      "Epoch [402/500], Step [1024/3451 (30%)]\tLoss: 0.062490\n",
      "Epoch [402/500], Step [2048/3451 (59%)]\tLoss: 0.056860\n",
      "Epoch [402/500], Step [3072/3451 (89%)]\tLoss: 0.105639\n",
      "Epoch [402/500], Average Loss: 0.0698\n",
      "Epoch [403/500], Step [0/3451 (0%)]\tLoss: 0.093236\n",
      "Epoch [403/500], Step [1024/3451 (30%)]\tLoss: 0.081410\n",
      "Epoch [403/500], Step [2048/3451 (59%)]\tLoss: 0.102715\n",
      "Epoch [403/500], Step [3072/3451 (89%)]\tLoss: 0.062724\n",
      "Epoch [403/500], Average Loss: 0.0702\n",
      "Epoch [404/500], Step [0/3451 (0%)]\tLoss: 0.070398\n",
      "Epoch [404/500], Step [1024/3451 (30%)]\tLoss: 0.035020\n",
      "Epoch [404/500], Step [2048/3451 (59%)]\tLoss: 0.090677\n",
      "Epoch [404/500], Step [3072/3451 (89%)]\tLoss: 0.070083\n",
      "Epoch [404/500], Average Loss: 0.0699\n",
      "Epoch [405/500], Step [0/3451 (0%)]\tLoss: 0.060159\n",
      "Epoch [405/500], Step [1024/3451 (30%)]\tLoss: 0.061718\n",
      "Epoch [405/500], Step [2048/3451 (59%)]\tLoss: 0.051885\n",
      "Epoch [405/500], Step [3072/3451 (89%)]\tLoss: 0.040662\n",
      "Epoch [405/500], Average Loss: 0.0698\n",
      "Epoch [406/500], Step [0/3451 (0%)]\tLoss: 0.070716\n",
      "Epoch [406/500], Step [1024/3451 (30%)]\tLoss: 0.069711\n",
      "Epoch [406/500], Step [2048/3451 (59%)]\tLoss: 0.043506\n",
      "Epoch [406/500], Step [3072/3451 (89%)]\tLoss: 0.115517\n",
      "Epoch [406/500], Average Loss: 0.0698\n",
      "\n",
      "Test set: Avg. loss: 4.9080 Accuracy: 333/863 (38.59%)\n",
      "\n",
      "Epoch [407/500], Step [0/3451 (0%)]\tLoss: 0.115486\n",
      "Epoch [407/500], Step [1024/3451 (30%)]\tLoss: 0.031928\n",
      "Epoch [407/500], Step [2048/3451 (59%)]\tLoss: 0.101945\n",
      "Epoch [407/500], Step [3072/3451 (89%)]\tLoss: 0.121998\n",
      "Epoch [407/500], Average Loss: 0.0705\n",
      "Epoch [408/500], Step [0/3451 (0%)]\tLoss: 0.066421\n",
      "Epoch [408/500], Step [1024/3451 (30%)]\tLoss: 0.061860\n",
      "Epoch [408/500], Step [2048/3451 (59%)]\tLoss: 0.035332\n",
      "Epoch [408/500], Step [3072/3451 (89%)]\tLoss: 0.060169\n",
      "Epoch [408/500], Average Loss: 0.0702\n",
      "Epoch [409/500], Step [0/3451 (0%)]\tLoss: 0.024001\n",
      "Epoch [409/500], Step [1024/3451 (30%)]\tLoss: 0.003537\n",
      "Epoch [409/500], Step [2048/3451 (59%)]\tLoss: 0.098615\n",
      "Epoch [409/500], Step [3072/3451 (89%)]\tLoss: 0.058801\n",
      "Epoch [409/500], Average Loss: 0.0696\n",
      "Epoch [410/500], Step [0/3451 (0%)]\tLoss: 0.041179\n",
      "Epoch [410/500], Step [1024/3451 (30%)]\tLoss: 0.017014\n",
      "Epoch [410/500], Step [2048/3451 (59%)]\tLoss: 0.067427\n",
      "Epoch [410/500], Step [3072/3451 (89%)]\tLoss: 0.073967\n",
      "Epoch [410/500], Average Loss: 0.0693\n",
      "Epoch [411/500], Step [0/3451 (0%)]\tLoss: 0.051633\n",
      "Epoch [411/500], Step [1024/3451 (30%)]\tLoss: 0.052188\n",
      "Epoch [411/500], Step [2048/3451 (59%)]\tLoss: 0.017373\n",
      "Epoch [411/500], Step [3072/3451 (89%)]\tLoss: 0.048327\n",
      "Epoch [411/500], Average Loss: 0.0694\n",
      "\n",
      "Test set: Avg. loss: 4.9927 Accuracy: 333/863 (38.59%)\n",
      "\n",
      "Epoch [412/500], Step [0/3451 (0%)]\tLoss: 0.083533\n",
      "Epoch [412/500], Step [1024/3451 (30%)]\tLoss: 0.012490\n",
      "Epoch [412/500], Step [2048/3451 (59%)]\tLoss: 0.050515\n",
      "Epoch [412/500], Step [3072/3451 (89%)]\tLoss: 0.053517\n",
      "Epoch [412/500], Average Loss: 0.0717\n",
      "Epoch [413/500], Step [0/3451 (0%)]\tLoss: 0.090625\n",
      "Epoch [413/500], Step [1024/3451 (30%)]\tLoss: 0.021335\n",
      "Epoch [413/500], Step [2048/3451 (59%)]\tLoss: 0.046818\n",
      "Epoch [413/500], Step [3072/3451 (89%)]\tLoss: 0.075755\n",
      "Epoch [413/500], Average Loss: 0.0701\n",
      "Epoch [414/500], Step [0/3451 (0%)]\tLoss: 0.029902\n",
      "Epoch [414/500], Step [1024/3451 (30%)]\tLoss: 0.019630\n",
      "Epoch [414/500], Step [2048/3451 (59%)]\tLoss: 0.074555\n",
      "Epoch [414/500], Step [3072/3451 (89%)]\tLoss: 0.032470\n",
      "Epoch [414/500], Average Loss: 0.0700\n",
      "Epoch [415/500], Step [0/3451 (0%)]\tLoss: 0.085180\n",
      "Epoch [415/500], Step [1024/3451 (30%)]\tLoss: 0.028165\n",
      "Epoch [415/500], Step [2048/3451 (59%)]\tLoss: 0.034015\n",
      "Epoch [415/500], Step [3072/3451 (89%)]\tLoss: 0.094417\n",
      "Epoch [415/500], Average Loss: 0.0692\n",
      "Epoch [416/500], Step [0/3451 (0%)]\tLoss: 0.074309\n",
      "Epoch [416/500], Step [1024/3451 (30%)]\tLoss: 0.087048\n",
      "Epoch [416/500], Step [2048/3451 (59%)]\tLoss: 0.100645\n",
      "Epoch [416/500], Step [3072/3451 (89%)]\tLoss: 0.083755\n",
      "Epoch [416/500], Average Loss: 0.0697\n",
      "\n",
      "Test set: Avg. loss: 4.9624 Accuracy: 344/863 (39.86%)\n",
      "\n",
      "Epoch [417/500], Step [0/3451 (0%)]\tLoss: 0.061350\n",
      "Epoch [417/500], Step [1024/3451 (30%)]\tLoss: 0.068861\n",
      "Epoch [417/500], Step [2048/3451 (59%)]\tLoss: 0.103042\n",
      "Epoch [417/500], Step [3072/3451 (89%)]\tLoss: 0.095917\n",
      "Epoch [417/500], Average Loss: 0.0689\n",
      "Epoch [418/500], Step [0/3451 (0%)]\tLoss: 0.060252\n",
      "Epoch [418/500], Step [1024/3451 (30%)]\tLoss: 0.109043\n",
      "Epoch [418/500], Step [2048/3451 (59%)]\tLoss: 0.065422\n",
      "Epoch [418/500], Step [3072/3451 (89%)]\tLoss: 0.012295\n",
      "Epoch [418/500], Average Loss: 0.0688\n",
      "Epoch [419/500], Step [0/3451 (0%)]\tLoss: 0.037585\n",
      "Epoch [419/500], Step [1024/3451 (30%)]\tLoss: 0.028299\n",
      "Epoch [419/500], Step [2048/3451 (59%)]\tLoss: 0.045580\n",
      "Epoch [419/500], Step [3072/3451 (89%)]\tLoss: 0.093948\n",
      "Epoch [419/500], Average Loss: 0.0693\n",
      "Epoch [420/500], Step [0/3451 (0%)]\tLoss: 0.032001\n",
      "Epoch [420/500], Step [1024/3451 (30%)]\tLoss: 0.069935\n",
      "Epoch [420/500], Step [2048/3451 (59%)]\tLoss: 0.126201\n",
      "Epoch [420/500], Step [3072/3451 (89%)]\tLoss: 0.075463\n",
      "Epoch [420/500], Average Loss: 0.0697\n",
      "Epoch [421/500], Step [0/3451 (0%)]\tLoss: 0.038654\n",
      "Epoch [421/500], Step [1024/3451 (30%)]\tLoss: 0.076426\n",
      "Epoch [421/500], Step [2048/3451 (59%)]\tLoss: 0.080430\n",
      "Epoch [421/500], Step [3072/3451 (89%)]\tLoss: 0.117823\n",
      "Epoch [421/500], Average Loss: 0.0696\n",
      "\n",
      "Test set: Avg. loss: 4.8786 Accuracy: 338/863 (39.17%)\n",
      "\n",
      "Epoch [422/500], Step [0/3451 (0%)]\tLoss: 0.015735\n",
      "Epoch [422/500], Step [1024/3451 (30%)]\tLoss: 0.073900\n",
      "Epoch [422/500], Step [2048/3451 (59%)]\tLoss: 0.044912\n",
      "Epoch [422/500], Step [3072/3451 (89%)]\tLoss: 0.044541\n",
      "Epoch [422/500], Average Loss: 0.0695\n",
      "Epoch [423/500], Step [0/3451 (0%)]\tLoss: 0.041863\n",
      "Epoch [423/500], Step [1024/3451 (30%)]\tLoss: 0.039341\n",
      "Epoch [423/500], Step [2048/3451 (59%)]\tLoss: 0.057625\n",
      "Epoch [423/500], Step [3072/3451 (89%)]\tLoss: 0.079476\n",
      "Epoch [423/500], Average Loss: 0.0699\n",
      "Epoch [424/500], Step [0/3451 (0%)]\tLoss: 0.089845\n",
      "Epoch [424/500], Step [1024/3451 (30%)]\tLoss: 0.046770\n",
      "Epoch [424/500], Step [2048/3451 (59%)]\tLoss: 0.099366\n",
      "Epoch [424/500], Step [3072/3451 (89%)]\tLoss: 0.101826\n",
      "Epoch [424/500], Average Loss: 0.0695\n",
      "Epoch [425/500], Step [0/3451 (0%)]\tLoss: 0.118201\n",
      "Epoch [425/500], Step [1024/3451 (30%)]\tLoss: 0.045388\n",
      "Epoch [425/500], Step [2048/3451 (59%)]\tLoss: 0.033691\n",
      "Epoch [425/500], Step [3072/3451 (89%)]\tLoss: 0.042147\n",
      "Epoch [425/500], Average Loss: 0.0690\n",
      "Epoch [426/500], Step [0/3451 (0%)]\tLoss: 0.031276\n",
      "Epoch [426/500], Step [1024/3451 (30%)]\tLoss: 0.006866\n",
      "Epoch [426/500], Step [2048/3451 (59%)]\tLoss: 0.122824\n",
      "Epoch [426/500], Step [3072/3451 (89%)]\tLoss: 0.150487\n",
      "Epoch [426/500], Average Loss: 0.0695\n",
      "\n",
      "Test set: Avg. loss: 5.0189 Accuracy: 337/863 (39.05%)\n",
      "\n",
      "Epoch [427/500], Step [0/3451 (0%)]\tLoss: 0.066124\n",
      "Epoch [427/500], Step [1024/3451 (30%)]\tLoss: 0.061838\n",
      "Epoch [427/500], Step [2048/3451 (59%)]\tLoss: 0.144685\n",
      "Epoch [427/500], Step [3072/3451 (89%)]\tLoss: 0.060294\n",
      "Epoch [427/500], Average Loss: 0.0692\n",
      "Epoch [428/500], Step [0/3451 (0%)]\tLoss: 0.043637\n",
      "Epoch [428/500], Step [1024/3451 (30%)]\tLoss: 0.040202\n",
      "Epoch [428/500], Step [2048/3451 (59%)]\tLoss: 0.050014\n",
      "Epoch [428/500], Step [3072/3451 (89%)]\tLoss: 0.071972\n",
      "Epoch [428/500], Average Loss: 0.0705\n",
      "Epoch [429/500], Step [0/3451 (0%)]\tLoss: 0.084969\n",
      "Epoch [429/500], Step [1024/3451 (30%)]\tLoss: 0.169314\n",
      "Epoch [429/500], Step [2048/3451 (59%)]\tLoss: 0.094835\n",
      "Epoch [429/500], Step [3072/3451 (89%)]\tLoss: 0.066548\n",
      "Epoch [429/500], Average Loss: 0.0708\n",
      "Epoch [430/500], Step [0/3451 (0%)]\tLoss: 0.031042\n",
      "Epoch [430/500], Step [1024/3451 (30%)]\tLoss: 0.083181\n",
      "Epoch [430/500], Step [2048/3451 (59%)]\tLoss: 0.142416\n",
      "Epoch [430/500], Step [3072/3451 (89%)]\tLoss: 0.104648\n",
      "Epoch [430/500], Average Loss: 0.0705\n",
      "Epoch [431/500], Step [0/3451 (0%)]\tLoss: 0.085683\n",
      "Epoch [431/500], Step [1024/3451 (30%)]\tLoss: 0.019048\n",
      "Epoch [431/500], Step [2048/3451 (59%)]\tLoss: 0.084270\n",
      "Epoch [431/500], Step [3072/3451 (89%)]\tLoss: 0.132477\n",
      "Epoch [431/500], Average Loss: 0.0700\n",
      "\n",
      "Test set: Avg. loss: 4.9536 Accuracy: 330/863 (38.24%)\n",
      "\n",
      "Epoch [432/500], Step [0/3451 (0%)]\tLoss: 0.044456\n",
      "Epoch [432/500], Step [1024/3451 (30%)]\tLoss: 0.139660\n",
      "Epoch [432/500], Step [2048/3451 (59%)]\tLoss: 0.045840\n",
      "Epoch [432/500], Step [3072/3451 (89%)]\tLoss: 0.039557\n",
      "Epoch [432/500], Average Loss: 0.0708\n",
      "Epoch [433/500], Step [0/3451 (0%)]\tLoss: 0.060814\n",
      "Epoch [433/500], Step [1024/3451 (30%)]\tLoss: 0.085409\n",
      "Epoch [433/500], Step [2048/3451 (59%)]\tLoss: 0.033039\n",
      "Epoch [433/500], Step [3072/3451 (89%)]\tLoss: 0.109377\n",
      "Epoch [433/500], Average Loss: 0.0694\n",
      "Epoch [434/500], Step [0/3451 (0%)]\tLoss: 0.058471\n",
      "Epoch [434/500], Step [1024/3451 (30%)]\tLoss: 0.069415\n",
      "Epoch [434/500], Step [2048/3451 (59%)]\tLoss: 0.026839\n",
      "Epoch [434/500], Step [3072/3451 (89%)]\tLoss: 0.142726\n",
      "Epoch [434/500], Average Loss: 0.0707\n",
      "Epoch [435/500], Step [0/3451 (0%)]\tLoss: 0.110947\n",
      "Epoch [435/500], Step [1024/3451 (30%)]\tLoss: 0.081985\n",
      "Epoch [435/500], Step [2048/3451 (59%)]\tLoss: 0.035918\n",
      "Epoch [435/500], Step [3072/3451 (89%)]\tLoss: 0.087304\n",
      "Epoch [435/500], Average Loss: 0.0704\n",
      "Epoch [436/500], Step [0/3451 (0%)]\tLoss: 0.062670\n",
      "Epoch [436/500], Step [1024/3451 (30%)]\tLoss: 0.030499\n",
      "Epoch [436/500], Step [2048/3451 (59%)]\tLoss: 0.033651\n",
      "Epoch [436/500], Step [3072/3451 (89%)]\tLoss: 0.031289\n",
      "Epoch [436/500], Average Loss: 0.0698\n",
      "\n",
      "Test set: Avg. loss: 4.9438 Accuracy: 339/863 (39.28%)\n",
      "\n",
      "Epoch [437/500], Step [0/3451 (0%)]\tLoss: 0.026111\n",
      "Epoch [437/500], Step [1024/3451 (30%)]\tLoss: 0.108428\n",
      "Epoch [437/500], Step [2048/3451 (59%)]\tLoss: 0.088372\n",
      "Epoch [437/500], Step [3072/3451 (89%)]\tLoss: 0.082461\n",
      "Epoch [437/500], Average Loss: 0.0695\n",
      "Epoch [438/500], Step [0/3451 (0%)]\tLoss: 0.027012\n",
      "Epoch [438/500], Step [1024/3451 (30%)]\tLoss: 0.070943\n",
      "Epoch [438/500], Step [2048/3451 (59%)]\tLoss: 0.015340\n",
      "Epoch [438/500], Step [3072/3451 (89%)]\tLoss: 0.097966\n",
      "Epoch [438/500], Average Loss: 0.0702\n",
      "Epoch [439/500], Step [0/3451 (0%)]\tLoss: 0.050762\n",
      "Epoch [439/500], Step [1024/3451 (30%)]\tLoss: 0.035232\n",
      "Epoch [439/500], Step [2048/3451 (59%)]\tLoss: 0.086675\n",
      "Epoch [439/500], Step [3072/3451 (89%)]\tLoss: 0.021557\n",
      "Epoch [439/500], Average Loss: 0.0689\n",
      "Epoch [440/500], Step [0/3451 (0%)]\tLoss: 0.054287\n",
      "Epoch [440/500], Step [1024/3451 (30%)]\tLoss: 0.097348\n",
      "Epoch [440/500], Step [2048/3451 (59%)]\tLoss: 0.069190\n",
      "Epoch [440/500], Step [3072/3451 (89%)]\tLoss: 0.013622\n",
      "Epoch [440/500], Average Loss: 0.0704\n",
      "Epoch [441/500], Step [0/3451 (0%)]\tLoss: 0.084261\n",
      "Epoch [441/500], Step [1024/3451 (30%)]\tLoss: 0.025179\n",
      "Epoch [441/500], Step [2048/3451 (59%)]\tLoss: 0.080090\n",
      "Epoch [441/500], Step [3072/3451 (89%)]\tLoss: 0.096332\n",
      "Epoch [441/500], Average Loss: 0.0694\n",
      "\n",
      "Test set: Avg. loss: 4.8450 Accuracy: 334/863 (38.70%)\n",
      "\n",
      "Epoch [442/500], Step [0/3451 (0%)]\tLoss: 0.072911\n",
      "Epoch [442/500], Step [1024/3451 (30%)]\tLoss: 0.073617\n",
      "Epoch [442/500], Step [2048/3451 (59%)]\tLoss: 0.112079\n",
      "Epoch [442/500], Step [3072/3451 (89%)]\tLoss: 0.132639\n",
      "Epoch [442/500], Average Loss: 0.0695\n",
      "Epoch [443/500], Step [0/3451 (0%)]\tLoss: 0.095385\n",
      "Epoch [443/500], Step [1024/3451 (30%)]\tLoss: 0.045493\n",
      "Epoch [443/500], Step [2048/3451 (59%)]\tLoss: 0.064431\n",
      "Epoch [443/500], Step [3072/3451 (89%)]\tLoss: 0.093890\n",
      "Epoch [443/500], Average Loss: 0.0696\n",
      "Epoch [444/500], Step [0/3451 (0%)]\tLoss: 0.077591\n",
      "Epoch [444/500], Step [1024/3451 (30%)]\tLoss: 0.069429\n",
      "Epoch [444/500], Step [2048/3451 (59%)]\tLoss: 0.053155\n",
      "Epoch [444/500], Step [3072/3451 (89%)]\tLoss: 0.038154\n",
      "Epoch [444/500], Average Loss: 0.0708\n",
      "Epoch [445/500], Step [0/3451 (0%)]\tLoss: 0.075990\n",
      "Epoch [445/500], Step [1024/3451 (30%)]\tLoss: 0.035908\n",
      "Epoch [445/500], Step [2048/3451 (59%)]\tLoss: 0.034539\n",
      "Epoch [445/500], Step [3072/3451 (89%)]\tLoss: 0.030520\n",
      "Epoch [445/500], Average Loss: 0.0693\n",
      "Epoch [446/500], Step [0/3451 (0%)]\tLoss: 0.044417\n",
      "Epoch [446/500], Step [1024/3451 (30%)]\tLoss: 0.086594\n",
      "Epoch [446/500], Step [2048/3451 (59%)]\tLoss: 0.087202\n",
      "Epoch [446/500], Step [3072/3451 (89%)]\tLoss: 0.035181\n",
      "Epoch [446/500], Average Loss: 0.0690\n",
      "\n",
      "Test set: Avg. loss: 4.8628 Accuracy: 333/863 (38.59%)\n",
      "\n",
      "Epoch [447/500], Step [0/3451 (0%)]\tLoss: 0.054665\n",
      "Epoch [447/500], Step [1024/3451 (30%)]\tLoss: 0.024482\n",
      "Epoch [447/500], Step [2048/3451 (59%)]\tLoss: 0.066919\n",
      "Epoch [447/500], Step [3072/3451 (89%)]\tLoss: 0.084516\n",
      "Epoch [447/500], Average Loss: 0.0697\n",
      "Epoch [448/500], Step [0/3451 (0%)]\tLoss: 0.073268\n",
      "Epoch [448/500], Step [1024/3451 (30%)]\tLoss: 0.106656\n",
      "Epoch [448/500], Step [2048/3451 (59%)]\tLoss: 0.078080\n",
      "Epoch [448/500], Step [3072/3451 (89%)]\tLoss: 0.114178\n",
      "Epoch [448/500], Average Loss: 0.0694\n",
      "Epoch [449/500], Step [0/3451 (0%)]\tLoss: 0.063982\n",
      "Epoch [449/500], Step [1024/3451 (30%)]\tLoss: 0.028299\n",
      "Epoch [449/500], Step [2048/3451 (59%)]\tLoss: 0.115979\n",
      "Epoch [449/500], Step [3072/3451 (89%)]\tLoss: 0.063077\n",
      "Epoch [449/500], Average Loss: 0.0688\n",
      "Epoch [450/500], Step [0/3451 (0%)]\tLoss: 0.098716\n",
      "Epoch [450/500], Step [1024/3451 (30%)]\tLoss: 0.130142\n",
      "Epoch [450/500], Step [2048/3451 (59%)]\tLoss: 0.060491\n",
      "Epoch [450/500], Step [3072/3451 (89%)]\tLoss: 0.031710\n",
      "Epoch [450/500], Average Loss: 0.0701\n",
      "Epoch [451/500], Step [0/3451 (0%)]\tLoss: 0.049749\n",
      "Epoch [451/500], Step [1024/3451 (30%)]\tLoss: 0.119735\n",
      "Epoch [451/500], Step [2048/3451 (59%)]\tLoss: 0.069089\n",
      "Epoch [451/500], Step [3072/3451 (89%)]\tLoss: 0.032582\n",
      "Epoch [451/500], Average Loss: 0.0695\n",
      "\n",
      "Test set: Avg. loss: 4.9642 Accuracy: 337/863 (39.05%)\n",
      "\n",
      "Epoch [452/500], Step [0/3451 (0%)]\tLoss: 0.052947\n",
      "Epoch [452/500], Step [1024/3451 (30%)]\tLoss: 0.050229\n",
      "Epoch [452/500], Step [2048/3451 (59%)]\tLoss: 0.035600\n",
      "Epoch [452/500], Step [3072/3451 (89%)]\tLoss: 0.081210\n",
      "Epoch [452/500], Average Loss: 0.0704\n",
      "Epoch [453/500], Step [0/3451 (0%)]\tLoss: 0.034527\n",
      "Epoch [453/500], Step [1024/3451 (30%)]\tLoss: 0.071422\n",
      "Epoch [453/500], Step [2048/3451 (59%)]\tLoss: 0.046038\n",
      "Epoch [453/500], Step [3072/3451 (89%)]\tLoss: 0.064183\n",
      "Epoch [453/500], Average Loss: 0.0686\n",
      "Epoch [454/500], Step [0/3451 (0%)]\tLoss: 0.081545\n",
      "Epoch [454/500], Step [1024/3451 (30%)]\tLoss: 0.109211\n",
      "Epoch [454/500], Step [2048/3451 (59%)]\tLoss: 0.047704\n",
      "Epoch [454/500], Step [3072/3451 (89%)]\tLoss: 0.131581\n",
      "Epoch [454/500], Average Loss: 0.0698\n",
      "Epoch [455/500], Step [0/3451 (0%)]\tLoss: 0.087938\n",
      "Epoch [455/500], Step [1024/3451 (30%)]\tLoss: 0.056334\n",
      "Epoch [455/500], Step [2048/3451 (59%)]\tLoss: 0.096366\n",
      "Epoch [455/500], Step [3072/3451 (89%)]\tLoss: 0.073389\n",
      "Epoch [455/500], Average Loss: 0.0698\n",
      "Epoch [456/500], Step [0/3451 (0%)]\tLoss: 0.080485\n",
      "Epoch [456/500], Step [1024/3451 (30%)]\tLoss: 0.044250\n",
      "Epoch [456/500], Step [2048/3451 (59%)]\tLoss: 0.048292\n",
      "Epoch [456/500], Step [3072/3451 (89%)]\tLoss: 0.093783\n",
      "Epoch [456/500], Average Loss: 0.0693\n",
      "\n",
      "Test set: Avg. loss: 4.9810 Accuracy: 336/863 (38.93%)\n",
      "\n",
      "Epoch [457/500], Step [0/3451 (0%)]\tLoss: 0.066638\n",
      "Epoch [457/500], Step [1024/3451 (30%)]\tLoss: 0.041095\n",
      "Epoch [457/500], Step [2048/3451 (59%)]\tLoss: 0.034998\n",
      "Epoch [457/500], Step [3072/3451 (89%)]\tLoss: 0.097655\n",
      "Epoch [457/500], Average Loss: 0.0699\n",
      "Epoch [458/500], Step [0/3451 (0%)]\tLoss: 0.033240\n",
      "Epoch [458/500], Step [1024/3451 (30%)]\tLoss: 0.075791\n",
      "Epoch [458/500], Step [2048/3451 (59%)]\tLoss: 0.128099\n",
      "Epoch [458/500], Step [3072/3451 (89%)]\tLoss: 0.065928\n",
      "Epoch [458/500], Average Loss: 0.0700\n",
      "Epoch [459/500], Step [0/3451 (0%)]\tLoss: 0.046140\n",
      "Epoch [459/500], Step [1024/3451 (30%)]\tLoss: 0.078377\n",
      "Epoch [459/500], Step [2048/3451 (59%)]\tLoss: 0.046854\n",
      "Epoch [459/500], Step [3072/3451 (89%)]\tLoss: 0.084617\n",
      "Epoch [459/500], Average Loss: 0.0695\n",
      "Epoch [460/500], Step [0/3451 (0%)]\tLoss: 0.120936\n",
      "Epoch [460/500], Step [1024/3451 (30%)]\tLoss: 0.053628\n",
      "Epoch [460/500], Step [2048/3451 (59%)]\tLoss: 0.092539\n",
      "Epoch [460/500], Step [3072/3451 (89%)]\tLoss: 0.062155\n",
      "Epoch [460/500], Average Loss: 0.0691\n",
      "Epoch [461/500], Step [0/3451 (0%)]\tLoss: 0.058375\n",
      "Epoch [461/500], Step [1024/3451 (30%)]\tLoss: 0.147614\n",
      "Epoch [461/500], Step [2048/3451 (59%)]\tLoss: 0.094244\n",
      "Epoch [461/500], Step [3072/3451 (89%)]\tLoss: 0.050009\n",
      "Epoch [461/500], Average Loss: 0.0700\n",
      "\n",
      "Test set: Avg. loss: 4.8549 Accuracy: 334/863 (38.70%)\n",
      "\n",
      "Epoch [462/500], Step [0/3451 (0%)]\tLoss: 0.077523\n",
      "Epoch [462/500], Step [1024/3451 (30%)]\tLoss: 0.050733\n",
      "Epoch [462/500], Step [2048/3451 (59%)]\tLoss: 0.071602\n",
      "Epoch [462/500], Step [3072/3451 (89%)]\tLoss: 0.031056\n",
      "Epoch [462/500], Average Loss: 0.0692\n",
      "Epoch [463/500], Step [0/3451 (0%)]\tLoss: 0.066305\n",
      "Epoch [463/500], Step [1024/3451 (30%)]\tLoss: 0.138225\n",
      "Epoch [463/500], Step [2048/3451 (59%)]\tLoss: 0.035234\n",
      "Epoch [463/500], Step [3072/3451 (89%)]\tLoss: 0.124101\n",
      "Epoch [463/500], Average Loss: 0.0692\n",
      "Epoch [464/500], Step [0/3451 (0%)]\tLoss: 0.063215\n",
      "Epoch [464/500], Step [1024/3451 (30%)]\tLoss: 0.099843\n",
      "Epoch [464/500], Step [2048/3451 (59%)]\tLoss: 0.086976\n",
      "Epoch [464/500], Step [3072/3451 (89%)]\tLoss: 0.018079\n",
      "Epoch [464/500], Average Loss: 0.0691\n",
      "Epoch [465/500], Step [0/3451 (0%)]\tLoss: 0.019973\n",
      "Epoch [465/500], Step [1024/3451 (30%)]\tLoss: 0.082028\n",
      "Epoch [465/500], Step [2048/3451 (59%)]\tLoss: 0.037848\n",
      "Epoch [465/500], Step [3072/3451 (89%)]\tLoss: 0.038946\n",
      "Epoch [465/500], Average Loss: 0.0694\n",
      "Epoch [466/500], Step [0/3451 (0%)]\tLoss: 0.071837\n",
      "Epoch [466/500], Step [1024/3451 (30%)]\tLoss: 0.104665\n",
      "Epoch [466/500], Step [2048/3451 (59%)]\tLoss: 0.041341\n",
      "Epoch [466/500], Step [3072/3451 (89%)]\tLoss: 0.095144\n",
      "Epoch [466/500], Average Loss: 0.0689\n",
      "\n",
      "Test set: Avg. loss: 4.9121 Accuracy: 334/863 (38.70%)\n",
      "\n",
      "Epoch [467/500], Step [0/3451 (0%)]\tLoss: 0.091969\n",
      "Epoch [467/500], Step [1024/3451 (30%)]\tLoss: 0.120458\n",
      "Epoch [467/500], Step [2048/3451 (59%)]\tLoss: 0.046327\n",
      "Epoch [467/500], Step [3072/3451 (89%)]\tLoss: 0.076573\n",
      "Epoch [467/500], Average Loss: 0.0696\n",
      "Epoch [468/500], Step [0/3451 (0%)]\tLoss: 0.114830\n",
      "Epoch [468/500], Step [1024/3451 (30%)]\tLoss: 0.057754\n",
      "Epoch [468/500], Step [2048/3451 (59%)]\tLoss: 0.075838\n",
      "Epoch [468/500], Step [3072/3451 (89%)]\tLoss: 0.071490\n",
      "Epoch [468/500], Average Loss: 0.0709\n",
      "Epoch [469/500], Step [0/3451 (0%)]\tLoss: 0.142255\n",
      "Epoch [469/500], Step [1024/3451 (30%)]\tLoss: 0.060225\n",
      "Epoch [469/500], Step [2048/3451 (59%)]\tLoss: 0.073959\n",
      "Epoch [469/500], Step [3072/3451 (89%)]\tLoss: 0.023643\n",
      "Epoch [469/500], Average Loss: 0.0694\n",
      "Epoch [470/500], Step [0/3451 (0%)]\tLoss: 0.058609\n",
      "Epoch [470/500], Step [1024/3451 (30%)]\tLoss: 0.017725\n",
      "Epoch [470/500], Step [2048/3451 (59%)]\tLoss: 0.025027\n",
      "Epoch [470/500], Step [3072/3451 (89%)]\tLoss: 0.067865\n",
      "Epoch [470/500], Average Loss: 0.0691\n",
      "Epoch [471/500], Step [0/3451 (0%)]\tLoss: 0.006599\n",
      "Epoch [471/500], Step [1024/3451 (30%)]\tLoss: 0.072837\n",
      "Epoch [471/500], Step [2048/3451 (59%)]\tLoss: 0.038426\n",
      "Epoch [471/500], Step [3072/3451 (89%)]\tLoss: 0.050789\n",
      "Epoch [471/500], Average Loss: 0.0689\n",
      "\n",
      "Test set: Avg. loss: 4.9249 Accuracy: 330/863 (38.24%)\n",
      "\n",
      "Epoch [472/500], Step [0/3451 (0%)]\tLoss: 0.106609\n",
      "Epoch [472/500], Step [1024/3451 (30%)]\tLoss: 0.067432\n",
      "Epoch [472/500], Step [2048/3451 (59%)]\tLoss: 0.107107\n",
      "Epoch [472/500], Step [3072/3451 (89%)]\tLoss: 0.146192\n",
      "Epoch [472/500], Average Loss: 0.0696\n",
      "Epoch [473/500], Step [0/3451 (0%)]\tLoss: 0.088149\n",
      "Epoch [473/500], Step [1024/3451 (30%)]\tLoss: 0.071492\n",
      "Epoch [473/500], Step [2048/3451 (59%)]\tLoss: 0.115779\n",
      "Epoch [473/500], Step [3072/3451 (89%)]\tLoss: 0.125534\n",
      "Epoch [473/500], Average Loss: 0.0690\n",
      "Epoch [474/500], Step [0/3451 (0%)]\tLoss: 0.061881\n",
      "Epoch [474/500], Step [1024/3451 (30%)]\tLoss: 0.048166\n",
      "Epoch [474/500], Step [2048/3451 (59%)]\tLoss: 0.029597\n",
      "Epoch [474/500], Step [3072/3451 (89%)]\tLoss: 0.050032\n",
      "Epoch [474/500], Average Loss: 0.0696\n",
      "Epoch [475/500], Step [0/3451 (0%)]\tLoss: 0.111476\n",
      "Epoch [475/500], Step [1024/3451 (30%)]\tLoss: 0.086249\n",
      "Epoch [475/500], Step [2048/3451 (59%)]\tLoss: 0.063276\n",
      "Epoch [475/500], Step [3072/3451 (89%)]\tLoss: 0.031645\n",
      "Epoch [475/500], Average Loss: 0.0703\n",
      "Epoch [476/500], Step [0/3451 (0%)]\tLoss: 0.058923\n",
      "Epoch [476/500], Step [1024/3451 (30%)]\tLoss: 0.071259\n",
      "Epoch [476/500], Step [2048/3451 (59%)]\tLoss: 0.037749\n",
      "Epoch [476/500], Step [3072/3451 (89%)]\tLoss: 0.061679\n",
      "Epoch [476/500], Average Loss: 0.0693\n",
      "\n",
      "Test set: Avg. loss: 4.9468 Accuracy: 335/863 (38.82%)\n",
      "\n",
      "Epoch [477/500], Step [0/3451 (0%)]\tLoss: 0.073502\n",
      "Epoch [477/500], Step [1024/3451 (30%)]\tLoss: 0.064473\n",
      "Epoch [477/500], Step [2048/3451 (59%)]\tLoss: 0.034635\n",
      "Epoch [477/500], Step [3072/3451 (89%)]\tLoss: 0.061763\n",
      "Epoch [477/500], Average Loss: 0.0701\n",
      "Epoch [478/500], Step [0/3451 (0%)]\tLoss: 0.039179\n",
      "Epoch [478/500], Step [1024/3451 (30%)]\tLoss: 0.044023\n",
      "Epoch [478/500], Step [2048/3451 (59%)]\tLoss: 0.110893\n",
      "Epoch [478/500], Step [3072/3451 (89%)]\tLoss: 0.088153\n",
      "Epoch [478/500], Average Loss: 0.0701\n",
      "Epoch [479/500], Step [0/3451 (0%)]\tLoss: 0.118702\n",
      "Epoch [479/500], Step [1024/3451 (30%)]\tLoss: 0.047674\n",
      "Epoch [479/500], Step [2048/3451 (59%)]\tLoss: 0.063447\n",
      "Epoch [479/500], Step [3072/3451 (89%)]\tLoss: 0.093539\n",
      "Epoch [479/500], Average Loss: 0.0696\n",
      "Epoch [480/500], Step [0/3451 (0%)]\tLoss: 0.049077\n",
      "Epoch [480/500], Step [1024/3451 (30%)]\tLoss: 0.051687\n",
      "Epoch [480/500], Step [2048/3451 (59%)]\tLoss: 0.060423\n",
      "Epoch [480/500], Step [3072/3451 (89%)]\tLoss: 0.055605\n",
      "Epoch [480/500], Average Loss: 0.0689\n",
      "Epoch [481/500], Step [0/3451 (0%)]\tLoss: 0.070349\n",
      "Epoch [481/500], Step [1024/3451 (30%)]\tLoss: 0.089859\n",
      "Epoch [481/500], Step [2048/3451 (59%)]\tLoss: 0.050114\n",
      "Epoch [481/500], Step [3072/3451 (89%)]\tLoss: 0.059722\n",
      "Epoch [481/500], Average Loss: 0.0710\n",
      "\n",
      "Test set: Avg. loss: 4.8850 Accuracy: 347/863 (40.21%)\n",
      "\n",
      "Epoch [482/500], Step [0/3451 (0%)]\tLoss: 0.089330\n",
      "Epoch [482/500], Step [1024/3451 (30%)]\tLoss: 0.047278\n",
      "Epoch [482/500], Step [2048/3451 (59%)]\tLoss: 0.080709\n",
      "Epoch [482/500], Step [3072/3451 (89%)]\tLoss: 0.004355\n",
      "Epoch [482/500], Average Loss: 0.0696\n",
      "Epoch [483/500], Step [0/3451 (0%)]\tLoss: 0.020961\n",
      "Epoch [483/500], Step [1024/3451 (30%)]\tLoss: 0.043557\n",
      "Epoch [483/500], Step [2048/3451 (59%)]\tLoss: 0.085779\n",
      "Epoch [483/500], Step [3072/3451 (89%)]\tLoss: 0.050068\n",
      "Epoch [483/500], Average Loss: 0.0713\n",
      "Epoch [484/500], Step [0/3451 (0%)]\tLoss: 0.039179\n",
      "Epoch [484/500], Step [1024/3451 (30%)]\tLoss: 0.098725\n",
      "Epoch [484/500], Step [2048/3451 (59%)]\tLoss: 0.105520\n",
      "Epoch [484/500], Step [3072/3451 (89%)]\tLoss: 0.111929\n",
      "Epoch [484/500], Average Loss: 0.0688\n",
      "Epoch [485/500], Step [0/3451 (0%)]\tLoss: 0.102993\n",
      "Epoch [485/500], Step [1024/3451 (30%)]\tLoss: 0.086969\n",
      "Epoch [485/500], Step [2048/3451 (59%)]\tLoss: 0.055196\n",
      "Epoch [485/500], Step [3072/3451 (89%)]\tLoss: 0.053548\n",
      "Epoch [485/500], Average Loss: 0.0693\n",
      "Epoch [486/500], Step [0/3451 (0%)]\tLoss: 0.042291\n",
      "Epoch [486/500], Step [1024/3451 (30%)]\tLoss: 0.152989\n",
      "Epoch [486/500], Step [2048/3451 (59%)]\tLoss: 0.038261\n",
      "Epoch [486/500], Step [3072/3451 (89%)]\tLoss: 0.145430\n",
      "Epoch [486/500], Average Loss: 0.0693\n",
      "\n",
      "Test set: Avg. loss: 4.8856 Accuracy: 341/863 (39.51%)\n",
      "\n",
      "Epoch [487/500], Step [0/3451 (0%)]\tLoss: 0.038191\n",
      "Epoch [487/500], Step [1024/3451 (30%)]\tLoss: 0.063532\n",
      "Epoch [487/500], Step [2048/3451 (59%)]\tLoss: 0.064605\n",
      "Epoch [487/500], Step [3072/3451 (89%)]\tLoss: 0.034939\n",
      "Epoch [487/500], Average Loss: 0.0698\n",
      "Epoch [488/500], Step [0/3451 (0%)]\tLoss: 0.082781\n",
      "Epoch [488/500], Step [1024/3451 (30%)]\tLoss: 0.081434\n",
      "Epoch [488/500], Step [2048/3451 (59%)]\tLoss: 0.044324\n",
      "Epoch [488/500], Step [3072/3451 (89%)]\tLoss: 0.058938\n",
      "Epoch [488/500], Average Loss: 0.0697\n",
      "Epoch [489/500], Step [0/3451 (0%)]\tLoss: 0.040626\n",
      "Epoch [489/500], Step [1024/3451 (30%)]\tLoss: 0.108038\n",
      "Epoch [489/500], Step [2048/3451 (59%)]\tLoss: 0.062402\n",
      "Epoch [489/500], Step [3072/3451 (89%)]\tLoss: 0.062166\n",
      "Epoch [489/500], Average Loss: 0.0691\n",
      "Epoch [490/500], Step [0/3451 (0%)]\tLoss: 0.075062\n",
      "Epoch [490/500], Step [1024/3451 (30%)]\tLoss: 0.102092\n",
      "Epoch [490/500], Step [2048/3451 (59%)]\tLoss: 0.072325\n",
      "Epoch [490/500], Step [3072/3451 (89%)]\tLoss: 0.092208\n",
      "Epoch [490/500], Average Loss: 0.0702\n",
      "Epoch [491/500], Step [0/3451 (0%)]\tLoss: 0.045187\n",
      "Epoch [491/500], Step [1024/3451 (30%)]\tLoss: 0.021853\n",
      "Epoch [491/500], Step [2048/3451 (59%)]\tLoss: 0.036126\n",
      "Epoch [491/500], Step [3072/3451 (89%)]\tLoss: 0.058344\n",
      "Epoch [491/500], Average Loss: 0.0704\n",
      "\n",
      "Test set: Avg. loss: 4.9876 Accuracy: 335/863 (38.82%)\n",
      "\n",
      "Epoch [492/500], Step [0/3451 (0%)]\tLoss: 0.106222\n",
      "Epoch [492/500], Step [1024/3451 (30%)]\tLoss: 0.017571\n",
      "Epoch [492/500], Step [2048/3451 (59%)]\tLoss: 0.069931\n",
      "Epoch [492/500], Step [3072/3451 (89%)]\tLoss: 0.076362\n",
      "Epoch [492/500], Average Loss: 0.0695\n",
      "Epoch [493/500], Step [0/3451 (0%)]\tLoss: 0.049329\n",
      "Epoch [493/500], Step [1024/3451 (30%)]\tLoss: 0.064471\n",
      "Epoch [493/500], Step [2048/3451 (59%)]\tLoss: 0.034511\n",
      "Epoch [493/500], Step [3072/3451 (89%)]\tLoss: 0.088233\n",
      "Epoch [493/500], Average Loss: 0.0701\n",
      "Epoch [494/500], Step [0/3451 (0%)]\tLoss: 0.086936\n",
      "Epoch [494/500], Step [1024/3451 (30%)]\tLoss: 0.053441\n",
      "Epoch [494/500], Step [2048/3451 (59%)]\tLoss: 0.033718\n",
      "Epoch [494/500], Step [3072/3451 (89%)]\tLoss: 0.057329\n",
      "Epoch [494/500], Average Loss: 0.0693\n",
      "Epoch [495/500], Step [0/3451 (0%)]\tLoss: 0.047373\n",
      "Epoch [495/500], Step [1024/3451 (30%)]\tLoss: 0.099093\n",
      "Epoch [495/500], Step [2048/3451 (59%)]\tLoss: 0.058232\n",
      "Epoch [495/500], Step [3072/3451 (89%)]\tLoss: 0.079770\n",
      "Epoch [495/500], Average Loss: 0.0691\n",
      "Epoch [496/500], Step [0/3451 (0%)]\tLoss: 0.153263\n",
      "Epoch [496/500], Step [1024/3451 (30%)]\tLoss: 0.076070\n",
      "Epoch [496/500], Step [2048/3451 (59%)]\tLoss: 0.034573\n",
      "Epoch [496/500], Step [3072/3451 (89%)]\tLoss: 0.008701\n",
      "Epoch [496/500], Average Loss: 0.0688\n",
      "\n",
      "Test set: Avg. loss: 4.8466 Accuracy: 336/863 (38.93%)\n",
      "\n",
      "Epoch [497/500], Step [0/3451 (0%)]\tLoss: 0.029760\n",
      "Epoch [497/500], Step [1024/3451 (30%)]\tLoss: 0.056800\n",
      "Epoch [497/500], Step [2048/3451 (59%)]\tLoss: 0.084463\n",
      "Epoch [497/500], Step [3072/3451 (89%)]\tLoss: 0.127608\n",
      "Epoch [497/500], Average Loss: 0.0703\n",
      "Epoch [498/500], Step [0/3451 (0%)]\tLoss: 0.112748\n",
      "Epoch [498/500], Step [1024/3451 (30%)]\tLoss: 0.114745\n",
      "Epoch [498/500], Step [2048/3451 (59%)]\tLoss: 0.097449\n",
      "Epoch [498/500], Step [3072/3451 (89%)]\tLoss: 0.043420\n",
      "Epoch [498/500], Average Loss: 0.0703\n",
      "Epoch [499/500], Step [0/3451 (0%)]\tLoss: 0.102749\n",
      "Epoch [499/500], Step [1024/3451 (30%)]\tLoss: 0.023731\n",
      "Epoch [499/500], Step [2048/3451 (59%)]\tLoss: 0.103848\n",
      "Epoch [499/500], Step [3072/3451 (89%)]\tLoss: 0.093701\n",
      "Epoch [499/500], Average Loss: 0.0694\n",
      "Epoch [500/500], Step [0/3451 (0%)]\tLoss: 0.056366\n",
      "Epoch [500/500], Step [1024/3451 (30%)]\tLoss: 0.061711\n",
      "Epoch [500/500], Step [2048/3451 (59%)]\tLoss: 0.030944\n",
      "Epoch [500/500], Step [3072/3451 (89%)]\tLoss: 0.019918\n",
      "Epoch [500/500], Average Loss: 0.0691\n",
      "\n",
      "Test set: Avg. loss: 4.9187 Accuracy: 339/863 (39.28%)\n",
      "\n",
      "Training Finished\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    num_epochs = 500 \n",
    "    batch_size = 32\n",
    "    learning_rate = 0.001\n",
    "    log_interval = 32\n",
    "    num_classes = 4\n",
    "    loss_list = []\n",
    "    accuracy_list = []\n",
    "\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    # device = 'cpu'\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    \n",
    "    train_loader, val_loader = getData()\n",
    "    # data = getData()\n",
    "    # data\n",
    "\n",
    "    model.to(device)\n",
    "     \n",
    "    \n",
    "    # criterion = nn.MultiLabelSoftMarginLoss()\n",
    "    # lossFunc_test = nn.MultiLabelSoftMarginLoss(reduction=\"sum\")\n",
    "    # criterion = nn.BCELoss()\n",
    "    # lossFunc_test = nn.BCELoss(reduction=\"sum\")\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    lossFunc_test = nn.BCEWithLogitsLoss(reduction=\"sum\")\n",
    "\n",
    "    \n",
    "    \n",
    "    # Try again 400 epochs\n",
    "    # optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    # optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, \n",
    "                                                    max_lr = 0.01, \n",
    "                                                    steps_per_epoch=len(train_loader),\n",
    "                                                    epochs=num_epochs\n",
    "                                                    )\n",
    "    # scheduler = ReduceLROnPlateau(\n",
    "    #     optimizer, \n",
    "    #     mode='min',  \n",
    "    #     factor=0.1, \n",
    "    #     patience=10,\n",
    "    #     threshold=0.0001, \n",
    "    #     cooldown=0, \n",
    "    #     min_lr = 1e-6, \n",
    "    #     verbose=True\n",
    "    # )                   \n",
    "    \n",
    "    print(\"Initial Test: \")\n",
    "    accuracy = test()\n",
    "    accuracy_list.append(accuracy)\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        running_loss += train(running_loss, epoch)\n",
    "\n",
    "        average_loss = running_loss / len(train_loader)\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Average Loss: {average_loss:.4f}\")\n",
    "\n",
    "        # Test every 5 epochs \n",
    "        if epoch % 5 == 0:\n",
    "            accuracy = test()\n",
    "            accuracy_list.append(accuracy)\n",
    "\n",
    "        loss_list.append(average_loss)\n",
    "\n",
    "\n",
    "    # Final test\n",
    "    accuracy = test()\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "    print(\"Training Finished\")\n",
    "    torch.save(model, \"./model_FULL.pth\")\n",
    "\n",
    "    epoch_range = range(0, num_epochs)\n",
    "    plt.plot(epoch_range, loss_list)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Average Loss\")\n",
    "    plt.title(\"Average Loss Over Time\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    epoch_range_test = range(0, len(accuracy_list))\n",
    "    plt.plot(epoch_range_test, accuracy_list)\n",
    "    plt.xlabel(\"Epoch (×5)\")\n",
    "    plt.ylabel(\"Accuracy %\")\n",
    "    plt.title(\"Accuracy (every 5 epochs)\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
